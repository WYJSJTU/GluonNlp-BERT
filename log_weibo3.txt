INFO:root:15:02:13 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:15:02:16 processing dataset...
INFO:root:15:02:39 Now we are doing BERT classification training on gpu(0)!
INFO:root:15:02:39 training steps=16406
INFO:root:15:05:05 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:15:05:20 processing dataset...
INFO:root:15:06:02 Now we are doing BERT classification training on gpu(0)!
INFO:root:15:06:02 training steps=16406
INFO:root:15:07:11 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:15:07:18 processing dataset...
INFO:root:15:07:42 Namespace(accumulate=None, batch_size=4, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:15:07:48 processing dataset...
INFO:root:15:08:27 Now we are doing BERT classification training on gpu(0)!
INFO:root:15:08:27 training steps=131250
INFO:root:15:08:56 [Epoch 1 Batch 400/26255] loss=1.1278, lr=0.0000006, metrics:accuracy:0.3262
INFO:root:15:09:24 [Epoch 1 Batch 800/26255] loss=1.1032, lr=0.0000012, metrics:accuracy:0.3441
INFO:root:15:09:53 [Epoch 1 Batch 1200/26255] loss=1.0998, lr=0.0000018, metrics:accuracy:0.3488
INFO:root:15:10:21 [Epoch 1 Batch 1600/26255] loss=1.1083, lr=0.0000024, metrics:accuracy:0.3511
INFO:root:15:10:47 [Epoch 1 Batch 2000/26255] loss=1.1001, lr=0.0000030, metrics:accuracy:0.3545
INFO:root:15:11:42 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:15:11:42 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:15:11:49 processing dataset...
INFO:root:15:14:11 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:15:14:11 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:15:14:17 processing dataset...
INFO:root:15:14:48 Now we are doing BERT classification training on gpu(0)!
INFO:root:15:14:49 training steps=32812
INFO:root:15:15:14 [Epoch 1 Batch 400/13130] loss=1.1171, lr=0.0000012, metrics:accuracy:0.3417
INFO:root:15:15:41 [Epoch 1 Batch 800/13130] loss=1.0991, lr=0.0000024, metrics:accuracy:0.3458
INFO:root:15:16:07 [Epoch 1 Batch 1200/13130] loss=1.0960, lr=0.0000037, metrics:accuracy:0.3511
INFO:root:15:16:33 [Epoch 1 Batch 1600/13130] loss=1.0974, lr=0.0000049, metrics:accuracy:0.3571
INFO:root:15:16:58 [Epoch 1 Batch 2000/13130] loss=1.0971, lr=0.0000061, metrics:accuracy:0.3587
INFO:root:15:17:22 [Epoch 1 Batch 2400/13130] loss=1.0961, lr=0.0000073, metrics:accuracy:0.3589
INFO:root:15:17:45 [Epoch 1 Batch 2800/13130] loss=1.0911, lr=0.0000085, metrics:accuracy:0.3618
INFO:root:15:18:08 [Epoch 1 Batch 3200/13130] loss=1.0985, lr=0.0000097, metrics:accuracy:0.3635
INFO:root:15:18:32 [Epoch 1 Batch 3600/13130] loss=1.0967, lr=0.0000110, metrics:accuracy:0.3660
INFO:root:15:18:55 [Epoch 1 Batch 4000/13130] loss=1.0913, lr=0.0000122, metrics:accuracy:0.3678
INFO:root:15:19:18 [Epoch 1 Batch 4400/13130] loss=1.0914, lr=0.0000134, metrics:accuracy:0.3690
INFO:root:15:19:43 [Epoch 1 Batch 4800/13130] loss=1.0887, lr=0.0000146, metrics:accuracy:0.3700
INFO:root:15:20:07 [Epoch 1 Batch 5200/13130] loss=1.0953, lr=0.0000158, metrics:accuracy:0.3711
INFO:root:15:20:32 [Epoch 1 Batch 5600/13130] loss=1.0849, lr=0.0000171, metrics:accuracy:0.3736
INFO:root:15:20:55 [Epoch 1 Batch 6000/13130] loss=1.0848, lr=0.0000183, metrics:accuracy:0.3754
INFO:root:15:21:18 [Epoch 1 Batch 6400/13130] loss=1.0897, lr=0.0000195, metrics:accuracy:0.3764
INFO:root:15:21:40 [Epoch 1 Batch 6800/13130] loss=1.0817, lr=0.0000199, metrics:accuracy:0.3783
INFO:root:15:22:02 [Epoch 1 Batch 7200/13130] loss=1.0828, lr=0.0000198, metrics:accuracy:0.3799
INFO:root:15:22:23 [Epoch 1 Batch 7600/13130] loss=1.0814, lr=0.0000196, metrics:accuracy:0.3810
INFO:root:15:22:46 [Epoch 1 Batch 8000/13130] loss=1.0869, lr=0.0000195, metrics:accuracy:0.3817
INFO:root:15:23:08 [Epoch 1 Batch 8400/13130] loss=1.0861, lr=0.0000194, metrics:accuracy:0.3821
INFO:root:15:23:31 [Epoch 1 Batch 8800/13130] loss=1.0771, lr=0.0000192, metrics:accuracy:0.3836
INFO:root:15:23:55 [Epoch 1 Batch 9200/13130] loss=1.0681, lr=0.0000191, metrics:accuracy:0.3857
INFO:root:15:24:19 [Epoch 1 Batch 9600/13130] loss=1.0690, lr=0.0000190, metrics:accuracy:0.3873
INFO:root:15:24:43 [Epoch 1 Batch 10000/13130] loss=1.0743, lr=0.0000188, metrics:accuracy:0.3883
INFO:root:15:25:06 [Epoch 1 Batch 10400/13130] loss=1.0600, lr=0.0000187, metrics:accuracy:0.3901
INFO:root:15:25:29 [Epoch 1 Batch 10800/13130] loss=1.0616, lr=0.0000186, metrics:accuracy:0.3921
INFO:root:15:25:52 [Epoch 1 Batch 11200/13130] loss=1.0743, lr=0.0000184, metrics:accuracy:0.3931
INFO:root:15:26:16 [Epoch 1 Batch 11600/13130] loss=1.0630, lr=0.0000183, metrics:accuracy:0.3945
INFO:root:15:26:40 [Epoch 1 Batch 12000/13130] loss=1.0732, lr=0.0000182, metrics:accuracy:0.3953
INFO:root:15:27:02 [Epoch 1 Batch 12400/13130] loss=1.0641, lr=0.0000180, metrics:accuracy:0.3966
INFO:root:15:27:26 [Epoch 1 Batch 12800/13130] loss=1.0665, lr=0.0000179, metrics:accuracy:0.3977
INFO:root:15:27:45 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:27:57 [Batch 400/3750] loss=1.1066, metrics:accuracy:0.4531
INFO:root:15:28:09 [Batch 800/3750] loss=1.1112, metrics:accuracy:0.4456
INFO:root:15:28:21 [Batch 1200/3750] loss=1.1097, metrics:accuracy:0.4425
INFO:root:15:28:33 [Batch 1600/3750] loss=1.1550, metrics:accuracy:0.3941
INFO:root:15:28:46 [Batch 2000/3750] loss=1.1583, metrics:accuracy:0.3608
INFO:root:15:28:58 [Batch 2400/3750] loss=1.1478, metrics:accuracy:0.3403
INFO:root:15:29:10 [Batch 2800/3750] loss=0.9681, metrics:accuracy:0.3707
INFO:root:15:29:22 [Batch 3200/3750] loss=0.8997, metrics:accuracy:0.4063
INFO:root:15:29:34 [Batch 3600/3750] loss=0.8986, metrics:accuracy:0.4343
INFO:root:15:29:38 validation metrics:accuracy:0.4440
INFO:root:15:29:38 Time cost=113.81s, throughput=263.61 samples/s
INFO:root:15:29:39 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:15:29:39 Time cost=890.79s
INFO:root:15:30:07 [Epoch 2 Batch 400/13130] loss=1.0514, lr=0.0000176, metrics:accuracy:0.4528
INFO:root:15:30:34 [Epoch 2 Batch 800/13130] loss=1.0555, lr=0.0000175, metrics:accuracy:0.4503
INFO:root:15:31:01 [Epoch 2 Batch 1200/13130] loss=1.0448, lr=0.0000174, metrics:accuracy:0.4517
INFO:root:15:31:25 [Epoch 2 Batch 1600/13130] loss=1.0406, lr=0.0000172, metrics:accuracy:0.4554
INFO:root:15:31:48 [Epoch 2 Batch 2000/13130] loss=1.0369, lr=0.0000171, metrics:accuracy:0.4569
INFO:root:15:32:12 [Epoch 2 Batch 2400/13130] loss=1.0409, lr=0.0000170, metrics:accuracy:0.4562
INFO:root:15:32:35 [Epoch 2 Batch 2800/13130] loss=1.0448, lr=0.0000168, metrics:accuracy:0.4567
INFO:root:15:32:59 [Epoch 2 Batch 3200/13130] loss=1.0497, lr=0.0000167, metrics:accuracy:0.4543
INFO:root:15:33:22 [Epoch 2 Batch 3600/13130] loss=1.0445, lr=0.0000166, metrics:accuracy:0.4540
INFO:root:15:33:47 [Epoch 2 Batch 4000/13130] loss=1.0423, lr=0.0000164, metrics:accuracy:0.4539
INFO:root:15:34:10 [Epoch 2 Batch 4400/13130] loss=1.0517, lr=0.0000163, metrics:accuracy:0.4544
INFO:root:15:34:32 [Epoch 2 Batch 4800/13130] loss=1.0392, lr=0.0000162, metrics:accuracy:0.4546
INFO:root:15:34:55 [Epoch 2 Batch 5200/13130] loss=1.0510, lr=0.0000160, metrics:accuracy:0.4543
INFO:root:15:35:18 [Epoch 2 Batch 5600/13130] loss=1.0423, lr=0.0000159, metrics:accuracy:0.4543
INFO:root:15:35:40 [Epoch 2 Batch 6000/13130] loss=1.0506, lr=0.0000157, metrics:accuracy:0.4552
INFO:root:15:36:04 [Epoch 2 Batch 6400/13130] loss=1.0242, lr=0.0000156, metrics:accuracy:0.4568
INFO:root:15:36:29 [Epoch 2 Batch 6800/13130] loss=1.0431, lr=0.0000155, metrics:accuracy:0.4567
INFO:root:15:36:52 [Epoch 2 Batch 7200/13130] loss=1.0590, lr=0.0000153, metrics:accuracy:0.4559
INFO:root:15:37:15 [Epoch 2 Batch 7600/13130] loss=1.0378, lr=0.0000152, metrics:accuracy:0.4570
INFO:root:15:37:39 [Epoch 2 Batch 8000/13130] loss=1.0429, lr=0.0000151, metrics:accuracy:0.4567
INFO:root:15:38:08 [Epoch 2 Batch 8400/13130] loss=1.0382, lr=0.0000149, metrics:accuracy:0.4573
INFO:root:15:38:35 [Epoch 2 Batch 8800/13130] loss=1.0539, lr=0.0000148, metrics:accuracy:0.4561
INFO:root:15:39:05 [Epoch 2 Batch 9200/13130] loss=1.0451, lr=0.0000147, metrics:accuracy:0.4562
INFO:root:15:39:33 [Epoch 2 Batch 9600/13130] loss=1.0327, lr=0.0000145, metrics:accuracy:0.4564
INFO:root:15:40:02 [Epoch 2 Batch 10000/13130] loss=1.0446, lr=0.0000144, metrics:accuracy:0.4567
INFO:root:15:40:32 [Epoch 2 Batch 10400/13130] loss=1.0309, lr=0.0000143, metrics:accuracy:0.4572
INFO:root:15:40:59 [Epoch 2 Batch 10800/13130] loss=1.0414, lr=0.0000141, metrics:accuracy:0.4571
INFO:root:15:41:26 [Epoch 2 Batch 11200/13130] loss=1.0263, lr=0.0000140, metrics:accuracy:0.4577
INFO:root:15:41:55 [Epoch 2 Batch 11600/13130] loss=1.0358, lr=0.0000138, metrics:accuracy:0.4581
INFO:root:15:42:22 [Epoch 2 Batch 12000/13130] loss=1.0404, lr=0.0000137, metrics:accuracy:0.4583
INFO:root:15:42:49 [Epoch 2 Batch 12400/13130] loss=1.0396, lr=0.0000136, metrics:accuracy:0.4584
INFO:root:15:43:15 [Epoch 2 Batch 12800/13130] loss=1.0171, lr=0.0000134, metrics:accuracy:0.4594
INFO:root:15:43:35 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:43:47 [Batch 400/3750] loss=1.0040, metrics:accuracy:0.5353
INFO:root:15:43:59 [Batch 800/3750] loss=1.0078, metrics:accuracy:0.5348
INFO:root:15:44:12 [Batch 1200/3750] loss=1.0044, metrics:accuracy:0.5331
INFO:root:15:44:24 [Batch 1600/3750] loss=1.1331, metrics:accuracy:0.4856
INFO:root:15:44:36 [Batch 2000/3750] loss=1.1448, metrics:accuracy:0.4541
INFO:root:15:44:48 [Batch 2400/3750] loss=1.1318, metrics:accuracy:0.4344
INFO:root:15:45:01 [Batch 2800/3750] loss=1.0089, metrics:accuracy:0.4416
INFO:root:15:45:13 [Batch 3200/3750] loss=0.9488, metrics:accuracy:0.4546
INFO:root:15:45:25 [Batch 3600/3750] loss=0.9517, metrics:accuracy:0.4639
INFO:root:15:45:29 validation metrics:accuracy:0.4668
INFO:root:15:45:29 Time cost=114.39s, throughput=262.27 samples/s
INFO:root:15:45:30 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:15:45:30 Time cost=950.84s
INFO:root:15:45:59 [Epoch 3 Batch 400/13130] loss=1.0111, lr=0.0000132, metrics:accuracy:0.4691
INFO:root:15:46:28 [Epoch 3 Batch 800/13130] loss=1.0127, lr=0.0000131, metrics:accuracy:0.4779
INFO:root:15:46:56 [Epoch 3 Batch 1200/13130] loss=1.0190, lr=0.0000129, metrics:accuracy:0.4801
INFO:root:15:47:24 [Epoch 3 Batch 1600/13130] loss=1.0073, lr=0.0000128, metrics:accuracy:0.4806
INFO:root:15:47:51 [Epoch 3 Batch 2000/13130] loss=1.0080, lr=0.0000127, metrics:accuracy:0.4816
INFO:root:15:48:18 [Epoch 3 Batch 2400/13130] loss=1.0271, lr=0.0000125, metrics:accuracy:0.4809
INFO:root:15:48:46 [Epoch 3 Batch 2800/13130] loss=1.0048, lr=0.0000124, metrics:accuracy:0.4835
INFO:root:15:49:12 [Epoch 3 Batch 3200/13130] loss=1.0014, lr=0.0000122, metrics:accuracy:0.4859
INFO:root:15:49:37 [Epoch 3 Batch 3600/13130] loss=1.0052, lr=0.0000121, metrics:accuracy:0.4850
INFO:root:15:50:02 [Epoch 3 Batch 4000/13130] loss=1.0063, lr=0.0000120, metrics:accuracy:0.4863
INFO:root:15:50:25 [Epoch 3 Batch 4400/13130] loss=1.0226, lr=0.0000118, metrics:accuracy:0.4849
INFO:root:15:50:50 [Epoch 3 Batch 4800/13130] loss=1.0126, lr=0.0000117, metrics:accuracy:0.4856
INFO:root:15:51:13 [Epoch 3 Batch 5200/13130] loss=1.0303, lr=0.0000116, metrics:accuracy:0.4842
INFO:root:15:51:38 [Epoch 3 Batch 5600/13130] loss=1.0062, lr=0.0000114, metrics:accuracy:0.4846
INFO:root:15:52:05 [Epoch 3 Batch 6000/13130] loss=1.0130, lr=0.0000113, metrics:accuracy:0.4848
INFO:root:15:52:29 [Epoch 3 Batch 6400/13130] loss=1.0204, lr=0.0000112, metrics:accuracy:0.4843
INFO:root:15:52:53 [Epoch 3 Batch 6800/13130] loss=0.9943, lr=0.0000110, metrics:accuracy:0.4852
INFO:root:15:53:16 [Epoch 3 Batch 7200/13130] loss=1.0277, lr=0.0000109, metrics:accuracy:0.4847
INFO:root:15:53:37 [Epoch 3 Batch 7600/13130] loss=1.0140, lr=0.0000108, metrics:accuracy:0.4845
INFO:root:15:54:00 [Epoch 3 Batch 8000/13130] loss=1.0118, lr=0.0000106, metrics:accuracy:0.4847
INFO:root:15:54:22 [Epoch 3 Batch 8400/13130] loss=0.9999, lr=0.0000105, metrics:accuracy:0.4852
INFO:root:15:54:44 [Epoch 3 Batch 8800/13130] loss=1.0056, lr=0.0000104, metrics:accuracy:0.4852
INFO:root:15:55:07 [Epoch 3 Batch 9200/13130] loss=1.0217, lr=0.0000102, metrics:accuracy:0.4847
INFO:root:15:55:30 [Epoch 3 Batch 9600/13130] loss=1.0102, lr=0.0000101, metrics:accuracy:0.4850
INFO:root:15:55:54 [Epoch 3 Batch 10000/13130] loss=1.0056, lr=0.0000099, metrics:accuracy:0.4850
INFO:root:15:56:19 [Epoch 3 Batch 10400/13130] loss=0.9990, lr=0.0000098, metrics:accuracy:0.4855
INFO:root:15:56:42 [Epoch 3 Batch 10800/13130] loss=1.0091, lr=0.0000097, metrics:accuracy:0.4856
INFO:root:15:57:05 [Epoch 3 Batch 11200/13130] loss=1.0104, lr=0.0000095, metrics:accuracy:0.4857
INFO:root:15:57:28 [Epoch 3 Batch 11600/13130] loss=1.0038, lr=0.0000094, metrics:accuracy:0.4859
INFO:root:15:57:52 [Epoch 3 Batch 12000/13130] loss=0.9970, lr=0.0000093, metrics:accuracy:0.4864
INFO:root:15:58:15 [Epoch 3 Batch 12400/13130] loss=0.9918, lr=0.0000091, metrics:accuracy:0.4869
INFO:root:15:58:38 [Epoch 3 Batch 12800/13130] loss=0.9792, lr=0.0000090, metrics:accuracy:0.4879
INFO:root:15:58:57 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:59:09 [Batch 400/3750] loss=0.9812, metrics:accuracy:0.5866
INFO:root:15:59:21 [Batch 800/3750] loss=0.9888, metrics:accuracy:0.5833
INFO:root:15:59:33 [Batch 1200/3750] loss=0.9819, metrics:accuracy:0.5842
INFO:root:15:59:46 [Batch 1600/3750] loss=1.1361, metrics:accuracy:0.5279
INFO:root:15:59:58 [Batch 2000/3750] loss=1.1435, metrics:accuracy:0.4897
INFO:root:16:00:10 [Batch 2400/3750] loss=1.1378, metrics:accuracy:0.4636
INFO:root:16:00:22 [Batch 2800/3750] loss=1.0162, metrics:accuracy:0.4635
INFO:root:16:00:34 [Batch 3200/3750] loss=0.9621, metrics:accuracy:0.4678
INFO:root:16:00:46 [Batch 3600/3750] loss=0.9616, metrics:accuracy:0.4717
INFO:root:16:00:50 validation metrics:accuracy:0.4729
INFO:root:16:00:50 Time cost=113.32s, throughput=264.75 samples/s
INFO:root:16:00:53 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:16:00:53 Time cost=923.00s
INFO:root:16:01:20 [Epoch 4 Batch 400/13130] loss=0.9760, lr=0.0000087, metrics:accuracy:0.5192
INFO:root:16:01:47 [Epoch 4 Batch 800/13130] loss=0.9784, lr=0.0000086, metrics:accuracy:0.5145
INFO:root:16:02:14 [Epoch 4 Batch 1200/13130] loss=0.9748, lr=0.0000085, metrics:accuracy:0.5172
INFO:root:16:02:41 [Epoch 4 Batch 1600/13130] loss=0.9696, lr=0.0000083, metrics:accuracy:0.5174
INFO:root:16:03:04 [Epoch 4 Batch 2000/13130] loss=0.9564, lr=0.0000082, metrics:accuracy:0.5188
INFO:root:16:03:28 [Epoch 4 Batch 2400/13130] loss=0.9754, lr=0.0000081, metrics:accuracy:0.5186
INFO:root:16:03:51 [Epoch 4 Batch 2800/13130] loss=0.9643, lr=0.0000079, metrics:accuracy:0.5189
INFO:root:16:04:13 [Epoch 4 Batch 3200/13130] loss=0.9747, lr=0.0000078, metrics:accuracy:0.5187
INFO:root:16:04:36 [Epoch 4 Batch 3600/13130] loss=0.9677, lr=0.0000077, metrics:accuracy:0.5180
INFO:root:16:05:01 [Epoch 4 Batch 4000/13130] loss=0.9665, lr=0.0000075, metrics:accuracy:0.5182
INFO:root:16:05:26 [Epoch 4 Batch 4400/13130] loss=0.9660, lr=0.0000074, metrics:accuracy:0.5192
INFO:root:16:05:47 [Epoch 4 Batch 4800/13130] loss=0.9739, lr=0.0000073, metrics:accuracy:0.5185
INFO:root:16:06:10 [Epoch 4 Batch 5200/13130] loss=0.9862, lr=0.0000071, metrics:accuracy:0.5179
INFO:root:16:06:32 [Epoch 4 Batch 5600/13130] loss=0.9759, lr=0.0000070, metrics:accuracy:0.5180
INFO:root:16:06:54 [Epoch 4 Batch 6000/13130] loss=0.9837, lr=0.0000069, metrics:accuracy:0.5174
INFO:root:16:07:17 [Epoch 4 Batch 6400/13130] loss=0.9815, lr=0.0000067, metrics:accuracy:0.5168
INFO:root:16:07:40 [Epoch 4 Batch 6800/13130] loss=0.9646, lr=0.0000066, metrics:accuracy:0.5174
INFO:root:16:08:04 [Epoch 4 Batch 7200/13130] loss=0.9636, lr=0.0000064, metrics:accuracy:0.5176
INFO:root:16:08:26 [Epoch 4 Batch 7600/13130] loss=0.9783, lr=0.0000063, metrics:accuracy:0.5174
INFO:root:16:08:49 [Epoch 4 Batch 8000/13130] loss=0.9635, lr=0.0000062, metrics:accuracy:0.5175
INFO:root:16:09:12 [Epoch 4 Batch 8400/13130] loss=0.9645, lr=0.0000060, metrics:accuracy:0.5179
INFO:root:16:09:40 [Epoch 4 Batch 8800/13130] loss=0.9652, lr=0.0000059, metrics:accuracy:0.5183
INFO:root:16:10:08 [Epoch 4 Batch 9200/13130] loss=0.9660, lr=0.0000058, metrics:accuracy:0.5186
INFO:root:16:10:35 [Epoch 4 Batch 9600/13130] loss=0.9741, lr=0.0000056, metrics:accuracy:0.5183
INFO:root:16:11:03 [Epoch 4 Batch 10000/13130] loss=0.9738, lr=0.0000055, metrics:accuracy:0.5181
INFO:root:16:11:31 [Epoch 4 Batch 10400/13130] loss=0.9546, lr=0.0000054, metrics:accuracy:0.5186
INFO:root:16:12:00 [Epoch 4 Batch 10800/13130] loss=0.9712, lr=0.0000052, metrics:accuracy:0.5183
INFO:root:16:12:28 [Epoch 4 Batch 11200/13130] loss=0.9633, lr=0.0000051, metrics:accuracy:0.5189
INFO:root:16:12:57 [Epoch 4 Batch 11600/13130] loss=0.9677, lr=0.0000050, metrics:accuracy:0.5186
INFO:root:16:13:24 [Epoch 4 Batch 12000/13130] loss=0.9761, lr=0.0000048, metrics:accuracy:0.5184
INFO:root:16:13:52 [Epoch 4 Batch 12400/13130] loss=0.9651, lr=0.0000047, metrics:accuracy:0.5184
INFO:root:16:14:20 [Epoch 4 Batch 12800/13130] loss=0.9712, lr=0.0000045, metrics:accuracy:0.5183
INFO:root:16:14:43 Now we are doing evaluation on dev with gpu(0).
INFO:root:16:14:56 [Batch 400/3750] loss=1.0386, metrics:accuracy:0.5428
INFO:root:16:15:08 [Batch 800/3750] loss=1.0519, metrics:accuracy:0.5445
INFO:root:16:15:21 [Batch 1200/3750] loss=1.0421, metrics:accuracy:0.5430
INFO:root:16:15:33 [Batch 1600/3750] loss=1.0671, metrics:accuracy:0.5166
INFO:root:16:15:45 [Batch 2000/3750] loss=1.0492, metrics:accuracy:0.5009
INFO:root:16:15:57 [Batch 2400/3750] loss=1.0486, metrics:accuracy:0.4904
INFO:root:16:16:09 [Batch 2800/3750] loss=1.0223, metrics:accuracy:0.4851
INFO:root:16:16:21 [Batch 3200/3750] loss=1.0021, metrics:accuracy:0.4825
INFO:root:16:16:34 [Batch 3600/3750] loss=1.0043, metrics:accuracy:0.4797
INFO:root:16:16:38 validation metrics:accuracy:0.4789
INFO:root:16:16:38 Time cost=115.01s, throughput=260.84 samples/s
INFO:root:16:16:39 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:16:16:39 Time cost=945.77s
INFO:root:16:17:09 [Epoch 5 Batch 400/13130] loss=0.9320, lr=0.0000043, metrics:accuracy:0.5517
INFO:root:16:17:37 [Epoch 5 Batch 800/13130] loss=0.9521, lr=0.0000042, metrics:accuracy:0.5401
INFO:root:16:18:06 [Epoch 5 Batch 1200/13130] loss=0.9129, lr=0.0000040, metrics:accuracy:0.5461
INFO:root:16:18:35 [Epoch 5 Batch 1600/13130] loss=0.9273, lr=0.0000039, metrics:accuracy:0.5457
INFO:root:16:19:03 [Epoch 5 Batch 2000/13130] loss=0.9422, lr=0.0000038, metrics:accuracy:0.5432
INFO:root:16:19:30 [Epoch 5 Batch 2400/13130] loss=0.9468, lr=0.0000036, metrics:accuracy:0.5435
INFO:root:16:19:58 [Epoch 5 Batch 2800/13130] loss=0.9326, lr=0.0000035, metrics:accuracy:0.5419
INFO:root:16:20:26 [Epoch 5 Batch 3200/13130] loss=0.9248, lr=0.0000034, metrics:accuracy:0.5430
INFO:root:16:20:52 [Epoch 5 Batch 3600/13130] loss=0.9457, lr=0.0000032, metrics:accuracy:0.5415
INFO:root:16:21:21 [Epoch 5 Batch 4000/13130] loss=0.9451, lr=0.0000031, metrics:accuracy:0.5409
INFO:root:16:21:47 [Epoch 5 Batch 4400/13130] loss=0.9195, lr=0.0000029, metrics:accuracy:0.5428
INFO:root:16:22:11 [Epoch 5 Batch 4800/13130] loss=0.9166, lr=0.0000028, metrics:accuracy:0.5431
INFO:root:16:22:35 [Epoch 5 Batch 5200/13130] loss=0.9181, lr=0.0000027, metrics:accuracy:0.5437
INFO:root:16:22:59 [Epoch 5 Batch 5600/13130] loss=0.9455, lr=0.0000025, metrics:accuracy:0.5426
INFO:root:16:23:23 [Epoch 5 Batch 6000/13130] loss=0.9235, lr=0.0000024, metrics:accuracy:0.5434
INFO:root:16:23:47 [Epoch 5 Batch 6400/13130] loss=0.9194, lr=0.0000023, metrics:accuracy:0.5450
INFO:root:16:24:13 [Epoch 5 Batch 6800/13130] loss=0.9312, lr=0.0000021, metrics:accuracy:0.5454
INFO:root:16:24:40 [Epoch 5 Batch 7200/13130] loss=0.9236, lr=0.0000020, metrics:accuracy:0.5452
INFO:root:16:25:05 [Epoch 5 Batch 7600/13130] loss=0.9345, lr=0.0000019, metrics:accuracy:0.5446
INFO:root:16:25:28 [Epoch 5 Batch 8000/13130] loss=0.9216, lr=0.0000017, metrics:accuracy:0.5444
INFO:root:16:25:51 [Epoch 5 Batch 8400/13130] loss=0.9193, lr=0.0000016, metrics:accuracy:0.5455
INFO:root:16:26:14 [Epoch 5 Batch 8800/13130] loss=0.9468, lr=0.0000015, metrics:accuracy:0.5450
INFO:root:16:26:36 [Epoch 5 Batch 9200/13130] loss=0.9272, lr=0.0000013, metrics:accuracy:0.5449
INFO:root:16:26:58 [Epoch 5 Batch 9600/13130] loss=0.9211, lr=0.0000012, metrics:accuracy:0.5453
INFO:root:16:27:54 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:16:27:54 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:16:27:59 processing dataset...
INFO:root:16:28:24 Now we are doing BERT classification training on gpu(0)!
INFO:root:16:28:24 training steps=32812
INFO:root:16:28:48 [Epoch 1 Batch 400/13130] loss=1.1171, lr=0.0000012, metrics:accuracy:0.3417
INFO:root:16:29:12 [Epoch 1 Batch 800/13130] loss=1.0991, lr=0.0000024, metrics:accuracy:0.3458
INFO:root:17:07:23 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:07:23 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:07:27 processing dataset...
INFO:root:17:07:50 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:07:50 training steps=32812
INFO:root:17:08:12 [Epoch 1 Batch 400/13130] loss=1.1171, lr=0.0000012, metrics:accuracy:0.3417
INFO:root:17:08:33 [Epoch 1 Batch 800/13130] loss=1.0991, lr=0.0000024, metrics:accuracy:0.3458
INFO:root:17:08:54 [Epoch 1 Batch 1200/13130] loss=1.0960, lr=0.0000037, metrics:accuracy:0.3511
INFO:root:17:09:16 [Epoch 1 Batch 1600/13130] loss=1.0974, lr=0.0000049, metrics:accuracy:0.3571
INFO:root:17:09:38 [Epoch 1 Batch 2000/13130] loss=1.0971, lr=0.0000061, metrics:accuracy:0.3587
INFO:root:17:10:00 [Epoch 1 Batch 2400/13130] loss=1.0961, lr=0.0000073, metrics:accuracy:0.3589
INFO:root:17:10:21 [Epoch 1 Batch 2800/13130] loss=1.0911, lr=0.0000085, metrics:accuracy:0.3618
INFO:root:17:10:43 [Epoch 1 Batch 3200/13130] loss=1.0985, lr=0.0000097, metrics:accuracy:0.3635
INFO:root:17:11:04 [Epoch 1 Batch 3600/13130] loss=1.0967, lr=0.0000110, metrics:accuracy:0.3660
INFO:root:17:11:21 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:11:21 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:11:25 processing dataset...
INFO:root:17:11:47 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:11:47 training steps=32812
INFO:root:17:11:48 Now we are doing evaluation on dev with gpu(0).
INFO:root:17:11:59 [Batch 400/3750] loss=1.4561, metrics:accuracy:400.0000
INFO:root:17:12:10 [Batch 800/3750] loss=1.4571, metrics:accuracy:800.0000
INFO:root:17:12:21 [Batch 1200/3750] loss=1.4584, metrics:accuracy:1200.0000
INFO:root:17:12:32 [Batch 1600/3750] loss=0.8493, metrics:accuracy:1600.0000
INFO:root:17:12:44 [Batch 2000/3750] loss=0.7593, metrics:accuracy:2000.0000
INFO:root:17:12:55 [Batch 2400/3750] loss=0.7608, metrics:accuracy:2400.0000
INFO:root:17:13:06 [Batch 2800/3750] loss=1.1021, metrics:accuracy:2800.0000
INFO:root:17:13:17 [Batch 3200/3750] loss=1.2194, metrics:accuracy:3200.0000
INFO:root:17:13:28 [Batch 3600/3750] loss=1.2194, metrics:accuracy:3600.0000
INFO:root:17:13:33 validation metrics:accuracy:0.3356
INFO:root:17:13:33 Time cost=104.50s, throughput=287.07 samples/s
INFO:root:17:13:34 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:17:13:34 Time cost=106.81s
INFO:root:17:13:34 Now we are doing evaluation on dev with gpu(0).
INFO:root:17:13:46 [Batch 400/3750] loss=1.4561, metrics:accuracy:4150.0000
INFO:root:17:13:57 [Batch 800/3750] loss=1.4571, metrics:accuracy:4550.0000
INFO:root:17:14:13 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:14:13 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:14:18 processing dataset...
INFO:root:17:14:44 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:14:44 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:14:48 processing dataset...
INFO:root:17:15:13 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:15:13 training steps=32812
INFO:root:17:15:13 Now we are doing evaluation on dev with gpu(0).
INFO:root:17:15:25 [Batch 400/3750] loss=1.4561, metrics:accuracy:400.0000
INFO:root:17:15:36 [Batch 800/3750] loss=1.4571, metrics:accuracy:800.0000
INFO:root:17:16:36 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:16:36 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:16:41 processing dataset...
INFO:root:17:16:56 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:16:56 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:17:02 processing dataset...
INFO:root:17:17:28 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:17:28 training steps=32812
INFO:root:17:17:50 [Epoch 1 Batch 400/13130] loss=1.1171, lr=0.0000012, metrics:accuracy:0.3417
INFO:root:17:18:13 [Epoch 1 Batch 800/13130] loss=1.0991, lr=0.0000024, metrics:accuracy:0.3458
INFO:root:17:18:36 [Epoch 1 Batch 1200/13130] loss=1.0960, lr=0.0000037, metrics:accuracy:0.3511
INFO:root:17:18:59 [Epoch 1 Batch 1600/13130] loss=1.0974, lr=0.0000049, metrics:accuracy:0.3571
INFO:root:17:19:21 [Epoch 1 Batch 2000/13130] loss=1.0971, lr=0.0000061, metrics:accuracy:0.3587
INFO:root:17:19:44 [Epoch 1 Batch 2400/13130] loss=1.0961, lr=0.0000073, metrics:accuracy:0.3589
INFO:root:17:20:06 [Epoch 1 Batch 2800/13130] loss=1.0911, lr=0.0000085, metrics:accuracy:0.3618
INFO:root:17:20:28 [Epoch 1 Batch 3200/13130] loss=1.0985, lr=0.0000097, metrics:accuracy:0.3635
INFO:root:17:20:49 [Epoch 1 Batch 3600/13130] loss=1.0967, lr=0.0000110, metrics:accuracy:0.3660
INFO:root:17:21:12 [Epoch 1 Batch 4000/13130] loss=1.0913, lr=0.0000122, metrics:accuracy:0.3678
INFO:root:17:21:34 [Epoch 1 Batch 4400/13130] loss=1.0914, lr=0.0000134, metrics:accuracy:0.3690
INFO:root:17:21:56 [Epoch 1 Batch 4800/13130] loss=1.0887, lr=0.0000146, metrics:accuracy:0.3700
INFO:root:17:22:18 [Epoch 1 Batch 5200/13130] loss=1.0953, lr=0.0000158, metrics:accuracy:0.3711
INFO:root:17:22:41 [Epoch 1 Batch 5600/13130] loss=1.0849, lr=0.0000171, metrics:accuracy:0.3736
INFO:root:17:23:04 [Epoch 1 Batch 6000/13130] loss=1.0848, lr=0.0000183, metrics:accuracy:0.3754
INFO:root:17:23:27 [Epoch 1 Batch 6400/13130] loss=1.0897, lr=0.0000195, metrics:accuracy:0.3764
INFO:root:17:23:50 [Epoch 1 Batch 6800/13130] loss=1.0817, lr=0.0000199, metrics:accuracy:0.3783
INFO:root:17:24:12 [Epoch 1 Batch 7200/13130] loss=1.0828, lr=0.0000198, metrics:accuracy:0.3799
INFO:root:17:24:35 [Epoch 1 Batch 7600/13130] loss=1.0814, lr=0.0000196, metrics:accuracy:0.3810
INFO:root:17:24:57 [Epoch 1 Batch 8000/13130] loss=1.0869, lr=0.0000195, metrics:accuracy:0.3817
INFO:root:17:25:19 [Epoch 1 Batch 8400/13130] loss=1.0861, lr=0.0000194, metrics:accuracy:0.3821
INFO:root:17:25:41 [Epoch 1 Batch 8800/13130] loss=1.0771, lr=0.0000192, metrics:accuracy:0.3836
INFO:root:17:26:03 [Epoch 1 Batch 9200/13130] loss=1.0681, lr=0.0000191, metrics:accuracy:0.3857
INFO:root:17:26:25 [Epoch 1 Batch 9600/13130] loss=1.0690, lr=0.0000190, metrics:accuracy:0.3873
INFO:root:17:26:47 [Epoch 1 Batch 10000/13130] loss=1.0743, lr=0.0000188, metrics:accuracy:0.3883
INFO:root:17:27:10 [Epoch 1 Batch 10400/13130] loss=1.0600, lr=0.0000187, metrics:accuracy:0.3901
INFO:root:17:27:31 [Epoch 1 Batch 10800/13130] loss=1.0616, lr=0.0000186, metrics:accuracy:0.3921
INFO:root:17:27:53 [Epoch 1 Batch 11200/13130] loss=1.0743, lr=0.0000184, metrics:accuracy:0.3931
INFO:root:17:28:15 [Epoch 1 Batch 11600/13130] loss=1.0630, lr=0.0000183, metrics:accuracy:0.3945
INFO:root:17:28:36 [Epoch 1 Batch 12000/13130] loss=1.0732, lr=0.0000182, metrics:accuracy:0.3953
INFO:root:17:28:58 [Epoch 1 Batch 12400/13130] loss=1.0641, lr=0.0000180, metrics:accuracy:0.3966
INFO:root:17:29:20 [Epoch 1 Batch 12800/13130] loss=1.0665, lr=0.0000179, metrics:accuracy:0.3977
INFO:root:17:29:37 Now we are doing evaluation on dev with gpu(0).
INFO:root:17:29:48 [Batch 400/3750] loss=1.1066, metrics:accuracy:400.0000
INFO:root:17:29:59 [Batch 800/3750] loss=1.1112, metrics:accuracy:800.0000
INFO:root:17:30:10 [Batch 1200/3750] loss=1.1097, metrics:accuracy:1200.0000
INFO:root:17:30:21 [Batch 1600/3750] loss=1.1550, metrics:accuracy:1600.0000
INFO:root:17:30:32 [Batch 2000/3750] loss=1.1583, metrics:accuracy:2000.0000
INFO:root:17:30:43 [Batch 2400/3750] loss=1.1478, metrics:accuracy:2400.0000
INFO:root:17:30:55 [Batch 2800/3750] loss=0.9681, metrics:accuracy:2800.0000
INFO:root:17:31:06 [Batch 3200/3750] loss=0.8997, metrics:accuracy:3200.0000
INFO:root:17:31:17 [Batch 3600/3750] loss=0.8986, metrics:accuracy:3600.0000
INFO:root:17:31:21 validation metrics:accuracy:0.4440
INFO:root:17:31:21 Time cost=103.65s, throughput=289.43 samples/s
INFO:root:17:31:23 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:17:31:23 Time cost=834.98s
INFO:root:17:31:44 [Epoch 2 Batch 400/13130] loss=1.0514, lr=0.0000176, metrics:accuracy:0.4528
INFO:root:17:32:06 [Epoch 2 Batch 800/13130] loss=1.0555, lr=0.0000175, metrics:accuracy:0.4503
INFO:root:17:32:28 [Epoch 2 Batch 1200/13130] loss=1.0448, lr=0.0000174, metrics:accuracy:0.4517
INFO:root:17:32:49 [Epoch 2 Batch 1600/13130] loss=1.0406, lr=0.0000172, metrics:accuracy:0.4554
INFO:root:17:33:10 [Epoch 2 Batch 2000/13130] loss=1.0369, lr=0.0000171, metrics:accuracy:0.4569
INFO:root:17:33:32 [Epoch 2 Batch 2400/13130] loss=1.0409, lr=0.0000170, metrics:accuracy:0.4562
INFO:root:17:33:53 [Epoch 2 Batch 2800/13130] loss=1.0448, lr=0.0000168, metrics:accuracy:0.4567
INFO:root:17:34:15 [Epoch 2 Batch 3200/13130] loss=1.0497, lr=0.0000167, metrics:accuracy:0.4543
INFO:root:17:34:36 [Epoch 2 Batch 3600/13130] loss=1.0445, lr=0.0000166, metrics:accuracy:0.4540
INFO:root:17:34:58 [Epoch 2 Batch 4000/13130] loss=1.0423, lr=0.0000164, metrics:accuracy:0.4539
INFO:root:17:35:19 [Epoch 2 Batch 4400/13130] loss=1.0517, lr=0.0000163, metrics:accuracy:0.4544
INFO:root:17:35:41 [Epoch 2 Batch 4800/13130] loss=1.0392, lr=0.0000162, metrics:accuracy:0.4546
INFO:root:17:36:03 [Epoch 2 Batch 5200/13130] loss=1.0510, lr=0.0000160, metrics:accuracy:0.4543
INFO:root:17:36:24 [Epoch 2 Batch 5600/13130] loss=1.0423, lr=0.0000159, metrics:accuracy:0.4543
INFO:root:17:36:46 [Epoch 2 Batch 6000/13130] loss=1.0506, lr=0.0000157, metrics:accuracy:0.4552
INFO:root:17:37:08 [Epoch 2 Batch 6400/13130] loss=1.0242, lr=0.0000156, metrics:accuracy:0.4568
INFO:root:17:37:31 [Epoch 2 Batch 6800/13130] loss=1.0431, lr=0.0000155, metrics:accuracy:0.4567
INFO:root:17:37:53 [Epoch 2 Batch 7200/13130] loss=1.0590, lr=0.0000153, metrics:accuracy:0.4559
INFO:root:17:38:15 [Epoch 2 Batch 7600/13130] loss=1.0378, lr=0.0000152, metrics:accuracy:0.4570
INFO:root:17:38:36 [Epoch 2 Batch 8000/13130] loss=1.0429, lr=0.0000151, metrics:accuracy:0.4567
INFO:root:17:38:58 [Epoch 2 Batch 8400/13130] loss=1.0382, lr=0.0000149, metrics:accuracy:0.4573
INFO:root:17:39:20 [Epoch 2 Batch 8800/13130] loss=1.0539, lr=0.0000148, metrics:accuracy:0.4561
INFO:root:17:39:42 [Epoch 2 Batch 9200/13130] loss=1.0451, lr=0.0000147, metrics:accuracy:0.4562
INFO:root:17:40:03 [Epoch 2 Batch 9600/13130] loss=1.0327, lr=0.0000145, metrics:accuracy:0.4564
INFO:root:17:40:25 [Epoch 2 Batch 10000/13130] loss=1.0446, lr=0.0000144, metrics:accuracy:0.4567
INFO:root:17:40:47 [Epoch 2 Batch 10400/13130] loss=1.0309, lr=0.0000143, metrics:accuracy:0.4572
INFO:root:17:41:08 [Epoch 2 Batch 10800/13130] loss=1.0414, lr=0.0000141, metrics:accuracy:0.4571
INFO:root:17:41:29 [Epoch 2 Batch 11200/13130] loss=1.0263, lr=0.0000140, metrics:accuracy:0.4577
INFO:root:17:41:52 [Epoch 2 Batch 11600/13130] loss=1.0358, lr=0.0000138, metrics:accuracy:0.4581
INFO:root:17:42:14 [Epoch 2 Batch 12000/13130] loss=1.0404, lr=0.0000137, metrics:accuracy:0.4583
INFO:root:17:42:35 [Epoch 2 Batch 12400/13130] loss=1.0396, lr=0.0000136, metrics:accuracy:0.4584
INFO:root:17:42:57 [Epoch 2 Batch 12800/13130] loss=1.0171, lr=0.0000134, metrics:accuracy:0.4594
INFO:root:17:43:15 Now we are doing evaluation on dev with gpu(0).
INFO:root:17:43:26 [Batch 400/3750] loss=1.0040, metrics:accuracy:4150.0000
INFO:root:17:43:37 [Batch 800/3750] loss=1.0078, metrics:accuracy:4550.0000
INFO:root:17:43:48 [Batch 1200/3750] loss=1.0044, metrics:accuracy:4950.0000
INFO:root:17:43:59 [Batch 1600/3750] loss=1.1331, metrics:accuracy:5350.0000
INFO:root:17:44:10 [Batch 2000/3750] loss=1.1448, metrics:accuracy:5750.0000
INFO:root:17:44:21 [Batch 2400/3750] loss=1.1318, metrics:accuracy:6150.0000
INFO:root:17:44:32 [Batch 2800/3750] loss=1.0089, metrics:accuracy:6550.0000
INFO:root:17:44:43 [Batch 3200/3750] loss=0.9488, metrics:accuracy:6950.0000
INFO:root:17:44:54 [Batch 3600/3750] loss=0.9517, metrics:accuracy:7350.0000
INFO:root:17:44:58 validation metrics:accuracy:0.4668
INFO:root:17:44:58 Time cost=103.22s, throughput=290.65 samples/s
INFO:root:17:45:00 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:17:45:00 Time cost=817.09s
INFO:root:17:45:22 [Epoch 3 Batch 400/13130] loss=1.0111, lr=0.0000132, metrics:accuracy:0.4691
INFO:root:17:45:43 [Epoch 3 Batch 800/13130] loss=1.0127, lr=0.0000131, metrics:accuracy:0.4779
INFO:root:17:46:04 [Epoch 3 Batch 1200/13130] loss=1.0190, lr=0.0000129, metrics:accuracy:0.4801
INFO:root:17:46:26 [Epoch 3 Batch 1600/13130] loss=1.0073, lr=0.0000128, metrics:accuracy:0.4806
INFO:root:17:46:49 [Epoch 3 Batch 2000/13130] loss=1.0080, lr=0.0000127, metrics:accuracy:0.4816
INFO:root:17:47:28 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:47:28 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:47:33 processing dataset...
INFO:root:17:48:27 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:17:48:27 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:48:32 processing dataset...
INFO:root:17:48:55 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:48:55 training steps=32812
INFO:root:17:49:18 [Epoch 1 Batch 400/13130] loss=1.1171, lr=0.0000012, metrics:accuracy:0.3417
INFO:root:17:49:40 [Epoch 1 Batch 800/13130] loss=1.0991, lr=0.0000024, metrics:accuracy:0.3458
INFO:root:17:50:03 [Epoch 1 Batch 1200/13130] loss=1.0960, lr=0.0000037, metrics:accuracy:0.3511
INFO:root:17:50:26 [Epoch 1 Batch 1600/13130] loss=1.0974, lr=0.0000049, metrics:accuracy:0.3571
INFO:root:17:50:50 [Epoch 1 Batch 2000/13130] loss=1.0971, lr=0.0000061, metrics:accuracy:0.3587
INFO:root:17:51:12 [Epoch 1 Batch 2400/13130] loss=1.0961, lr=0.0000073, metrics:accuracy:0.3589
INFO:root:17:51:35 [Epoch 1 Batch 2800/13130] loss=1.0911, lr=0.0000085, metrics:accuracy:0.3618
INFO:root:17:51:57 [Epoch 1 Batch 3200/13130] loss=1.0985, lr=0.0000097, metrics:accuracy:0.3635
INFO:root:17:52:20 [Epoch 1 Batch 3600/13130] loss=1.0967, lr=0.0000110, metrics:accuracy:0.3660
INFO:root:17:52:44 [Epoch 1 Batch 4000/13130] loss=1.0913, lr=0.0000122, metrics:accuracy:0.3678
INFO:root:17:53:07 [Epoch 1 Batch 4400/13130] loss=1.0914, lr=0.0000134, metrics:accuracy:0.3690
INFO:root:17:53:30 [Epoch 1 Batch 4800/13130] loss=1.0887, lr=0.0000146, metrics:accuracy:0.3700
INFO:root:17:53:52 [Epoch 1 Batch 5200/13130] loss=1.0953, lr=0.0000158, metrics:accuracy:0.3711
INFO:root:17:54:16 [Epoch 1 Batch 5600/13130] loss=1.0849, lr=0.0000171, metrics:accuracy:0.3736
INFO:root:17:54:39 [Epoch 1 Batch 6000/13130] loss=1.0848, lr=0.0000183, metrics:accuracy:0.3754
INFO:root:17:55:02 [Epoch 1 Batch 6400/13130] loss=1.0897, lr=0.0000195, metrics:accuracy:0.3764
INFO:root:17:55:25 [Epoch 1 Batch 6800/13130] loss=1.0817, lr=0.0000199, metrics:accuracy:0.3783
INFO:root:17:55:47 [Epoch 1 Batch 7200/13130] loss=1.0828, lr=0.0000198, metrics:accuracy:0.3799
INFO:root:17:56:10 [Epoch 1 Batch 7600/13130] loss=1.0814, lr=0.0000196, metrics:accuracy:0.3810
INFO:root:17:56:33 [Epoch 1 Batch 8000/13130] loss=1.0869, lr=0.0000195, metrics:accuracy:0.3817
INFO:root:17:56:56 [Epoch 1 Batch 8400/13130] loss=1.0861, lr=0.0000194, metrics:accuracy:0.3821
INFO:root:17:57:19 [Epoch 1 Batch 8800/13130] loss=1.0771, lr=0.0000192, metrics:accuracy:0.3836
INFO:root:17:57:42 [Epoch 1 Batch 9200/13130] loss=1.0681, lr=0.0000191, metrics:accuracy:0.3857
INFO:root:17:58:04 [Epoch 1 Batch 9600/13130] loss=1.0690, lr=0.0000190, metrics:accuracy:0.3873
INFO:root:17:58:27 [Epoch 1 Batch 10000/13130] loss=1.0743, lr=0.0000188, metrics:accuracy:0.3883
INFO:root:17:58:49 [Epoch 1 Batch 10400/13130] loss=1.0600, lr=0.0000187, metrics:accuracy:0.3901
INFO:root:17:59:11 [Epoch 1 Batch 10800/13130] loss=1.0616, lr=0.0000186, metrics:accuracy:0.3921
INFO:root:17:59:33 [Epoch 1 Batch 11200/13130] loss=1.0743, lr=0.0000184, metrics:accuracy:0.3931
INFO:root:17:59:56 [Epoch 1 Batch 11600/13130] loss=1.0630, lr=0.0000183, metrics:accuracy:0.3945
INFO:root:18:00:18 [Epoch 1 Batch 12000/13130] loss=1.0732, lr=0.0000182, metrics:accuracy:0.3953
INFO:root:18:00:40 [Epoch 1 Batch 12400/13130] loss=1.0641, lr=0.0000180, metrics:accuracy:0.3966
INFO:root:18:01:03 [Epoch 1 Batch 12800/13130] loss=1.0665, lr=0.0000179, metrics:accuracy:0.3977
INFO:root:18:01:21 Now we are doing evaluation on dev with gpu(0).
INFO:root:18:01:32 [Batch 400/3750] loss=1.1066, metrics:accuracy:400.0000
INFO:root:18:01:44 [Batch 800/3750] loss=1.1112, metrics:accuracy:800.0000
INFO:root:18:01:55 [Batch 1200/3750] loss=1.1097, metrics:accuracy:1200.0000
INFO:root:18:02:07 [Batch 1600/3750] loss=1.1550, metrics:accuracy:1600.0000
INFO:root:18:02:19 [Batch 2000/3750] loss=1.1583, metrics:accuracy:2000.0000
INFO:root:18:02:30 [Batch 2400/3750] loss=1.1478, metrics:accuracy:2400.0000
INFO:root:18:02:42 [Batch 2800/3750] loss=0.9681, metrics:accuracy:2800.0000
INFO:root:18:02:54 [Batch 3200/3750] loss=0.8997, metrics:accuracy:3200.0000
INFO:root:18:03:06 [Batch 3600/3750] loss=0.8986, metrics:accuracy:3600.0000
INFO:root:18:03:10 validation metrics:accuracy:0.4440
INFO:root:18:03:10 Time cost=108.98s, throughput=275.29 samples/s
INFO:root:18:03:11 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:18:03:11 Time cost=855.42s
INFO:root:18:03:33 [Epoch 2 Batch 400/13130] loss=1.0514, lr=0.0000176, metrics:accuracy:0.4528
INFO:root:18:03:55 [Epoch 2 Batch 800/13130] loss=1.0555, lr=0.0000175, metrics:accuracy:0.4503
INFO:root:18:04:18 [Epoch 2 Batch 1200/13130] loss=1.0448, lr=0.0000174, metrics:accuracy:0.4517
INFO:root:18:04:40 [Epoch 2 Batch 1600/13130] loss=1.0406, lr=0.0000172, metrics:accuracy:0.4554
INFO:root:18:05:02 [Epoch 2 Batch 2000/13130] loss=1.0369, lr=0.0000171, metrics:accuracy:0.4569
INFO:root:18:05:24 [Epoch 2 Batch 2400/13130] loss=1.0409, lr=0.0000170, metrics:accuracy:0.4562
INFO:root:18:05:47 [Epoch 2 Batch 2800/13130] loss=1.0448, lr=0.0000168, metrics:accuracy:0.4567
INFO:root:18:06:09 [Epoch 2 Batch 3200/13130] loss=1.0497, lr=0.0000167, metrics:accuracy:0.4543
INFO:root:18:06:31 [Epoch 2 Batch 3600/13130] loss=1.0445, lr=0.0000166, metrics:accuracy:0.4540
INFO:root:18:06:53 [Epoch 2 Batch 4000/13130] loss=1.0423, lr=0.0000164, metrics:accuracy:0.4539
INFO:root:18:07:15 [Epoch 2 Batch 4400/13130] loss=1.0517, lr=0.0000163, metrics:accuracy:0.4544
INFO:root:18:07:38 [Epoch 2 Batch 4800/13130] loss=1.0392, lr=0.0000162, metrics:accuracy:0.4546
INFO:root:18:08:00 [Epoch 2 Batch 5200/13130] loss=1.0510, lr=0.0000160, metrics:accuracy:0.4543
INFO:root:18:08:22 [Epoch 2 Batch 5600/13130] loss=1.0423, lr=0.0000159, metrics:accuracy:0.4543
INFO:root:18:08:45 [Epoch 2 Batch 6000/13130] loss=1.0506, lr=0.0000157, metrics:accuracy:0.4552
INFO:root:18:09:07 [Epoch 2 Batch 6400/13130] loss=1.0242, lr=0.0000156, metrics:accuracy:0.4568
INFO:root:18:09:30 [Epoch 2 Batch 6800/13130] loss=1.0431, lr=0.0000155, metrics:accuracy:0.4567
INFO:root:18:09:52 [Epoch 2 Batch 7200/13130] loss=1.0590, lr=0.0000153, metrics:accuracy:0.4559
INFO:root:18:10:15 [Epoch 2 Batch 7600/13130] loss=1.0378, lr=0.0000152, metrics:accuracy:0.4570
INFO:root:18:10:37 [Epoch 2 Batch 8000/13130] loss=1.0429, lr=0.0000151, metrics:accuracy:0.4567
INFO:root:18:11:01 [Epoch 2 Batch 8400/13130] loss=1.0382, lr=0.0000149, metrics:accuracy:0.4573
INFO:root:18:11:24 [Epoch 2 Batch 8800/13130] loss=1.0539, lr=0.0000148, metrics:accuracy:0.4561
INFO:root:18:11:47 [Epoch 2 Batch 9200/13130] loss=1.0451, lr=0.0000147, metrics:accuracy:0.4562
INFO:root:18:12:10 [Epoch 2 Batch 9600/13130] loss=1.0327, lr=0.0000145, metrics:accuracy:0.4564
INFO:root:18:12:34 [Epoch 2 Batch 10000/13130] loss=1.0446, lr=0.0000144, metrics:accuracy:0.4567
INFO:root:18:12:57 [Epoch 2 Batch 10400/13130] loss=1.0309, lr=0.0000143, metrics:accuracy:0.4572
INFO:root:18:13:19 [Epoch 2 Batch 10800/13130] loss=1.0414, lr=0.0000141, metrics:accuracy:0.4571
INFO:root:18:13:41 [Epoch 2 Batch 11200/13130] loss=1.0263, lr=0.0000140, metrics:accuracy:0.4577
INFO:root:18:14:04 [Epoch 2 Batch 11600/13130] loss=1.0358, lr=0.0000138, metrics:accuracy:0.4581
INFO:root:18:14:26 [Epoch 2 Batch 12000/13130] loss=1.0404, lr=0.0000137, metrics:accuracy:0.4583
INFO:root:18:14:48 [Epoch 2 Batch 12400/13130] loss=1.0396, lr=0.0000136, metrics:accuracy:0.4584
INFO:root:18:15:10 [Epoch 2 Batch 12800/13130] loss=1.0171, lr=0.0000134, metrics:accuracy:0.4594
INFO:root:18:15:29 Now we are doing evaluation on dev with gpu(0).
INFO:root:18:15:41 [Batch 400/3750] loss=1.0040, metrics:accuracy:4150.0000
INFO:root:18:15:52 [Batch 800/3750] loss=1.0078, metrics:accuracy:4550.0000
INFO:root:18:16:03 [Batch 1200/3750] loss=1.0044, metrics:accuracy:4950.0000
INFO:root:18:16:15 [Batch 1600/3750] loss=1.1331, metrics:accuracy:5350.0000
INFO:root:18:16:26 [Batch 2000/3750] loss=1.1448, metrics:accuracy:5750.0000
INFO:root:18:16:37 [Batch 2400/3750] loss=1.1318, metrics:accuracy:6150.0000
INFO:root:18:16:49 [Batch 2800/3750] loss=1.0089, metrics:accuracy:6550.0000
INFO:root:18:17:00 [Batch 3200/3750] loss=0.9488, metrics:accuracy:6950.0000
INFO:root:18:17:11 [Batch 3600/3750] loss=0.9517, metrics:accuracy:7350.0000
INFO:root:18:17:15 validation metrics:accuracy:0.4668
INFO:root:18:17:15 Time cost=106.20s, throughput=282.48 samples/s
INFO:root:18:17:16 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:18:17:16 Time cost=844.98s
INFO:root:18:17:38 [Epoch 3 Batch 400/13130] loss=1.0111, lr=0.0000132, metrics:accuracy:0.4691
INFO:root:18:17:59 [Epoch 3 Batch 800/13130] loss=1.0127, lr=0.0000131, metrics:accuracy:0.4779
INFO:root:18:18:20 [Epoch 3 Batch 1200/13130] loss=1.0190, lr=0.0000129, metrics:accuracy:0.4801
INFO:root:18:18:42 [Epoch 3 Batch 1600/13130] loss=1.0073, lr=0.0000128, metrics:accuracy:0.4806
INFO:root:18:19:04 [Epoch 3 Batch 2000/13130] loss=1.0080, lr=0.0000127, metrics:accuracy:0.4816
INFO:root:18:19:25 [Epoch 3 Batch 2400/13130] loss=1.0271, lr=0.0000125, metrics:accuracy:0.4809
INFO:root:18:19:47 [Epoch 3 Batch 2800/13130] loss=1.0048, lr=0.0000124, metrics:accuracy:0.4835
INFO:root:18:20:08 [Epoch 3 Batch 3200/13130] loss=1.0014, lr=0.0000122, metrics:accuracy:0.4859
INFO:root:18:20:30 [Epoch 3 Batch 3600/13130] loss=1.0052, lr=0.0000121, metrics:accuracy:0.4850
INFO:root:18:20:51 [Epoch 3 Batch 4000/13130] loss=1.0063, lr=0.0000120, metrics:accuracy:0.4863
INFO:root:18:21:12 [Epoch 3 Batch 4400/13130] loss=1.0226, lr=0.0000118, metrics:accuracy:0.4849
INFO:root:18:21:34 [Epoch 3 Batch 4800/13130] loss=1.0126, lr=0.0000117, metrics:accuracy:0.4856
INFO:root:18:21:55 [Epoch 3 Batch 5200/13130] loss=1.0303, lr=0.0000116, metrics:accuracy:0.4842
INFO:root:18:22:17 [Epoch 3 Batch 5600/13130] loss=1.0062, lr=0.0000114, metrics:accuracy:0.4846
INFO:root:18:22:41 [Epoch 3 Batch 6000/13130] loss=1.0130, lr=0.0000113, metrics:accuracy:0.4848
INFO:root:18:23:03 [Epoch 3 Batch 6400/13130] loss=1.0204, lr=0.0000112, metrics:accuracy:0.4843
INFO:root:18:23:26 [Epoch 3 Batch 6800/13130] loss=0.9943, lr=0.0000110, metrics:accuracy:0.4852
INFO:root:18:23:49 [Epoch 3 Batch 7200/13130] loss=1.0277, lr=0.0000109, metrics:accuracy:0.4847
INFO:root:18:24:11 [Epoch 3 Batch 7600/13130] loss=1.0140, lr=0.0000108, metrics:accuracy:0.4845
INFO:root:18:24:34 [Epoch 3 Batch 8000/13130] loss=1.0118, lr=0.0000106, metrics:accuracy:0.4847
INFO:root:18:24:56 [Epoch 3 Batch 8400/13130] loss=0.9999, lr=0.0000105, metrics:accuracy:0.4852
INFO:root:18:25:18 [Epoch 3 Batch 8800/13130] loss=1.0056, lr=0.0000104, metrics:accuracy:0.4852
INFO:root:18:25:40 [Epoch 3 Batch 9200/13130] loss=1.0217, lr=0.0000102, metrics:accuracy:0.4847
INFO:root:18:26:02 [Epoch 3 Batch 9600/13130] loss=1.0102, lr=0.0000101, metrics:accuracy:0.4850
INFO:root:18:26:24 [Epoch 3 Batch 10000/13130] loss=1.0056, lr=0.0000099, metrics:accuracy:0.4850
INFO:root:18:26:47 [Epoch 3 Batch 10400/13130] loss=0.9990, lr=0.0000098, metrics:accuracy:0.4855
INFO:root:18:27:09 [Epoch 3 Batch 10800/13130] loss=1.0091, lr=0.0000097, metrics:accuracy:0.4856
INFO:root:18:27:30 [Epoch 3 Batch 11200/13130] loss=1.0104, lr=0.0000095, metrics:accuracy:0.4857
INFO:root:18:27:53 [Epoch 3 Batch 11600/13130] loss=1.0038, lr=0.0000094, metrics:accuracy:0.4859
INFO:root:18:28:15 [Epoch 3 Batch 12000/13130] loss=0.9970, lr=0.0000093, metrics:accuracy:0.4864
INFO:root:18:28:37 [Epoch 3 Batch 12400/13130] loss=0.9918, lr=0.0000091, metrics:accuracy:0.4869
INFO:root:18:28:59 [Epoch 3 Batch 12800/13130] loss=0.9792, lr=0.0000090, metrics:accuracy:0.4879
INFO:root:18:29:17 Now we are doing evaluation on dev with gpu(0).
INFO:root:18:29:28 [Batch 400/3750] loss=0.9812, metrics:accuracy:7900.0000
INFO:root:18:29:39 [Batch 800/3750] loss=0.9888, metrics:accuracy:8300.0000
INFO:root:18:29:50 [Batch 1200/3750] loss=0.9819, metrics:accuracy:8700.0000
INFO:root:18:30:01 [Batch 1600/3750] loss=1.1361, metrics:accuracy:9100.0000
INFO:root:18:30:12 [Batch 2000/3750] loss=1.1435, metrics:accuracy:9500.0000
INFO:root:18:30:23 [Batch 2400/3750] loss=1.1378, metrics:accuracy:9900.0000
INFO:root:18:30:35 [Batch 2800/3750] loss=1.0162, metrics:accuracy:10300.0000
INFO:root:18:30:46 [Batch 3200/3750] loss=0.9621, metrics:accuracy:10700.0000
INFO:root:18:30:57 [Batch 3600/3750] loss=0.9616, metrics:accuracy:11100.0000
INFO:root:18:31:01 validation metrics:accuracy:0.4729
INFO:root:18:31:01 Time cost=104.44s, throughput=287.24 samples/s
INFO:root:18:31:02 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:18:31:02 Time cost=826.30s
INFO:root:18:31:25 [Epoch 4 Batch 400/13130] loss=0.9760, lr=0.0000087, metrics:accuracy:0.5192
INFO:root:18:31:47 [Epoch 4 Batch 800/13130] loss=0.9784, lr=0.0000086, metrics:accuracy:0.5145
INFO:root:18:32:10 [Epoch 4 Batch 1200/13130] loss=0.9748, lr=0.0000085, metrics:accuracy:0.5172
INFO:root:18:32:32 [Epoch 4 Batch 1600/13130] loss=0.9696, lr=0.0000083, metrics:accuracy:0.5174
INFO:root:18:32:55 [Epoch 4 Batch 2000/13130] loss=0.9564, lr=0.0000082, metrics:accuracy:0.5188
INFO:root:18:33:18 [Epoch 4 Batch 2400/13130] loss=0.9754, lr=0.0000081, metrics:accuracy:0.5186
INFO:root:18:33:40 [Epoch 4 Batch 2800/13130] loss=0.9643, lr=0.0000079, metrics:accuracy:0.5189
INFO:root:18:34:01 [Epoch 4 Batch 3200/13130] loss=0.9747, lr=0.0000078, metrics:accuracy:0.5187
INFO:root:18:34:23 [Epoch 4 Batch 3600/13130] loss=0.9677, lr=0.0000077, metrics:accuracy:0.5180
INFO:root:18:34:45 [Epoch 4 Batch 4000/13130] loss=0.9665, lr=0.0000075, metrics:accuracy:0.5182
INFO:root:18:35:07 [Epoch 4 Batch 4400/13130] loss=0.9660, lr=0.0000074, metrics:accuracy:0.5192
INFO:root:18:35:28 [Epoch 4 Batch 4800/13130] loss=0.9739, lr=0.0000073, metrics:accuracy:0.5185
INFO:root:18:35:50 [Epoch 4 Batch 5200/13130] loss=0.9862, lr=0.0000071, metrics:accuracy:0.5179
INFO:root:18:36:11 [Epoch 4 Batch 5600/13130] loss=0.9759, lr=0.0000070, metrics:accuracy:0.5180
INFO:root:18:36:33 [Epoch 4 Batch 6000/13130] loss=0.9837, lr=0.0000069, metrics:accuracy:0.5174
INFO:root:18:36:54 [Epoch 4 Batch 6400/13130] loss=0.9815, lr=0.0000067, metrics:accuracy:0.5168
INFO:root:18:37:16 [Epoch 4 Batch 6800/13130] loss=0.9646, lr=0.0000066, metrics:accuracy:0.5174
INFO:root:18:37:37 [Epoch 4 Batch 7200/13130] loss=0.9636, lr=0.0000064, metrics:accuracy:0.5176
INFO:root:18:37:58 [Epoch 4 Batch 7600/13130] loss=0.9783, lr=0.0000063, metrics:accuracy:0.5174
INFO:root:18:38:20 [Epoch 4 Batch 8000/13130] loss=0.9635, lr=0.0000062, metrics:accuracy:0.5175
INFO:root:18:38:42 [Epoch 4 Batch 8400/13130] loss=0.9645, lr=0.0000060, metrics:accuracy:0.5179
INFO:root:18:39:03 [Epoch 4 Batch 8800/13130] loss=0.9652, lr=0.0000059, metrics:accuracy:0.5183
INFO:root:18:39:25 [Epoch 4 Batch 9200/13130] loss=0.9660, lr=0.0000058, metrics:accuracy:0.5186
INFO:root:18:39:47 [Epoch 4 Batch 9600/13130] loss=0.9741, lr=0.0000056, metrics:accuracy:0.5183
INFO:root:18:40:08 [Epoch 4 Batch 10000/13130] loss=0.9738, lr=0.0000055, metrics:accuracy:0.5181
INFO:root:18:40:30 [Epoch 4 Batch 10400/13130] loss=0.9546, lr=0.0000054, metrics:accuracy:0.5186
INFO:root:18:40:52 [Epoch 4 Batch 10800/13130] loss=0.9712, lr=0.0000052, metrics:accuracy:0.5183
INFO:root:18:41:13 [Epoch 4 Batch 11200/13130] loss=0.9633, lr=0.0000051, metrics:accuracy:0.5189
INFO:root:18:41:35 [Epoch 4 Batch 11600/13130] loss=0.9677, lr=0.0000050, metrics:accuracy:0.5186
INFO:root:18:41:56 [Epoch 4 Batch 12000/13130] loss=0.9761, lr=0.0000048, metrics:accuracy:0.5184
INFO:root:18:42:18 [Epoch 4 Batch 12400/13130] loss=0.9651, lr=0.0000047, metrics:accuracy:0.5184
INFO:root:18:42:40 [Epoch 4 Batch 12800/13130] loss=0.9712, lr=0.0000045, metrics:accuracy:0.5183
INFO:root:18:42:58 Now we are doing evaluation on dev with gpu(0).
INFO:root:18:43:09 [Batch 400/3750] loss=1.0386, metrics:accuracy:11650.0000
INFO:root:18:43:20 [Batch 800/3750] loss=1.0519, metrics:accuracy:12050.0000
INFO:root:18:43:31 [Batch 1200/3750] loss=1.0421, metrics:accuracy:12450.0000
INFO:root:18:43:42 [Batch 1600/3750] loss=1.0671, metrics:accuracy:12850.0000
INFO:root:18:43:53 [Batch 2000/3750] loss=1.0492, metrics:accuracy:13250.0000
INFO:root:18:44:05 [Batch 2400/3750] loss=1.0486, metrics:accuracy:13650.0000
INFO:root:18:44:16 [Batch 2800/3750] loss=1.0223, metrics:accuracy:14050.0000
INFO:root:18:44:27 [Batch 3200/3750] loss=1.0021, metrics:accuracy:14450.0000
INFO:root:18:44:38 [Batch 3600/3750] loss=1.0043, metrics:accuracy:14850.0000
INFO:root:18:44:42 validation metrics:accuracy:0.4789
INFO:root:18:44:42 Time cost=104.12s, throughput=288.13 samples/s
INFO:root:18:44:44 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:18:44:44 Time cost=821.72s
INFO:root:18:45:06 [Epoch 5 Batch 400/13130] loss=0.9320, lr=0.0000043, metrics:accuracy:0.5517
INFO:root:18:45:30 [Epoch 5 Batch 800/13130] loss=0.9521, lr=0.0000042, metrics:accuracy:0.5401
INFO:root:18:45:52 [Epoch 5 Batch 1200/13130] loss=0.9129, lr=0.0000040, metrics:accuracy:0.5461
INFO:root:18:46:15 [Epoch 5 Batch 1600/13130] loss=0.9273, lr=0.0000039, metrics:accuracy:0.5457
INFO:root:18:46:37 [Epoch 5 Batch 2000/13130] loss=0.9422, lr=0.0000038, metrics:accuracy:0.5432
INFO:root:18:46:59 [Epoch 5 Batch 2400/13130] loss=0.9468, lr=0.0000036, metrics:accuracy:0.5435
INFO:root:18:47:22 [Epoch 5 Batch 2800/13130] loss=0.9326, lr=0.0000035, metrics:accuracy:0.5419
INFO:root:18:47:43 [Epoch 5 Batch 3200/13130] loss=0.9248, lr=0.0000034, metrics:accuracy:0.5430
INFO:root:18:48:04 [Epoch 5 Batch 3600/13130] loss=0.9457, lr=0.0000032, metrics:accuracy:0.5415
INFO:root:18:48:26 [Epoch 5 Batch 4000/13130] loss=0.9451, lr=0.0000031, metrics:accuracy:0.5409
INFO:root:18:48:48 [Epoch 5 Batch 4400/13130] loss=0.9195, lr=0.0000029, metrics:accuracy:0.5428
INFO:root:18:49:09 [Epoch 5 Batch 4800/13130] loss=0.9166, lr=0.0000028, metrics:accuracy:0.5431
INFO:root:18:49:30 [Epoch 5 Batch 5200/13130] loss=0.9181, lr=0.0000027, metrics:accuracy:0.5437
INFO:root:18:49:53 [Epoch 5 Batch 5600/13130] loss=0.9455, lr=0.0000025, metrics:accuracy:0.5426
INFO:root:18:50:14 [Epoch 5 Batch 6000/13130] loss=0.9235, lr=0.0000024, metrics:accuracy:0.5434
INFO:root:18:50:36 [Epoch 5 Batch 6400/13130] loss=0.9194, lr=0.0000023, metrics:accuracy:0.5450
INFO:root:18:50:58 [Epoch 5 Batch 6800/13130] loss=0.9312, lr=0.0000021, metrics:accuracy:0.5454
INFO:root:18:51:19 [Epoch 5 Batch 7200/13130] loss=0.9236, lr=0.0000020, metrics:accuracy:0.5452
INFO:root:18:51:41 [Epoch 5 Batch 7600/13130] loss=0.9345, lr=0.0000019, metrics:accuracy:0.5446
INFO:root:18:52:02 [Epoch 5 Batch 8000/13130] loss=0.9216, lr=0.0000017, metrics:accuracy:0.5444
INFO:root:18:52:24 [Epoch 5 Batch 8400/13130] loss=0.9193, lr=0.0000016, metrics:accuracy:0.5455
INFO:root:18:52:45 [Epoch 5 Batch 8800/13130] loss=0.9468, lr=0.0000015, metrics:accuracy:0.5450
INFO:root:18:53:08 [Epoch 5 Batch 9200/13130] loss=0.9272, lr=0.0000013, metrics:accuracy:0.5449
INFO:root:18:53:30 [Epoch 5 Batch 9600/13130] loss=0.9211, lr=0.0000012, metrics:accuracy:0.5453
INFO:root:18:53:51 [Epoch 5 Batch 10000/13130] loss=0.9266, lr=0.0000011, metrics:accuracy:0.5457
INFO:root:18:54:14 [Epoch 5 Batch 10400/13130] loss=0.9322, lr=0.0000009, metrics:accuracy:0.5457
INFO:root:18:54:37 [Epoch 5 Batch 10800/13130] loss=0.9293, lr=0.0000008, metrics:accuracy:0.5455
INFO:root:18:54:59 [Epoch 5 Batch 11200/13130] loss=0.9294, lr=0.0000006, metrics:accuracy:0.5456
INFO:root:18:55:23 [Epoch 5 Batch 11600/13130] loss=0.9406, lr=0.0000005, metrics:accuracy:0.5453
INFO:root:18:55:45 [Epoch 5 Batch 12000/13130] loss=0.9294, lr=0.0000004, metrics:accuracy:0.5454
INFO:root:18:56:10 [Epoch 5 Batch 12400/13130] loss=0.9149, lr=0.0000002, metrics:accuracy:0.5456
INFO:root:18:56:32 [Epoch 5 Batch 12800/13130] loss=0.9219, lr=0.0000001, metrics:accuracy:0.5459
INFO:root:18:56:50 Finish training step: 32812
INFO:root:18:56:50 Now we are doing evaluation on dev with gpu(0).
INFO:root:18:57:01 [Batch 400/3750] loss=0.9923, metrics:accuracy:15400.0000
INFO:root:18:57:12 [Batch 800/3750] loss=1.0058, metrics:accuracy:15800.0000
INFO:root:18:57:23 [Batch 1200/3750] loss=0.9982, metrics:accuracy:16200.0000
INFO:root:18:57:34 [Batch 1600/3750] loss=1.1126, metrics:accuracy:16600.0000
INFO:root:18:57:45 [Batch 2000/3750] loss=1.1045, metrics:accuracy:17000.0000
INFO:root:18:57:56 [Batch 2400/3750] loss=1.1015, metrics:accuracy:17400.0000
INFO:root:18:58:07 [Batch 2800/3750] loss=1.0800, metrics:accuracy:17800.0000
INFO:root:18:58:18 [Batch 3200/3750] loss=1.0656, metrics:accuracy:18200.0000
INFO:root:18:58:29 [Batch 3600/3750] loss=1.0695, metrics:accuracy:18600.0000
INFO:root:18:58:34 validation metrics:accuracy:0.4773
INFO:root:18:58:34 Time cost=104.01s, throughput=288.43 samples/s
INFO:root:18:58:34 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:18:58:34 Time cost=830.64s
INFO:root:18:58:35 Best model at epoch 3. Validation metrics:accuracy:0.4789
INFO:root:18:58:35 Now we are doing testing on test with gpu(0).
INFO:root:18:59:26 Time cost=51.68s, throughput=290.24 samples/s
INFO:root:21:07:33 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=100, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:21:08:02 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=100, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:21:08:09 processing dataset...
INFO:root:10:40:10 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float16', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:10:40:10 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:10:40:10 Using AMP
INFO:root:10:40:15 processing dataset...
INFO:root:10:40:37 Now we are doing BERT classification training on gpu(0)!
INFO:root:10:40:37 training steps=32812
WARNING:py.warnings:10:40:40 finetune_classifier.py:641: UserWarning: nan or inf is detected. Clipping results will be undefined.
  nlp.utils.clip_grad_global_norm(params, 1)

INFO:root:10:40:40 AMP: decreasing loss scale to 32768.000000
INFO:root:10:40:44 AMP: decreasing loss scale to 16384.000000
INFO:root:10:40:55 AMP: decreasing loss scale to 8192.000000
INFO:root:10:40:57 AMP: decreasing loss scale to 4096.000000
INFO:root:10:40:57 [Epoch 1 Batch 400/13130] loss=1.1410, lr=0.0000012, metrics:accuracy:0.3458
INFO:root:10:41:14 [Epoch 1 Batch 800/13130] loss=1.1344, lr=0.0000024, metrics:accuracy:0.3340
INFO:root:10:41:31 [Epoch 1 Batch 1200/13130] loss=1.1180, lr=0.0000037, metrics:accuracy:0.3320
INFO:root:10:41:48 [Epoch 1 Batch 1600/13130] loss=1.1060, lr=0.0000049, metrics:accuracy:0.3355
INFO:root:10:42:05 [Epoch 1 Batch 2000/13130] loss=1.1037, lr=0.0000061, metrics:accuracy:0.3396
INFO:root:10:42:22 [Epoch 1 Batch 2400/13130] loss=1.1054, lr=0.0000073, metrics:accuracy:0.3412
INFO:root:10:42:28 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=4, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:10:42:28 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:10:42:39 [Epoch 1 Batch 2800/13130] loss=1.1062, lr=0.0000085, metrics:accuracy:0.3398
INFO:root:10:42:53 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=4, lr=4e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:10:42:53 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:10:42:56 [Epoch 1 Batch 3200/13130] loss=1.1036, lr=0.0000097, metrics:accuracy:0.3388
INFO:root:10:42:57 processing dataset...
INFO:root:10:43:13 [Epoch 1 Batch 3600/13130] loss=1.1027, lr=0.0000110, metrics:accuracy:0.3399
INFO:root:10:43:20 Now we are doing BERT classification training on gpu(0)!
INFO:root:10:43:20 training steps=32812
INFO:root:10:43:21 [Epoch 1 Batch 4/13130] loss=1.1448, lr=0.0000000, metrics:accuracy:0.3125
INFO:root:10:43:21 [Epoch 1 Batch 8/13130] loss=1.1452, lr=0.0000000, metrics:accuracy:0.3125
INFO:root:10:43:21 [Epoch 1 Batch 12/13130] loss=1.2271, lr=0.0000001, metrics:accuracy:0.2812
INFO:root:10:43:22 [Epoch 1 Batch 16/13130] loss=1.2270, lr=0.0000001, metrics:accuracy:0.2656
INFO:root:10:43:22 [Epoch 1 Batch 20/13130] loss=1.0746, lr=0.0000001, metrics:accuracy:0.3000
INFO:root:10:43:22 [Epoch 1 Batch 24/13130] loss=1.2017, lr=0.0000001, metrics:accuracy:0.2969
INFO:root:10:43:23 [Epoch 1 Batch 28/13130] loss=1.1181, lr=0.0000002, metrics:accuracy:0.3080
INFO:root:10:43:23 [Epoch 1 Batch 32/13130] loss=1.0690, lr=0.0000002, metrics:accuracy:0.3203
INFO:root:10:43:23 [Epoch 1 Batch 36/13130] loss=1.2215, lr=0.0000002, metrics:accuracy:0.3056
INFO:root:10:43:24 [Epoch 1 Batch 40/13130] loss=1.1468, lr=0.0000002, metrics:accuracy:0.3094
INFO:root:10:43:24 [Epoch 1 Batch 44/13130] loss=1.1421, lr=0.0000003, metrics:accuracy:0.3097
INFO:root:10:43:24 [Epoch 1 Batch 48/13130] loss=1.0557, lr=0.0000003, metrics:accuracy:0.3203
INFO:root:10:43:25 [Epoch 1 Batch 52/13130] loss=1.1002, lr=0.0000003, metrics:accuracy:0.3221
INFO:root:10:43:25 [Epoch 1 Batch 56/13130] loss=1.1271, lr=0.0000003, metrics:accuracy:0.3192
INFO:root:10:43:25 [Epoch 1 Batch 60/13130] loss=1.0707, lr=0.0000004, metrics:accuracy:0.3271
INFO:root:10:43:25 [Epoch 1 Batch 64/13130] loss=1.0798, lr=0.0000004, metrics:accuracy:0.3281
INFO:root:10:43:26 [Epoch 1 Batch 68/13130] loss=1.1157, lr=0.0000004, metrics:accuracy:0.3272
INFO:root:10:43:26 [Epoch 1 Batch 72/13130] loss=1.1022, lr=0.0000004, metrics:accuracy:0.3281
INFO:root:10:43:26 [Epoch 1 Batch 76/13130] loss=1.1763, lr=0.0000005, metrics:accuracy:0.3224
INFO:root:10:43:27 [Epoch 1 Batch 80/13130] loss=1.1417, lr=0.0000005, metrics:accuracy:0.3234
INFO:root:10:43:27 [Epoch 1 Batch 84/13130] loss=1.1461, lr=0.0000005, metrics:accuracy:0.3244
INFO:root:10:43:27 [Epoch 1 Batch 88/13130] loss=1.1133, lr=0.0000005, metrics:accuracy:0.3239
INFO:root:10:43:28 [Epoch 1 Batch 92/13130] loss=1.0715, lr=0.0000005, metrics:accuracy:0.3315
INFO:root:10:43:28 [Epoch 1 Batch 96/13130] loss=1.1402, lr=0.0000006, metrics:accuracy:0.3281
INFO:root:10:43:28 [Epoch 1 Batch 100/13130] loss=1.0853, lr=0.0000006, metrics:accuracy:0.3300
INFO:root:10:43:29 [Epoch 1 Batch 104/13130] loss=1.0871, lr=0.0000006, metrics:accuracy:0.3329
INFO:root:10:43:29 [Epoch 1 Batch 108/13130] loss=1.0525, lr=0.0000006, metrics:accuracy:0.3380
INFO:root:10:43:29 [Epoch 1 Batch 112/13130] loss=1.0784, lr=0.0000007, metrics:accuracy:0.3415
INFO:root:10:43:29 [Epoch 1 Batch 116/13130] loss=1.0553, lr=0.0000007, metrics:accuracy:0.3448
INFO:root:10:43:30 [Epoch 1 Batch 120/13130] loss=1.0945, lr=0.0000007, metrics:accuracy:0.3479
INFO:root:10:43:30 [Epoch 1 Batch 124/13130] loss=1.1417, lr=0.0000007, metrics:accuracy:0.3468
INFO:root:10:43:30 [Epoch 1 Batch 128/13130] loss=1.1254, lr=0.0000008, metrics:accuracy:0.3447
INFO:root:10:43:31 [Epoch 1 Batch 132/13130] loss=1.1365, lr=0.0000008, metrics:accuracy:0.3438
INFO:root:10:43:31 [Epoch 1 Batch 136/13130] loss=1.1284, lr=0.0000008, metrics:accuracy:0.3438
INFO:root:10:43:31 [Epoch 1 Batch 140/13130] loss=1.1574, lr=0.0000008, metrics:accuracy:0.3438
INFO:root:10:43:32 [Epoch 1 Batch 144/13130] loss=1.1774, lr=0.0000009, metrics:accuracy:0.3403
INFO:root:10:43:32 [Epoch 1 Batch 148/13130] loss=1.1176, lr=0.0000009, metrics:accuracy:0.3378
INFO:root:10:43:32 [Epoch 1 Batch 152/13130] loss=1.1253, lr=0.0000009, metrics:accuracy:0.3355
INFO:root:10:43:32 [Epoch 1 Batch 156/13130] loss=1.1005, lr=0.0000009, metrics:accuracy:0.3349
INFO:root:10:43:33 [Epoch 1 Batch 160/13130] loss=1.0829, lr=0.0000010, metrics:accuracy:0.3367
INFO:root:10:43:33 [Epoch 1 Batch 164/13130] loss=1.1894, lr=0.0000010, metrics:accuracy:0.3338
INFO:root:10:43:33 [Epoch 1 Batch 168/13130] loss=1.1223, lr=0.0000010, metrics:accuracy:0.3341
INFO:root:10:43:34 [Epoch 1 Batch 172/13130] loss=1.0871, lr=0.0000010, metrics:accuracy:0.3343
INFO:root:10:43:34 [Epoch 1 Batch 176/13130] loss=1.1291, lr=0.0000011, metrics:accuracy:0.3324
INFO:root:10:43:34 [Epoch 1 Batch 180/13130] loss=1.1134, lr=0.0000011, metrics:accuracy:0.3340
INFO:root:10:43:34 [Epoch 1 Batch 184/13130] loss=1.1051, lr=0.0000011, metrics:accuracy:0.3356
INFO:root:10:43:35 [Epoch 1 Batch 188/13130] loss=1.0916, lr=0.0000011, metrics:accuracy:0.3351
INFO:root:10:43:35 [Epoch 1 Batch 192/13130] loss=1.1636, lr=0.0000012, metrics:accuracy:0.3340
INFO:root:10:43:35 [Epoch 1 Batch 196/13130] loss=1.0652, lr=0.0000012, metrics:accuracy:0.3380
INFO:root:10:43:36 [Epoch 1 Batch 200/13130] loss=1.0621, lr=0.0000012, metrics:accuracy:0.3406
INFO:root:10:43:36 [Epoch 1 Batch 204/13130] loss=1.0679, lr=0.0000012, metrics:accuracy:0.3425
INFO:root:10:43:36 [Epoch 1 Batch 208/13130] loss=1.0960, lr=0.0000013, metrics:accuracy:0.3419
INFO:root:10:43:36 [Epoch 1 Batch 212/13130] loss=1.1162, lr=0.0000013, metrics:accuracy:0.3414
INFO:root:10:43:37 [Epoch 1 Batch 216/13130] loss=1.1153, lr=0.0000013, metrics:accuracy:0.3426
INFO:root:10:43:37 [Epoch 1 Batch 220/13130] loss=1.1031, lr=0.0000013, metrics:accuracy:0.3449
INFO:root:10:43:37 [Epoch 1 Batch 224/13130] loss=1.1839, lr=0.0000014, metrics:accuracy:0.3432
INFO:root:10:43:38 [Epoch 1 Batch 228/13130] loss=1.0908, lr=0.0000014, metrics:accuracy:0.3416
INFO:root:10:43:38 [Epoch 1 Batch 232/13130] loss=1.1367, lr=0.0000014, metrics:accuracy:0.3394
INFO:root:10:43:38 [Epoch 1 Batch 236/13130] loss=1.1304, lr=0.0000014, metrics:accuracy:0.3369
INFO:root:10:43:39 [Epoch 1 Batch 240/13130] loss=1.1147, lr=0.0000015, metrics:accuracy:0.3385
INFO:root:10:43:39 [Epoch 1 Batch 244/13130] loss=1.1151, lr=0.0000015, metrics:accuracy:0.3376
INFO:root:10:43:39 [Epoch 1 Batch 248/13130] loss=1.1409, lr=0.0000015, metrics:accuracy:0.3372
INFO:root:10:43:40 [Epoch 1 Batch 252/13130] loss=1.1020, lr=0.0000015, metrics:accuracy:0.3368
INFO:root:10:43:40 [Epoch 1 Batch 256/13130] loss=1.1029, lr=0.0000015, metrics:accuracy:0.3364
INFO:root:10:43:40 [Epoch 1 Batch 260/13130] loss=1.1014, lr=0.0000016, metrics:accuracy:0.3361
INFO:root:10:43:40 [Epoch 1 Batch 264/13130] loss=1.1042, lr=0.0000016, metrics:accuracy:0.3362
INFO:root:10:43:41 [Epoch 1 Batch 268/13130] loss=1.1151, lr=0.0000016, metrics:accuracy:0.3372
INFO:root:10:43:41 [Epoch 1 Batch 272/13130] loss=1.0857, lr=0.0000016, metrics:accuracy:0.3369
INFO:root:10:43:41 [Epoch 1 Batch 276/13130] loss=1.0738, lr=0.0000017, metrics:accuracy:0.3388
INFO:root:10:43:42 [Epoch 1 Batch 280/13130] loss=1.1095, lr=0.0000017, metrics:accuracy:0.3393
INFO:root:10:43:42 [Epoch 1 Batch 284/13130] loss=1.1165, lr=0.0000017, metrics:accuracy:0.3385
INFO:root:10:43:42 [Epoch 1 Batch 288/13130] loss=1.0658, lr=0.0000017, metrics:accuracy:0.3411
INFO:root:10:43:43 [Epoch 1 Batch 292/13130] loss=1.1201, lr=0.0000018, metrics:accuracy:0.3403
INFO:root:10:43:43 [Epoch 1 Batch 296/13130] loss=1.1361, lr=0.0000018, metrics:accuracy:0.3408
INFO:root:10:43:43 [Epoch 1 Batch 4000/13130] loss=1.0995, lr=0.0000122, metrics:accuracy:0.3400
INFO:root:10:43:43 [Epoch 1 Batch 300/13130] loss=1.0844, lr=0.0000018, metrics:accuracy:0.3421
INFO:root:10:43:43 [Epoch 1 Batch 304/13130] loss=1.1284, lr=0.0000018, metrics:accuracy:0.3425
INFO:root:10:43:44 [Epoch 1 Batch 308/13130] loss=1.1468, lr=0.0000019, metrics:accuracy:0.3421
INFO:root:10:43:44 [Epoch 1 Batch 312/13130] loss=1.0414, lr=0.0000019, metrics:accuracy:0.3433
INFO:root:10:43:44 [Epoch 1 Batch 316/13130] loss=1.0559, lr=0.0000019, metrics:accuracy:0.3449
INFO:root:10:43:45 [Epoch 1 Batch 320/13130] loss=1.2078, lr=0.0000019, metrics:accuracy:0.3434
INFO:root:10:43:45 [Epoch 1 Batch 324/13130] loss=1.1277, lr=0.0000020, metrics:accuracy:0.3438
INFO:root:10:43:45 [Epoch 1 Batch 328/13130] loss=1.1616, lr=0.0000020, metrics:accuracy:0.3438
INFO:root:10:43:46 [Epoch 1 Batch 332/13130] loss=1.1297, lr=0.0000020, metrics:accuracy:0.3441
INFO:root:10:43:46 [Epoch 1 Batch 336/13130] loss=1.0996, lr=0.0000020, metrics:accuracy:0.3449
INFO:root:10:43:46 [Epoch 1 Batch 340/13130] loss=1.1111, lr=0.0000021, metrics:accuracy:0.3445
INFO:root:10:43:47 [Epoch 1 Batch 344/13130] loss=1.1190, lr=0.0000021, metrics:accuracy:0.3441
INFO:root:10:43:47 [Epoch 1 Batch 348/13130] loss=1.1224, lr=0.0000021, metrics:accuracy:0.3438
INFO:root:10:43:47 [Epoch 1 Batch 352/13130] loss=1.1467, lr=0.0000021, metrics:accuracy:0.3430
INFO:root:10:43:48 [Epoch 1 Batch 356/13130] loss=1.1561, lr=0.0000022, metrics:accuracy:0.3413
INFO:root:10:43:48 [Epoch 1 Batch 360/13130] loss=1.1298, lr=0.0000022, metrics:accuracy:0.3408
INFO:root:10:43:48 [Epoch 1 Batch 364/13130] loss=1.0860, lr=0.0000022, metrics:accuracy:0.3411
INFO:root:10:43:49 [Epoch 1 Batch 368/13130] loss=1.1421, lr=0.0000022, metrics:accuracy:0.3398
INFO:root:10:43:49 [Epoch 1 Batch 372/13130] loss=1.1282, lr=0.0000023, metrics:accuracy:0.3392
INFO:root:10:43:49 [Epoch 1 Batch 376/13130] loss=1.1047, lr=0.0000023, metrics:accuracy:0.3389
INFO:root:10:43:49 [Epoch 1 Batch 380/13130] loss=1.1093, lr=0.0000023, metrics:accuracy:0.3386
INFO:root:10:43:50 [Epoch 1 Batch 384/13130] loss=1.1508, lr=0.0000023, metrics:accuracy:0.3367
INFO:root:10:43:50 [Epoch 1 Batch 388/13130] loss=1.0962, lr=0.0000024, metrics:accuracy:0.3371
INFO:root:10:43:50 [Epoch 1 Batch 392/13130] loss=1.0722, lr=0.0000024, metrics:accuracy:0.3388
INFO:root:10:43:51 [Epoch 1 Batch 396/13130] loss=1.1245, lr=0.0000024, metrics:accuracy:0.3385
INFO:root:10:43:51 [Epoch 1 Batch 400/13130] loss=1.0881, lr=0.0000024, metrics:accuracy:0.3395
INFO:root:10:43:51 [Epoch 1 Batch 404/13130] loss=1.1010, lr=0.0000025, metrics:accuracy:0.3389
INFO:root:10:43:52 [Epoch 1 Batch 408/13130] loss=1.1420, lr=0.0000025, metrics:accuracy:0.3374
INFO:root:10:43:52 [Epoch 1 Batch 412/13130] loss=1.1216, lr=0.0000025, metrics:accuracy:0.3372
INFO:root:10:43:52 [Epoch 1 Batch 416/13130] loss=1.1553, lr=0.0000025, metrics:accuracy:0.3354
INFO:root:10:43:53 [Epoch 1 Batch 420/13130] loss=1.1366, lr=0.0000025, metrics:accuracy:0.3337
INFO:root:10:43:53 [Epoch 1 Batch 424/13130] loss=1.1263, lr=0.0000026, metrics:accuracy:0.3338
INFO:root:10:43:53 [Epoch 1 Batch 428/13130] loss=1.0837, lr=0.0000026, metrics:accuracy:0.3339
INFO:root:10:43:54 [Epoch 1 Batch 432/13130] loss=1.1098, lr=0.0000026, metrics:accuracy:0.3334
INFO:root:10:43:54 [Epoch 1 Batch 436/13130] loss=1.0892, lr=0.0000026, metrics:accuracy:0.3341
INFO:root:10:43:54 [Epoch 1 Batch 440/13130] loss=1.1204, lr=0.0000027, metrics:accuracy:0.3345
INFO:root:10:43:55 [Epoch 1 Batch 444/13130] loss=1.0594, lr=0.0000027, metrics:accuracy:0.3348
INFO:root:10:43:55 [Epoch 1 Batch 448/13130] loss=1.1044, lr=0.0000027, metrics:accuracy:0.3349
INFO:root:10:43:55 [Epoch 1 Batch 452/13130] loss=1.1241, lr=0.0000027, metrics:accuracy:0.3333
INFO:root:10:43:55 [Epoch 1 Batch 456/13130] loss=1.1191, lr=0.0000028, metrics:accuracy:0.3329
INFO:root:10:43:56 [Epoch 1 Batch 460/13130] loss=1.1308, lr=0.0000028, metrics:accuracy:0.3330
INFO:root:10:43:56 [Epoch 1 Batch 464/13130] loss=1.1288, lr=0.0000028, metrics:accuracy:0.3325
INFO:root:10:43:56 [Epoch 1 Batch 468/13130] loss=1.0976, lr=0.0000028, metrics:accuracy:0.3337
INFO:root:10:43:57 [Epoch 1 Batch 472/13130] loss=1.0803, lr=0.0000029, metrics:accuracy:0.3335
INFO:root:10:43:57 [Epoch 1 Batch 476/13130] loss=1.1048, lr=0.0000029, metrics:accuracy:0.3331
INFO:root:10:43:57 [Epoch 1 Batch 480/13130] loss=1.1102, lr=0.0000029, metrics:accuracy:0.3339
INFO:root:10:43:58 [Epoch 1 Batch 484/13130] loss=1.0808, lr=0.0000029, metrics:accuracy:0.3345
INFO:root:10:43:58 [Epoch 1 Batch 488/13130] loss=1.0529, lr=0.0000030, metrics:accuracy:0.3356
INFO:root:10:43:58 [Epoch 1 Batch 492/13130] loss=1.0784, lr=0.0000030, metrics:accuracy:0.3365
INFO:root:10:43:58 [Epoch 1 Batch 496/13130] loss=1.0882, lr=0.0000030, metrics:accuracy:0.3373
INFO:root:10:43:59 [Epoch 1 Batch 500/13130] loss=1.0951, lr=0.0000030, metrics:accuracy:0.3378
INFO:root:10:43:59 [Epoch 1 Batch 504/13130] loss=1.0962, lr=0.0000031, metrics:accuracy:0.3381
INFO:root:10:43:59 [Epoch 1 Batch 508/13130] loss=1.1237, lr=0.0000031, metrics:accuracy:0.3379
INFO:root:10:44:00 [Epoch 1 Batch 512/13130] loss=1.1407, lr=0.0000031, metrics:accuracy:0.3375
INFO:root:10:44:00 [Epoch 1 Batch 516/13130] loss=1.1343, lr=0.0000031, metrics:accuracy:0.3373
INFO:root:10:44:00 [Epoch 1 Batch 520/13130] loss=1.1014, lr=0.0000032, metrics:accuracy:0.3371
INFO:root:10:44:01 [Epoch 1 Batch 524/13130] loss=1.0449, lr=0.0000032, metrics:accuracy:0.3384
INFO:root:10:44:10 AMP: increasing loss scale to 8192.000000
INFO:root:10:44:11 [Epoch 1 Batch 4400/13130] loss=1.1021, lr=0.0000134, metrics:accuracy:0.3406
INFO:root:10:44:13 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=4e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:10:44:13 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:10:44:17 processing dataset...
INFO:root:10:44:28 [Epoch 1 Batch 4800/13130] loss=1.1010, lr=0.0000146, metrics:accuracy:0.3412
INFO:root:10:44:40 Now we are doing BERT classification training on gpu(0)!
INFO:root:10:44:40 training steps=32812
INFO:root:10:44:51 [Epoch 1 Batch 5200/13130] loss=1.1015, lr=0.0000158, metrics:accuracy:0.3413
INFO:root:10:45:10 [Epoch 1 Batch 400/13130] loss=1.1175, lr=0.0000024, metrics:accuracy:0.3395
INFO:root:10:45:32 [Epoch 1 Batch 5600/13130] loss=1.0961, lr=0.0000171, metrics:accuracy:0.3419
INFO:root:10:45:41 [Epoch 1 Batch 800/13130] loss=1.0980, lr=0.0000049, metrics:accuracy:0.3471
INFO:root:10:46:12 [Epoch 1 Batch 1200/13130] loss=1.0962, lr=0.0000073, metrics:accuracy:0.3552
INFO:root:10:46:13 [Epoch 1 Batch 6000/13130] loss=1.1000, lr=0.0000183, metrics:accuracy:0.3424
INFO:root:10:46:43 [Epoch 1 Batch 1600/13130] loss=1.0983, lr=0.0000097, metrics:accuracy:0.3607
INFO:root:10:46:53 [Epoch 1 Batch 6400/13130] loss=1.0974, lr=0.0000195, metrics:accuracy:0.3435
INFO:root:10:47:14 [Epoch 1 Batch 2000/13130] loss=1.0988, lr=0.0000122, metrics:accuracy:0.3630
INFO:root:10:47:34 [Epoch 1 Batch 6800/13130] loss=1.0958, lr=0.0000199, metrics:accuracy:0.3448
INFO:root:10:47:45 [Epoch 1 Batch 2400/13130] loss=1.0967, lr=0.0000146, metrics:accuracy:0.3645
INFO:root:10:48:15 [Epoch 1 Batch 7200/13130] loss=1.0989, lr=0.0000198, metrics:accuracy:0.3454
INFO:root:10:48:15 [Epoch 1 Batch 2800/13130] loss=1.0934, lr=0.0000171, metrics:accuracy:0.3665
INFO:root:10:48:46 [Epoch 1 Batch 3200/13130] loss=1.0934, lr=0.0000195, metrics:accuracy:0.3685
INFO:root:10:48:55 [Epoch 1 Batch 7600/13130] loss=1.0979, lr=0.0000196, metrics:accuracy:0.3462
INFO:root:10:49:16 [Epoch 1 Batch 3600/13130] loss=1.0890, lr=0.0000219, metrics:accuracy:0.3712
INFO:root:10:49:35 [Epoch 1 Batch 8000/13130] loss=1.0988, lr=0.0000195, metrics:accuracy:0.3468
INFO:root:10:49:47 [Epoch 1 Batch 4000/13130] loss=1.0836, lr=0.0000244, metrics:accuracy:0.3740
INFO:root:10:50:15 AMP: increasing loss scale to 16384.000000
INFO:root:10:50:16 [Epoch 1 Batch 8400/13130] loss=1.0967, lr=0.0000194, metrics:accuracy:0.3471
INFO:root:10:50:18 [Epoch 1 Batch 4400/13130] loss=1.0888, lr=0.0000268, metrics:accuracy:0.3754
INFO:root:10:50:49 [Epoch 1 Batch 4800/13130] loss=1.0846, lr=0.0000292, metrics:accuracy:0.3772
INFO:root:10:50:57 [Epoch 1 Batch 8800/13130] loss=1.0935, lr=0.0000192, metrics:accuracy:0.3481
INFO:root:10:51:19 [Epoch 1 Batch 5200/13130] loss=1.0959, lr=0.0000317, metrics:accuracy:0.3776
INFO:root:10:51:37 [Epoch 1 Batch 9200/13130] loss=1.0957, lr=0.0000191, metrics:accuracy:0.3488
INFO:root:10:51:50 [Epoch 1 Batch 5600/13130] loss=1.0805, lr=0.0000341, metrics:accuracy:0.3806
INFO:root:10:52:18 [Epoch 1 Batch 9600/13130] loss=1.0926, lr=0.0000190, metrics:accuracy:0.3494
INFO:root:10:52:20 [Epoch 1 Batch 6000/13130] loss=1.0829, lr=0.0000366, metrics:accuracy:0.3818
INFO:root:10:52:51 [Epoch 1 Batch 6400/13130] loss=1.0808, lr=0.0000390, metrics:accuracy:0.3840
INFO:root:10:52:58 [Epoch 1 Batch 10000/13130] loss=1.0964, lr=0.0000188, metrics:accuracy:0.3499
INFO:root:10:53:01 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=4, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:10:53:01 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:10:53:21 [Epoch 1 Batch 6800/13130] loss=1.0788, lr=0.0000398, metrics:accuracy:0.3861
INFO:root:10:53:39 [Epoch 1 Batch 10400/13130] loss=1.0923, lr=0.0000187, metrics:accuracy:0.3508
INFO:root:10:53:51 [Epoch 1 Batch 7200/13130] loss=1.0768, lr=0.0000396, metrics:accuracy:0.3874
INFO:root:10:54:19 [Epoch 1 Batch 10800/13130] loss=1.0946, lr=0.0000186, metrics:accuracy:0.3513
INFO:root:10:54:21 [Epoch 1 Batch 7600/13130] loss=1.0792, lr=0.0000393, metrics:accuracy:0.3890
INFO:root:10:54:52 [Epoch 1 Batch 8000/13130] loss=1.0822, lr=0.0000390, metrics:accuracy:0.3899
INFO:root:10:54:59 [Epoch 1 Batch 11200/13130] loss=1.0973, lr=0.0000184, metrics:accuracy:0.3512
INFO:root:10:55:23 [Epoch 1 Batch 8400/13130] loss=1.0800, lr=0.0000388, metrics:accuracy:0.3909
INFO:root:10:55:39 [Epoch 1 Batch 11600/13130] loss=1.0934, lr=0.0000183, metrics:accuracy:0.3516
INFO:root:10:55:53 [Epoch 1 Batch 8800/13130] loss=1.0762, lr=0.0000385, metrics:accuracy:0.3918
INFO:root:10:56:18 [Epoch 1 Batch 12000/13130] loss=1.0931, lr=0.0000182, metrics:accuracy:0.3522
INFO:root:10:56:24 [Epoch 1 Batch 9200/13130] loss=1.0702, lr=0.0000382, metrics:accuracy:0.3936
INFO:root:10:56:54 [Epoch 1 Batch 9600/13130] loss=1.0665, lr=0.0000379, metrics:accuracy:0.3953
INFO:root:10:56:57 AMP: increasing loss scale to 32768.000000
INFO:root:10:56:59 [Epoch 1 Batch 12400/13130] loss=1.0905, lr=0.0000180, metrics:accuracy:0.3531
INFO:root:10:56:59 AMP: decreasing loss scale to 16384.000000
INFO:root:10:57:25 [Epoch 1 Batch 10000/13130] loss=1.0743, lr=0.0000377, metrics:accuracy:0.3960
INFO:root:10:57:39 [Epoch 1 Batch 12800/13130] loss=1.0972, lr=0.0000179, metrics:accuracy:0.3533
INFO:root:10:57:55 [Epoch 1 Batch 10400/13130] loss=1.0595, lr=0.0000374, metrics:accuracy:0.3974
INFO:root:10:58:12 Now we are doing evaluation on dev with gpu(0).
INFO:root:10:58:24 [Epoch 1 Batch 10800/13130] loss=1.0586, lr=0.0000371, metrics:accuracy:0.3994
INFO:root:10:58:30 [Batch 400/3750] loss=1.0495, metrics:accuracy:400.0000
INFO:root:10:58:48 [Batch 800/3750] loss=1.0510, metrics:accuracy:800.0000
INFO:root:10:58:51 [Epoch 1 Batch 11200/13130] loss=1.0748, lr=0.0000369, metrics:accuracy:0.4004
INFO:root:10:59:06 [Batch 1200/3750] loss=1.0514, metrics:accuracy:1200.0000
INFO:root:10:59:19 [Epoch 1 Batch 11600/13130] loss=1.0592, lr=0.0000366, metrics:accuracy:0.4017
INFO:root:10:59:25 [Batch 1600/3750] loss=1.0884, metrics:accuracy:1600.0000
INFO:root:10:59:43 [Batch 2000/3750] loss=1.0892, metrics:accuracy:2000.0000
INFO:root:10:59:47 [Epoch 1 Batch 12000/13130] loss=1.0717, lr=0.0000363, metrics:accuracy:0.4026
INFO:root:11:00:02 [Batch 2400/3750] loss=1.0829, metrics:accuracy:2400.0000
INFO:root:11:00:14 [Epoch 1 Batch 12400/13130] loss=1.0653, lr=0.0000360, metrics:accuracy:0.4036
INFO:root:11:00:20 [Batch 2800/3750] loss=1.1212, metrics:accuracy:2800.0000
INFO:root:11:00:38 [Batch 3200/3750] loss=1.1241, metrics:accuracy:3200.0000
INFO:root:11:00:42 [Epoch 1 Batch 12800/13130] loss=1.0663, lr=0.0000358, metrics:accuracy:0.4046
INFO:root:11:00:57 [Batch 3600/3750] loss=1.1306, metrics:accuracy:3600.0000
INFO:root:11:01:03 validation metrics:accuracy:0.3827
INFO:root:11:01:03 Time cost=171.26s, throughput=175.17 samples/s
INFO:root:11:01:04 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:01:05 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:11:01:05 Time cost=1227.59s
INFO:root:11:01:21 [Batch 400/3750] loss=1.1322, metrics:accuracy:400.0000
INFO:root:11:01:37 [Batch 800/3750] loss=1.1377, metrics:accuracy:800.0000
INFO:root:11:01:38 [Epoch 2 Batch 400/13130] loss=1.0943, lr=0.0000176, metrics:accuracy:0.3663
INFO:root:11:01:54 [Batch 1200/3750] loss=1.1360, metrics:accuracy:1200.0000
INFO:root:11:02:10 [Epoch 2 Batch 800/13130] loss=1.0942, lr=0.0000175, metrics:accuracy:0.3642
INFO:root:11:02:11 [Batch 1600/3750] loss=1.1347, metrics:accuracy:1600.0000
INFO:root:11:02:27 [Batch 2000/3750] loss=1.1293, metrics:accuracy:2000.0000
INFO:root:11:02:43 [Epoch 2 Batch 1200/13130] loss=1.0919, lr=0.0000174, metrics:accuracy:0.3673
INFO:root:11:02:44 [Batch 2400/3750] loss=1.1216, metrics:accuracy:2400.0000
INFO:root:11:03:00 [Batch 2800/3750] loss=0.9595, metrics:accuracy:2800.0000
INFO:root:11:03:16 [Epoch 2 Batch 1600/13130] loss=1.0940, lr=0.0000172, metrics:accuracy:0.3653
INFO:root:11:03:17 [Batch 3200/3750] loss=0.8933, metrics:accuracy:3200.0000
INFO:root:11:03:33 [Batch 3600/3750] loss=0.8955, metrics:accuracy:3600.0000
INFO:root:11:03:39 validation metrics:accuracy:0.4463
INFO:root:11:03:39 Time cost=154.92s, throughput=193.64 samples/s
INFO:root:11:03:41 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:11:03:41 Time cost=1140.93s
INFO:root:11:03:49 [Epoch 2 Batch 2000/13130] loss=1.0900, lr=0.0000171, metrics:accuracy:0.3688
INFO:root:11:04:11 [Epoch 2 Batch 400/13130] loss=1.0473, lr=0.0000353, metrics:accuracy:0.4537
INFO:root:11:04:29 [Epoch 2 Batch 2400/13130] loss=1.0946, lr=0.0000170, metrics:accuracy:0.3681
INFO:root:11:04:36 AMP: decreasing loss scale to 8192.000000
INFO:root:11:04:41 [Epoch 2 Batch 800/13130] loss=1.0639, lr=0.0000350, metrics:accuracy:0.4427
INFO:root:11:05:10 [Epoch 2 Batch 2800/13130] loss=1.0942, lr=0.0000168, metrics:accuracy:0.3682
INFO:root:11:05:11 [Epoch 2 Batch 1200/13130] loss=1.0435, lr=0.0000347, metrics:accuracy:0.4510
INFO:root:11:05:41 [Epoch 2 Batch 1600/13130] loss=1.0405, lr=0.0000345, metrics:accuracy:0.4533
INFO:root:11:05:50 [Epoch 2 Batch 3200/13130] loss=1.0910, lr=0.0000167, metrics:accuracy:0.3690
INFO:root:11:06:11 [Epoch 2 Batch 2000/13130] loss=1.0344, lr=0.0000342, metrics:accuracy:0.4568
INFO:root:11:06:31 [Epoch 2 Batch 3600/13130] loss=1.0951, lr=0.0000166, metrics:accuracy:0.3690
INFO:root:11:06:41 [Epoch 2 Batch 2400/13130] loss=1.0393, lr=0.0000339, metrics:accuracy:0.4588
INFO:root:11:07:11 [Epoch 2 Batch 4000/13130] loss=1.0903, lr=0.0000164, metrics:accuracy:0.3700
INFO:root:11:07:12 [Epoch 2 Batch 2800/13130] loss=1.0413, lr=0.0000337, metrics:accuracy:0.4598
INFO:root:11:07:42 [Epoch 2 Batch 3200/13130] loss=1.0487, lr=0.0000334, metrics:accuracy:0.4582
INFO:root:11:07:52 [Epoch 2 Batch 4400/13130] loss=1.0889, lr=0.0000163, metrics:accuracy:0.3716
INFO:root:11:08:12 [Epoch 2 Batch 3600/13130] loss=1.0425, lr=0.0000331, metrics:accuracy:0.4573
INFO:root:11:08:32 [Epoch 2 Batch 4800/13130] loss=1.0885, lr=0.0000162, metrics:accuracy:0.3725
INFO:root:11:08:42 [Epoch 2 Batch 4000/13130] loss=1.0420, lr=0.0000328, metrics:accuracy:0.4580
INFO:root:11:09:13 [Epoch 2 Batch 4400/13130] loss=1.0470, lr=0.0000326, metrics:accuracy:0.4578
INFO:root:11:09:13 [Epoch 2 Batch 5200/13130] loss=1.0968, lr=0.0000160, metrics:accuracy:0.3721
INFO:root:11:09:43 [Epoch 2 Batch 4800/13130] loss=1.0761, lr=0.0000323, metrics:accuracy:0.4533
INFO:root:11:09:53 [Epoch 2 Batch 5600/13130] loss=1.0908, lr=0.0000159, metrics:accuracy:0.3718
INFO:root:11:10:14 [Epoch 2 Batch 5200/13130] loss=1.0639, lr=0.0000320, metrics:accuracy:0.4524
INFO:root:11:10:33 [Epoch 2 Batch 6000/13130] loss=1.0935, lr=0.0000157, metrics:accuracy:0.3717
INFO:root:11:10:45 [Epoch 2 Batch 5600/13130] loss=1.0462, lr=0.0000318, metrics:accuracy:0.4526
INFO:root:11:11:14 [Epoch 2 Batch 6400/13130] loss=1.0887, lr=0.0000156, metrics:accuracy:0.3720
INFO:root:11:11:15 [Epoch 2 Batch 6000/13130] loss=1.0588, lr=0.0000315, metrics:accuracy:0.4525
INFO:root:11:11:21 AMP: increasing loss scale to 16384.000000
INFO:root:11:11:46 [Epoch 2 Batch 6400/13130] loss=1.0381, lr=0.0000312, metrics:accuracy:0.4535
INFO:root:11:11:55 [Epoch 2 Batch 6800/13130] loss=1.0881, lr=0.0000155, metrics:accuracy:0.3725
INFO:root:11:12:17 [Epoch 2 Batch 6800/13130] loss=1.0449, lr=0.0000309, metrics:accuracy:0.4534
INFO:root:11:12:35 [Epoch 2 Batch 7200/13130] loss=1.0977, lr=0.0000153, metrics:accuracy:0.3715
INFO:root:11:12:47 [Epoch 2 Batch 7200/13130] loss=1.0612, lr=0.0000307, metrics:accuracy:0.4527
INFO:root:11:12:58 AMP: decreasing loss scale to 8192.000000
INFO:root:11:13:15 [Epoch 2 Batch 7600/13130] loss=1.0874, lr=0.0000152, metrics:accuracy:0.3717
INFO:root:11:13:17 [Epoch 2 Batch 7600/13130] loss=1.0416, lr=0.0000304, metrics:accuracy:0.4535
INFO:root:11:13:47 [Epoch 2 Batch 8000/13130] loss=1.0480, lr=0.0000301, metrics:accuracy:0.4531
INFO:root:11:13:54 [Epoch 2 Batch 8000/13130] loss=1.0887, lr=0.0000151, metrics:accuracy:0.3724
INFO:root:11:14:17 [Epoch 2 Batch 8400/13130] loss=1.0438, lr=0.0000299, metrics:accuracy:0.4536
INFO:root:11:14:34 [Epoch 2 Batch 8400/13130] loss=1.0891, lr=0.0000149, metrics:accuracy:0.3730
INFO:root:11:14:47 [Epoch 2 Batch 8800/13130] loss=1.0535, lr=0.0000296, metrics:accuracy:0.4531
INFO:root:11:15:15 [Epoch 2 Batch 8800/13130] loss=1.0895, lr=0.0000148, metrics:accuracy:0.3734
INFO:root:11:15:18 [Epoch 2 Batch 9200/13130] loss=1.0511, lr=0.0000293, metrics:accuracy:0.4528
INFO:root:11:15:48 [Epoch 2 Batch 9600/13130] loss=1.0413, lr=0.0000291, metrics:accuracy:0.4527
INFO:root:11:15:56 [Epoch 2 Batch 9200/13130] loss=1.0927, lr=0.0000147, metrics:accuracy:0.3733
INFO:root:11:16:18 [Epoch 2 Batch 10000/13130] loss=1.0500, lr=0.0000288, metrics:accuracy:0.4528
INFO:root:11:16:37 [Epoch 2 Batch 9600/13130] loss=1.0961, lr=0.0000145, metrics:accuracy:0.3730
INFO:root:11:16:49 [Epoch 2 Batch 10400/13130] loss=1.0408, lr=0.0000285, metrics:accuracy:0.4530
INFO:root:11:17:18 [Epoch 2 Batch 10800/13130] loss=1.0427, lr=0.0000282, metrics:accuracy:0.4530
INFO:root:11:17:18 [Epoch 2 Batch 10000/13130] loss=1.0906, lr=0.0000144, metrics:accuracy:0.3731
INFO:root:11:17:48 [Epoch 2 Batch 11200/13130] loss=1.0285, lr=0.0000280, metrics:accuracy:0.4537
INFO:root:11:17:58 [Epoch 2 Batch 10400/13130] loss=1.0879, lr=0.0000143, metrics:accuracy:0.3735
INFO:root:11:18:19 [Epoch 2 Batch 11600/13130] loss=1.0400, lr=0.0000277, metrics:accuracy:0.4540
INFO:root:11:18:39 [Epoch 2 Batch 10800/13130] loss=1.0888, lr=0.0000141, metrics:accuracy:0.3738
INFO:root:11:18:49 [Epoch 2 Batch 12000/13130] loss=1.0422, lr=0.0000274, metrics:accuracy:0.4543
INFO:root:11:19:18 [Epoch 2 Batch 11200/13130] loss=1.0849, lr=0.0000140, metrics:accuracy:0.3746
INFO:root:11:19:20 [Epoch 2 Batch 12400/13130] loss=1.0374, lr=0.0000272, metrics:accuracy:0.4547
INFO:root:11:19:42 AMP: increasing loss scale to 16384.000000
INFO:root:11:19:50 [Epoch 2 Batch 12800/13130] loss=1.0250, lr=0.0000269, metrics:accuracy:0.4554
INFO:root:11:20:00 [Epoch 2 Batch 11600/13130] loss=1.0929, lr=0.0000138, metrics:accuracy:0.3745
INFO:root:11:20:15 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:20:32 [Batch 400/3750] loss=1.0309, metrics:accuracy:4150.0000
INFO:root:11:20:36 [Epoch 2 Batch 12000/13130] loss=1.0897, lr=0.0000137, metrics:accuracy:0.3745
INFO:root:11:20:41 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=1e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:11:20:41 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:11:20:45 processing dataset...
INFO:root:11:20:48 [Batch 800/3750] loss=1.0367, metrics:accuracy:4550.0000
INFO:root:11:21:04 [Batch 1200/3750] loss=1.0247, metrics:accuracy:4950.0000
INFO:root:11:21:08 Now we are doing BERT classification training on gpu(0)!
INFO:root:11:21:08 training steps=32812
INFO:root:11:21:08 [Epoch 2 Batch 12400/13130] loss=1.0882, lr=0.0000136, metrics:accuracy:0.3751
INFO:root:11:21:20 [Batch 1600/3750] loss=1.1553, metrics:accuracy:5350.0000
INFO:root:11:21:36 [Batch 2000/3750] loss=1.1732, metrics:accuracy:5750.0000
INFO:root:11:21:41 [Epoch 1 Batch 400/13130] loss=1.1195, lr=0.0000006, metrics:accuracy:0.3436
INFO:root:11:21:42 [Epoch 2 Batch 12800/13130] loss=1.0824, lr=0.0000134, metrics:accuracy:0.3760
INFO:root:11:21:53 [Batch 2400/3750] loss=1.1548, metrics:accuracy:6150.0000
INFO:root:11:22:09 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:22:09 [Batch 2800/3750] loss=1.0016, metrics:accuracy:6550.0000
INFO:root:11:22:11 [Epoch 1 Batch 800/13130] loss=1.1026, lr=0.0000012, metrics:accuracy:0.3444
INFO:root:11:22:25 [Batch 400/3750] loss=1.0802, metrics:accuracy:4150.0000
INFO:root:11:22:25 [Batch 3200/3750] loss=0.9306, metrics:accuracy:6950.0000
INFO:root:11:22:40 [Batch 800/3750] loss=1.0836, metrics:accuracy:4550.0000
INFO:root:11:22:40 [Batch 3600/3750] loss=0.9329, metrics:accuracy:7350.0000
INFO:root:11:22:44 [Epoch 1 Batch 1200/13130] loss=1.0992, lr=0.0000018, metrics:accuracy:0.3482
INFO:root:11:22:46 validation metrics:accuracy:0.4637
INFO:root:11:22:46 Time cost=150.62s, throughput=199.17 samples/s
INFO:root:11:22:47 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:11:22:47 Time cost=1146.59s
INFO:root:11:22:54 [Batch 1200/3750] loss=1.0846, metrics:accuracy:4950.0000
INFO:root:11:23:12 [Batch 1600/3750] loss=1.0807, metrics:accuracy:5350.0000
INFO:root:11:23:16 [Epoch 3 Batch 400/13130] loss=1.0160, lr=0.0000264, metrics:accuracy:0.4784
INFO:root:11:23:16 [Epoch 1 Batch 1600/13130] loss=1.0997, lr=0.0000024, metrics:accuracy:0.3508
INFO:root:11:23:21 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.3)
INFO:root:11:23:21 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:11:23:26 processing dataset...
INFO:root:11:23:30 [Batch 2000/3750] loss=1.0734, metrics:accuracy:5750.0000
INFO:root:11:23:43 [Epoch 3 Batch 800/13130] loss=1.0133, lr=0.0000261, metrics:accuracy:0.4805
INFO:root:11:23:47 [Epoch 1 Batch 2000/13130] loss=1.0967, lr=0.0000030, metrics:accuracy:0.3538
INFO:root:11:23:48 Now we are doing BERT classification training on gpu(0)!
INFO:root:11:23:48 training steps=32812
INFO:root:11:23:49 [Batch 2400/3750] loss=1.0633, metrics:accuracy:6150.0000
INFO:root:11:24:04 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=8e-06, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:11:24:04 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:11:24:06 [Batch 2800/3750] loss=1.0956, metrics:accuracy:6550.0000
INFO:root:11:24:08 processing dataset...
INFO:root:11:24:11 [Epoch 3 Batch 1200/13130] loss=1.0228, lr=0.0000258, metrics:accuracy:0.4794
INFO:root:11:24:20 [Epoch 1 Batch 2400/13130] loss=1.0943, lr=0.0000037, metrics:accuracy:0.3568
INFO:root:11:24:25 [Batch 3200/3750] loss=1.0901, metrics:accuracy:6950.0000
INFO:root:11:24:31 Now we are doing BERT classification training on gpu(0)!
INFO:root:11:24:31 training steps=32812
INFO:root:11:24:39 [Epoch 3 Batch 1600/13130] loss=1.0055, lr=0.0000256, metrics:accuracy:0.4833
INFO:root:11:24:43 [Batch 3600/3750] loss=1.1018, metrics:accuracy:7350.0000
INFO:root:11:24:50 [Epoch 1 Batch 2800/13130] loss=1.0929, lr=0.0000043, metrics:accuracy:0.3595
INFO:root:11:24:50 validation metrics:accuracy:0.3931
INFO:root:11:24:50 Time cost=160.95s, throughput=186.39 samples/s
INFO:root:11:24:52 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:11:24:52 Time cost=1426.68s
INFO:root:11:24:53 [Epoch 1 Batch 400/13130] loss=1.1207, lr=0.0000005, metrics:accuracy:0.3454
INFO:root:11:25:08 [Epoch 3 Batch 2000/13130] loss=1.0060, lr=0.0000253, metrics:accuracy:0.4846
INFO:root:11:25:15 [Epoch 1 Batch 800/13130] loss=1.1038, lr=0.0000010, metrics:accuracy:0.3460
INFO:root:11:25:22 [Epoch 1 Batch 3200/13130] loss=1.0930, lr=0.0000049, metrics:accuracy:0.3623
INFO:root:11:25:32 [Epoch 3 Batch 400/13130] loss=1.0898, lr=0.0000132, metrics:accuracy:0.3691
INFO:root:11:25:37 [Epoch 1 Batch 1200/13130] loss=1.0989, lr=0.0000015, metrics:accuracy:0.3489
INFO:root:11:25:38 [Epoch 3 Batch 2400/13130] loss=1.0245, lr=0.0000250, metrics:accuracy:0.4828
INFO:root:11:25:54 [Epoch 1 Batch 3600/13130] loss=1.0902, lr=0.0000055, metrics:accuracy:0.3646
INFO:root:11:25:59 [Epoch 1 Batch 1600/13130] loss=1.0972, lr=0.0000019, metrics:accuracy:0.3525
INFO:root:11:26:09 [Epoch 3 Batch 2800/13130] loss=1.0053, lr=0.0000248, metrics:accuracy:0.4855
INFO:root:11:26:13 [Epoch 3 Batch 800/13130] loss=1.0829, lr=0.0000131, metrics:accuracy:0.3758
INFO:root:11:26:22 [Epoch 1 Batch 2000/13130] loss=1.0964, lr=0.0000024, metrics:accuracy:0.3555
INFO:root:11:26:24 [Epoch 1 Batch 4000/13130] loss=1.0866, lr=0.0000061, metrics:accuracy:0.3679
INFO:root:11:26:39 [Epoch 3 Batch 3200/13130] loss=1.0021, lr=0.0000245, metrics:accuracy:0.4873
INFO:root:11:26:44 [Epoch 1 Batch 2400/13130] loss=1.0942, lr=0.0000029, metrics:accuracy:0.3566
INFO:root:11:26:54 [Epoch 3 Batch 1200/13130] loss=1.0859, lr=0.0000129, metrics:accuracy:0.3820
INFO:root:11:26:57 [Epoch 1 Batch 4400/13130] loss=1.0851, lr=0.0000067, metrics:accuracy:0.3707
INFO:root:11:27:07 [Epoch 1 Batch 2800/13130] loss=1.0912, lr=0.0000034, metrics:accuracy:0.3603
INFO:root:11:27:10 [Epoch 3 Batch 3600/13130] loss=1.0096, lr=0.0000242, metrics:accuracy:0.4867
INFO:root:11:27:28 [Epoch 1 Batch 4800/13130] loss=1.0844, lr=0.0000073, metrics:accuracy:0.3726
INFO:root:11:27:29 [Epoch 1 Batch 3200/13130] loss=1.0929, lr=0.0000039, metrics:accuracy:0.3626
INFO:root:11:27:35 [Epoch 3 Batch 1600/13130] loss=1.0876, lr=0.0000128, metrics:accuracy:0.3841
INFO:root:11:27:40 [Epoch 3 Batch 4000/13130] loss=1.0074, lr=0.0000240, metrics:accuracy:0.4870
INFO:root:11:27:51 [Epoch 1 Batch 3600/13130] loss=1.0889, lr=0.0000044, metrics:accuracy:0.3651
INFO:root:11:27:51 [Epoch 1 Batch 5200/13130] loss=1.0909, lr=0.0000079, metrics:accuracy:0.3744
INFO:root:11:28:10 [Epoch 3 Batch 4400/13130] loss=1.0193, lr=0.0000237, metrics:accuracy:0.4864
INFO:root:11:28:13 [Epoch 1 Batch 5600/13130] loss=1.0793, lr=0.0000085, metrics:accuracy:0.3770
INFO:root:11:28:14 [Epoch 1 Batch 4000/13130] loss=1.0848, lr=0.0000049, metrics:accuracy:0.3681
INFO:root:11:28:15 [Epoch 3 Batch 2000/13130] loss=1.0806, lr=0.0000127, metrics:accuracy:0.3869
INFO:root:11:28:36 [Epoch 1 Batch 6000/13130] loss=1.0830, lr=0.0000091, metrics:accuracy:0.3785
INFO:root:11:28:36 [Epoch 1 Batch 4400/13130] loss=1.0857, lr=0.0000054, metrics:accuracy:0.3704
INFO:root:11:28:41 [Epoch 3 Batch 4800/13130] loss=1.0129, lr=0.0000234, metrics:accuracy:0.4870
INFO:root:11:28:46 AMP: increasing loss scale to 32768.000000
INFO:root:11:28:54 AMP: decreasing loss scale to 16384.000000
INFO:root:11:28:56 [Epoch 3 Batch 2400/13130] loss=1.0873, lr=0.0000125, metrics:accuracy:0.3868
INFO:root:11:28:58 [Epoch 1 Batch 6400/13130] loss=1.0767, lr=0.0000098, metrics:accuracy:0.3802
INFO:root:11:28:59 [Epoch 1 Batch 4800/13130] loss=1.0836, lr=0.0000058, metrics:accuracy:0.3718
INFO:root:11:29:12 [Epoch 3 Batch 5200/13130] loss=1.0319, lr=0.0000231, metrics:accuracy:0.4856
INFO:root:11:29:20 [Epoch 1 Batch 6800/13130] loss=1.0744, lr=0.0000100, metrics:accuracy:0.3828
INFO:root:11:29:21 [Epoch 1 Batch 5200/13130] loss=1.0912, lr=0.0000063, metrics:accuracy:0.3730
INFO:root:11:29:38 [Epoch 3 Batch 2800/13130] loss=1.0887, lr=0.0000124, metrics:accuracy:0.3848
INFO:root:11:29:41 [Epoch 1 Batch 7200/13130] loss=1.0785, lr=0.0000099, metrics:accuracy:0.3842
INFO:root:11:29:42 [Epoch 3 Batch 5600/13130] loss=1.0078, lr=0.0000229, metrics:accuracy:0.4857
INFO:root:11:29:43 [Epoch 1 Batch 5600/13130] loss=1.0787, lr=0.0000068, metrics:accuracy:0.3754
INFO:root:11:30:03 [Epoch 1 Batch 7600/13130] loss=1.0782, lr=0.0000098, metrics:accuracy:0.3859
INFO:root:11:30:06 [Epoch 1 Batch 6000/13130] loss=1.0834, lr=0.0000073, metrics:accuracy:0.3774
INFO:root:11:30:13 [Epoch 3 Batch 6000/13130] loss=1.0180, lr=0.0000226, metrics:accuracy:0.4858
INFO:root:11:30:19 [Epoch 3 Batch 3200/13130] loss=1.0831, lr=0.0000122, metrics:accuracy:0.3860
INFO:root:11:30:25 [Epoch 1 Batch 8000/13130] loss=1.0797, lr=0.0000098, metrics:accuracy:0.3869
INFO:root:11:30:28 [Epoch 1 Batch 6400/13130] loss=1.0758, lr=0.0000078, metrics:accuracy:0.3800
INFO:root:11:30:43 [Epoch 3 Batch 6400/13130] loss=1.0165, lr=0.0000223, metrics:accuracy:0.4851
INFO:root:11:30:47 [Epoch 1 Batch 8400/13130] loss=1.0793, lr=0.0000097, metrics:accuracy:0.3877
INFO:root:11:30:51 [Epoch 1 Batch 6800/13130] loss=1.0748, lr=0.0000080, metrics:accuracy:0.3820
INFO:root:11:31:01 [Epoch 3 Batch 3600/13130] loss=1.0890, lr=0.0000121, metrics:accuracy:0.3862
INFO:root:11:31:09 [Epoch 1 Batch 8800/13130] loss=1.0743, lr=0.0000096, metrics:accuracy:0.3891
INFO:root:11:31:12 [Epoch 1 Batch 7200/13130] loss=1.0773, lr=0.0000079, metrics:accuracy:0.3836
INFO:root:11:31:14 [Epoch 3 Batch 6800/13130] loss=1.0043, lr=0.0000221, metrics:accuracy:0.4854
INFO:root:11:31:31 [Epoch 1 Batch 9200/13130] loss=1.0681, lr=0.0000096, metrics:accuracy:0.3912
INFO:root:11:31:35 [Epoch 1 Batch 7600/13130] loss=1.0760, lr=0.0000079, metrics:accuracy:0.3852
INFO:root:11:31:43 [Epoch 3 Batch 4000/13130] loss=1.0841, lr=0.0000120, metrics:accuracy:0.3863
INFO:root:11:31:45 [Epoch 3 Batch 7200/13130] loss=1.0229, lr=0.0000218, metrics:accuracy:0.4848
INFO:root:11:31:53 [Epoch 1 Batch 9600/13130] loss=1.0675, lr=0.0000095, metrics:accuracy:0.3927
INFO:root:11:31:57 [Epoch 1 Batch 8000/13130] loss=1.0816, lr=0.0000078, metrics:accuracy:0.3863
INFO:root:11:32:14 [Epoch 3 Batch 7600/13130] loss=1.0157, lr=0.0000215, metrics:accuracy:0.4847
INFO:root:11:32:15 [Epoch 1 Batch 10000/13130] loss=1.0721, lr=0.0000094, metrics:accuracy:0.3935
INFO:root:11:32:19 [Epoch 1 Batch 8400/13130] loss=1.0773, lr=0.0000078, metrics:accuracy:0.3876
INFO:root:11:32:23 [Epoch 3 Batch 4400/13130] loss=1.0925, lr=0.0000118, metrics:accuracy:0.3855
INFO:root:11:32:37 [Epoch 1 Batch 10400/13130] loss=1.0599, lr=0.0000094, metrics:accuracy:0.3951
INFO:root:11:32:41 [Epoch 1 Batch 8800/13130] loss=1.0740, lr=0.0000077, metrics:accuracy:0.3887
INFO:root:11:32:45 [Epoch 3 Batch 8000/13130] loss=1.0161, lr=0.0000212, metrics:accuracy:0.4848
INFO:root:11:32:59 [Epoch 1 Batch 10800/13130] loss=1.0635, lr=0.0000093, metrics:accuracy:0.3966
INFO:root:11:33:03 [Epoch 3 Batch 4800/13130] loss=1.0835, lr=0.0000117, metrics:accuracy:0.3862
INFO:root:11:33:03 [Epoch 1 Batch 9200/13130] loss=1.0674, lr=0.0000076, metrics:accuracy:0.3908
INFO:root:11:33:15 [Epoch 3 Batch 8400/13130] loss=1.0038, lr=0.0000210, metrics:accuracy:0.4851
INFO:root:11:33:20 [Epoch 1 Batch 11200/13130] loss=1.0709, lr=0.0000092, metrics:accuracy:0.3977
INFO:root:11:33:26 [Epoch 1 Batch 9600/13130] loss=1.0677, lr=0.0000076, metrics:accuracy:0.3924
INFO:root:11:33:42 [Epoch 1 Batch 11600/13130] loss=1.0645, lr=0.0000091, metrics:accuracy:0.3990
INFO:root:11:33:44 [Epoch 3 Batch 5200/13130] loss=1.0926, lr=0.0000116, metrics:accuracy:0.3859
INFO:root:11:33:46 [Epoch 3 Batch 8800/13130] loss=1.0063, lr=0.0000207, metrics:accuracy:0.4857
INFO:root:11:33:48 [Epoch 1 Batch 10000/13130] loss=1.0714, lr=0.0000075, metrics:accuracy:0.3931
INFO:root:11:34:04 [Epoch 1 Batch 12000/13130] loss=1.0703, lr=0.0000091, metrics:accuracy:0.3999
INFO:root:11:34:11 [Epoch 1 Batch 10400/13130] loss=1.0601, lr=0.0000075, metrics:accuracy:0.3947
INFO:root:11:34:16 [Epoch 3 Batch 9200/13130] loss=1.0194, lr=0.0000204, metrics:accuracy:0.4852
INFO:root:11:34:23 AMP: decreasing loss scale to 8192.000000
INFO:root:11:34:24 [Epoch 3 Batch 5600/13130] loss=1.0854, lr=0.0000114, metrics:accuracy:0.3862
INFO:root:11:34:26 [Epoch 1 Batch 12400/13130] loss=1.0638, lr=0.0000090, metrics:accuracy:0.4010
INFO:root:11:34:33 [Epoch 1 Batch 10800/13130] loss=1.0623, lr=0.0000074, metrics:accuracy:0.3963
INFO:root:11:34:47 [Epoch 3 Batch 9600/13130] loss=1.0086, lr=0.0000202, metrics:accuracy:0.4857
INFO:root:11:34:48 [Epoch 1 Batch 12800/13130] loss=1.0645, lr=0.0000089, metrics:accuracy:0.4020
INFO:root:11:34:54 [Epoch 1 Batch 11200/13130] loss=1.0707, lr=0.0000074, metrics:accuracy:0.3973
INFO:root:11:35:06 [Epoch 3 Batch 6000/13130] loss=1.0892, lr=0.0000113, metrics:accuracy:0.3856
INFO:root:11:35:06 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:35:17 [Epoch 1 Batch 11600/13130] loss=1.0650, lr=0.0000073, metrics:accuracy:0.3986
INFO:root:11:35:17 [Batch 400/3750] loss=1.0828, metrics:accuracy:400.0000
INFO:root:11:35:18 [Epoch 3 Batch 10000/13130] loss=1.0028, lr=0.0000199, metrics:accuracy:0.4859
INFO:root:11:35:29 [Batch 800/3750] loss=1.0875, metrics:accuracy:800.0000
INFO:root:11:35:39 [Epoch 1 Batch 12000/13130] loss=1.0684, lr=0.0000073, metrics:accuracy:0.3990
INFO:root:11:35:40 [Batch 1200/3750] loss=1.0857, metrics:accuracy:1200.0000
INFO:root:11:35:47 [Epoch 3 Batch 6400/13130] loss=1.0871, lr=0.0000112, metrics:accuracy:0.3852
INFO:root:11:35:48 [Epoch 3 Batch 10400/13130] loss=1.0065, lr=0.0000196, metrics:accuracy:0.4863
INFO:root:11:35:51 [Batch 1600/3750] loss=1.1225, metrics:accuracy:1600.0000
INFO:root:11:36:01 [Epoch 1 Batch 12400/13130] loss=1.0632, lr=0.0000072, metrics:accuracy:0.4000
INFO:root:11:36:02 [Batch 2000/3750] loss=1.1262, metrics:accuracy:2000.0000
INFO:root:11:36:13 [Batch 2400/3750] loss=1.1153, metrics:accuracy:2400.0000
INFO:root:11:36:18 [Epoch 3 Batch 10800/13130] loss=1.0112, lr=0.0000193, metrics:accuracy:0.4863
INFO:root:11:36:24 [Epoch 1 Batch 12800/13130] loss=1.0671, lr=0.0000072, metrics:accuracy:0.4009
INFO:root:11:36:24 [Batch 2800/3750] loss=0.9916, metrics:accuracy:2800.0000
INFO:root:11:36:28 [Epoch 3 Batch 6800/13130] loss=1.0811, lr=0.0000110, metrics:accuracy:0.3858
INFO:root:11:36:36 [Batch 3200/3750] loss=0.9378, metrics:accuracy:3200.0000
INFO:root:11:36:42 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:36:47 [Batch 3600/3750] loss=0.9411, metrics:accuracy:3600.0000
INFO:root:11:36:48 [Epoch 3 Batch 11200/13130] loss=1.0143, lr=0.0000191, metrics:accuracy:0.4864
INFO:root:11:36:51 validation metrics:accuracy:0.4468
INFO:root:11:36:51 Time cost=104.69s, throughput=286.56 samples/s
INFO:root:11:36:53 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:11:36:53 Time cost=944.45s
INFO:root:11:36:53 [Batch 400/3750] loss=1.1223, metrics:accuracy:400.0000
INFO:root:11:37:05 [Batch 800/3750] loss=1.1316, metrics:accuracy:800.0000
INFO:root:11:37:09 [Epoch 3 Batch 7200/13130] loss=1.0823, lr=0.0000109, metrics:accuracy:0.3860
INFO:root:11:37:15 [Epoch 2 Batch 400/13130] loss=1.0529, lr=0.0000088, metrics:accuracy:0.4522
INFO:root:11:37:16 [Batch 1200/3750] loss=1.1287, metrics:accuracy:1200.0000
INFO:root:11:37:19 [Epoch 3 Batch 11600/13130] loss=1.0081, lr=0.0000188, metrics:accuracy:0.4866
INFO:root:11:37:27 [Batch 1600/3750] loss=1.1208, metrics:accuracy:1600.0000
INFO:root:11:37:37 [Epoch 2 Batch 800/13130] loss=1.0530, lr=0.0000088, metrics:accuracy:0.4462
INFO:root:11:37:38 [Batch 2000/3750] loss=1.1182, metrics:accuracy:2000.0000
INFO:root:11:37:49 [Epoch 3 Batch 12000/13130] loss=0.9981, lr=0.0000185, metrics:accuracy:0.4874
INFO:root:11:37:50 [Epoch 3 Batch 7600/13130] loss=1.0892, lr=0.0000108, metrics:accuracy:0.3859
INFO:root:11:37:50 [Batch 2400/3750] loss=1.1049, metrics:accuracy:2400.0000
INFO:root:11:37:59 [Epoch 2 Batch 1200/13130] loss=1.0462, lr=0.0000087, metrics:accuracy:0.4484
INFO:root:11:38:01 [Batch 2800/3750] loss=0.9709, metrics:accuracy:2800.0000
INFO:root:11:38:12 [Batch 3200/3750] loss=0.9118, metrics:accuracy:3200.0000
INFO:root:11:38:20 [Epoch 3 Batch 12400/13130] loss=0.9957, lr=0.0000183, metrics:accuracy:0.4882
INFO:root:11:38:21 [Epoch 2 Batch 1600/13130] loss=1.0411, lr=0.0000086, metrics:accuracy:0.4505
INFO:root:11:38:23 [Batch 3600/3750] loss=0.9159, metrics:accuracy:3600.0000
INFO:root:11:38:28 validation metrics:accuracy:0.4456
INFO:root:11:38:28 Time cost=105.60s, throughput=284.10 samples/s
INFO:root:11:38:30 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:11:38:30 Time cost=838.56s
INFO:root:11:38:31 [Epoch 3 Batch 8000/13130] loss=1.0887, lr=0.0000106, metrics:accuracy:0.3860
INFO:root:11:38:42 [Epoch 2 Batch 2000/13130] loss=1.0401, lr=0.0000085, metrics:accuracy:0.4531
INFO:root:11:38:50 [Epoch 3 Batch 12800/13130] loss=0.9829, lr=0.0000180, metrics:accuracy:0.4888
INFO:root:11:38:52 [Epoch 2 Batch 400/13130] loss=1.0544, lr=0.0000071, metrics:accuracy:0.4494
INFO:root:11:39:05 [Epoch 2 Batch 2400/13130] loss=1.0404, lr=0.0000085, metrics:accuracy:0.4542
INFO:root:11:39:13 [Epoch 3 Batch 8400/13130] loss=1.0838, lr=0.0000105, metrics:accuracy:0.3865
INFO:root:11:39:14 [Epoch 2 Batch 800/13130] loss=1.0564, lr=0.0000070, metrics:accuracy:0.4448
INFO:root:11:39:15 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:39:26 [Epoch 2 Batch 2800/13130] loss=1.0466, lr=0.0000084, metrics:accuracy:0.4544
INFO:root:11:39:32 [Batch 400/3750] loss=0.9708, metrics:accuracy:7900.0000
INFO:root:11:39:36 [Epoch 2 Batch 1200/13130] loss=1.0471, lr=0.0000069, metrics:accuracy:0.4462
INFO:root:11:39:46 [Epoch 3 Batch 8800/13130] loss=1.0871, lr=0.0000104, metrics:accuracy:0.3864
INFO:root:11:39:48 [Batch 800/3750] loss=0.9842, metrics:accuracy:8300.0000
INFO:root:11:39:49 [Epoch 2 Batch 3200/13130] loss=1.0496, lr=0.0000083, metrics:accuracy:0.4542
INFO:root:11:39:59 [Epoch 2 Batch 1600/13130] loss=1.0422, lr=0.0000069, metrics:accuracy:0.4508
INFO:root:11:40:05 [Batch 1200/3750] loss=0.9796, metrics:accuracy:8700.0000
INFO:root:11:40:11 [Epoch 2 Batch 3600/13130] loss=1.0447, lr=0.0000083, metrics:accuracy:0.4539
INFO:root:11:40:20 [Epoch 3 Batch 9200/13130] loss=1.0858, lr=0.0000102, metrics:accuracy:0.3859
INFO:root:11:40:20 [Epoch 2 Batch 2000/13130] loss=1.0416, lr=0.0000068, metrics:accuracy:0.4531
INFO:root:11:40:21 [Batch 1600/3750] loss=1.1310, metrics:accuracy:9100.0000
INFO:root:11:40:33 [Epoch 2 Batch 4000/13130] loss=1.0402, lr=0.0000082, metrics:accuracy:0.4546
INFO:root:11:40:38 [Batch 2000/3750] loss=1.1465, metrics:accuracy:9500.0000
INFO:root:11:40:43 [Epoch 2 Batch 2400/13130] loss=1.0412, lr=0.0000068, metrics:accuracy:0.4536
INFO:root:11:40:53 AMP: increasing loss scale to 16384.000000
INFO:root:11:40:54 [Epoch 3 Batch 9600/13130] loss=1.0858, lr=0.0000101, metrics:accuracy:0.3857
INFO:root:11:40:54 [Batch 2400/3750] loss=1.1414, metrics:accuracy:9900.0000
INFO:root:11:40:55 [Epoch 2 Batch 4400/13130] loss=1.0495, lr=0.0000081, metrics:accuracy:0.4545
INFO:root:11:41:06 [Epoch 2 Batch 2800/13130] loss=1.0514, lr=0.0000067, metrics:accuracy:0.4529
INFO:root:11:41:11 [Batch 2800/3750] loss=1.0103, metrics:accuracy:10300.0000
INFO:root:11:41:17 [Epoch 2 Batch 4800/13130] loss=1.0434, lr=0.0000081, metrics:accuracy:0.4541
INFO:root:11:41:27 [Batch 3200/3750] loss=0.9621, metrics:accuracy:10700.0000
INFO:root:11:41:27 [Epoch 3 Batch 10000/13130] loss=1.0862, lr=0.0000099, metrics:accuracy:0.3856
INFO:root:11:41:29 [Epoch 2 Batch 3200/13130] loss=1.0517, lr=0.0000067, metrics:accuracy:0.4527
INFO:root:11:41:39 [Epoch 2 Batch 5200/13130] loss=1.0504, lr=0.0000080, metrics:accuracy:0.4540
INFO:root:11:41:43 [Batch 3600/3750] loss=0.9611, metrics:accuracy:11100.0000
INFO:root:11:41:50 validation metrics:accuracy:0.4735
INFO:root:11:41:50 Time cost=154.10s, throughput=194.68 samples/s
INFO:root:11:41:51 [Epoch 2 Batch 3600/13130] loss=1.0462, lr=0.0000066, metrics:accuracy:0.4526
INFO:root:11:41:51 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:11:41:51 Time cost=1143.65s
INFO:root:11:42:01 [Epoch 2 Batch 5600/13130] loss=1.0421, lr=0.0000079, metrics:accuracy:0.4538
INFO:root:11:42:03 [Epoch 3 Batch 10400/13130] loss=1.0840, lr=0.0000098, metrics:accuracy:0.3859
INFO:root:11:42:13 [Epoch 2 Batch 4000/13130] loss=1.0428, lr=0.0000066, metrics:accuracy:0.4536
INFO:root:11:42:22 [Epoch 4 Batch 400/13130] loss=0.9754, lr=0.0000175, metrics:accuracy:0.5311
INFO:root:11:42:23 [Epoch 2 Batch 6000/13130] loss=1.0522, lr=0.0000079, metrics:accuracy:0.4538
INFO:root:11:42:36 [Epoch 2 Batch 4400/13130] loss=1.0524, lr=0.0000065, metrics:accuracy:0.4537
INFO:root:11:42:43 [Epoch 3 Batch 10800/13130] loss=1.0848, lr=0.0000097, metrics:accuracy:0.3865
INFO:root:11:42:46 [Epoch 2 Batch 6400/13130] loss=1.0261, lr=0.0000078, metrics:accuracy:0.4551
INFO:root:11:42:52 [Epoch 4 Batch 800/13130] loss=0.9854, lr=0.0000172, metrics:accuracy:0.5157
INFO:root:11:42:58 [Epoch 2 Batch 4800/13130] loss=1.0438, lr=0.0000065, metrics:accuracy:0.4535
INFO:root:11:43:09 [Epoch 2 Batch 6800/13130] loss=1.0416, lr=0.0000077, metrics:accuracy:0.4549
INFO:root:11:43:21 [Epoch 2 Batch 5200/13130] loss=1.0540, lr=0.0000064, metrics:accuracy:0.4522
INFO:root:11:43:23 [Epoch 4 Batch 1200/13130] loss=0.9700, lr=0.0000170, metrics:accuracy:0.5186
INFO:root:11:43:24 [Epoch 3 Batch 11200/13130] loss=1.0829, lr=0.0000095, metrics:accuracy:0.3864
INFO:root:11:43:30 [Epoch 2 Batch 7200/13130] loss=1.0590, lr=0.0000077, metrics:accuracy:0.4541
INFO:root:11:43:43 [Epoch 2 Batch 5600/13130] loss=1.0405, lr=0.0000064, metrics:accuracy:0.4522
INFO:root:11:43:52 [Epoch 2 Batch 7600/13130] loss=1.0407, lr=0.0000076, metrics:accuracy:0.4544
INFO:root:11:43:54 [Epoch 4 Batch 1600/13130] loss=0.9677, lr=0.0000167, metrics:accuracy:0.5182
INFO:root:11:44:05 [Epoch 2 Batch 6000/13130] loss=1.0530, lr=0.0000063, metrics:accuracy:0.4518
INFO:root:11:44:06 [Epoch 3 Batch 11600/13130] loss=1.0861, lr=0.0000094, metrics:accuracy:0.3862
INFO:root:11:44:13 [Epoch 2 Batch 8000/13130] loss=1.0464, lr=0.0000075, metrics:accuracy:0.4542
INFO:root:11:44:25 [Epoch 4 Batch 2000/13130] loss=0.9538, lr=0.0000164, metrics:accuracy:0.5197
INFO:root:11:44:29 [Epoch 2 Batch 6400/13130] loss=1.0266, lr=0.0000062, metrics:accuracy:0.4532
INFO:root:11:44:35 [Epoch 2 Batch 8400/13130] loss=1.0359, lr=0.0000075, metrics:accuracy:0.4551
INFO:root:11:44:47 [Epoch 3 Batch 12000/13130] loss=1.0846, lr=0.0000093, metrics:accuracy:0.3864
INFO:root:11:44:52 [Epoch 2 Batch 6800/13130] loss=1.0424, lr=0.0000062, metrics:accuracy:0.4534
INFO:root:11:44:56 [Epoch 4 Batch 2400/13130] loss=0.9745, lr=0.0000161, metrics:accuracy:0.5188
INFO:root:11:44:57 [Epoch 2 Batch 8800/13130] loss=1.0577, lr=0.0000074, metrics:accuracy:0.4540
INFO:root:11:45:14 [Epoch 2 Batch 7200/13130] loss=1.0596, lr=0.0000061, metrics:accuracy:0.4525
INFO:root:11:45:20 [Epoch 2 Batch 9200/13130] loss=1.0494, lr=0.0000073, metrics:accuracy:0.4542
INFO:root:11:45:27 [Epoch 4 Batch 2800/13130] loss=0.9631, lr=0.0000159, metrics:accuracy:0.5204
INFO:root:11:45:29 [Epoch 3 Batch 12400/13130] loss=1.0812, lr=0.0000091, metrics:accuracy:0.3869
INFO:root:11:45:36 [Epoch 2 Batch 7600/13130] loss=1.0415, lr=0.0000061, metrics:accuracy:0.4529
INFO:root:11:45:42 [Epoch 2 Batch 9600/13130] loss=1.0352, lr=0.0000073, metrics:accuracy:0.4543
INFO:root:11:45:57 [Epoch 4 Batch 3200/13130] loss=0.9693, lr=0.0000156, metrics:accuracy:0.5200
INFO:root:11:45:57 [Epoch 2 Batch 8000/13130] loss=1.0476, lr=0.0000060, metrics:accuracy:0.4529
INFO:root:11:46:04 [Epoch 2 Batch 10000/13130] loss=1.0469, lr=0.0000072, metrics:accuracy:0.4542
INFO:root:11:46:10 [Epoch 3 Batch 12800/13130] loss=1.0765, lr=0.0000090, metrics:accuracy:0.3876
INFO:root:11:46:20 [Epoch 2 Batch 8400/13130] loss=1.0373, lr=0.0000060, metrics:accuracy:0.4535
INFO:root:11:46:26 [Epoch 2 Batch 10400/13130] loss=1.0336, lr=0.0000071, metrics:accuracy:0.4545
INFO:root:11:46:28 [Epoch 4 Batch 3600/13130] loss=0.9684, lr=0.0000153, metrics:accuracy:0.5193
INFO:root:11:46:42 [Epoch 2 Batch 8800/13130] loss=1.0575, lr=0.0000059, metrics:accuracy:0.4528
INFO:root:11:46:44 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:46:48 [Epoch 2 Batch 10800/13130] loss=1.0431, lr=0.0000071, metrics:accuracy:0.4547
INFO:root:11:46:58 [Epoch 4 Batch 4000/13130] loss=0.9633, lr=0.0000151, metrics:accuracy:0.5195
INFO:root:11:47:02 [Batch 400/3750] loss=1.0359, metrics:accuracy:7900.0000
INFO:root:11:47:05 [Epoch 2 Batch 9200/13130] loss=1.0471, lr=0.0000059, metrics:accuracy:0.4531
INFO:root:11:47:09 [Epoch 2 Batch 11200/13130] loss=1.0309, lr=0.0000070, metrics:accuracy:0.4553
INFO:root:11:47:21 [Batch 800/3750] loss=1.0409, metrics:accuracy:8300.0000
INFO:root:11:47:26 [Epoch 4 Batch 4400/13130] loss=0.9632, lr=0.0000148, metrics:accuracy:0.5205
INFO:root:11:47:27 [Epoch 2 Batch 9600/13130] loss=1.0363, lr=0.0000058, metrics:accuracy:0.4532
INFO:root:11:47:32 [Epoch 2 Batch 11600/13130] loss=1.0407, lr=0.0000069, metrics:accuracy:0.4558
INFO:root:11:47:39 [Batch 1200/3750] loss=1.0417, metrics:accuracy:8700.0000
INFO:root:11:47:50 [Epoch 2 Batch 10000/13130] loss=1.0476, lr=0.0000058, metrics:accuracy:0.4530
INFO:root:11:47:54 [Epoch 4 Batch 4800/13130] loss=0.9733, lr=0.0000145, metrics:accuracy:0.5204
INFO:root:11:47:55 [Epoch 2 Batch 12000/13130] loss=1.0381, lr=0.0000069, metrics:accuracy:0.4559
INFO:root:11:47:57 [Batch 1600/3750] loss=1.0844, metrics:accuracy:9100.0000
INFO:root:11:48:12 [Epoch 2 Batch 10400/13130] loss=1.0369, lr=0.0000057, metrics:accuracy:0.4535
INFO:root:11:48:16 [Batch 2000/3750] loss=1.0832, metrics:accuracy:9500.0000
INFO:root:11:48:16 [Epoch 2 Batch 12400/13130] loss=1.0359, lr=0.0000068, metrics:accuracy:0.4562
INFO:root:11:48:22 [Epoch 4 Batch 5200/13130] loss=0.9889, lr=0.0000142, metrics:accuracy:0.5195
INFO:root:11:48:34 [Epoch 2 Batch 10800/13130] loss=1.0426, lr=0.0000056, metrics:accuracy:0.4535
INFO:root:11:48:34 [Batch 2400/3750] loss=1.0735, metrics:accuracy:9900.0000
INFO:root:11:48:39 [Epoch 2 Batch 12800/13130] loss=1.0168, lr=0.0000067, metrics:accuracy:0.4569
INFO:root:11:48:51 [Epoch 4 Batch 5600/13130] loss=0.9734, lr=0.0000140, metrics:accuracy:0.5203
INFO:root:11:48:53 [Batch 2800/3750] loss=1.1165, metrics:accuracy:10300.0000
INFO:root:11:48:56 [Epoch 2 Batch 11200/13130] loss=1.0326, lr=0.0000056, metrics:accuracy:0.4538
INFO:root:11:48:57 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:49:08 [Batch 400/3750] loss=0.9980, metrics:accuracy:4150.0000
INFO:root:11:49:11 [Batch 3200/3750] loss=1.1154, metrics:accuracy:10700.0000
INFO:root:11:49:19 [Epoch 4 Batch 6000/13130] loss=0.9750, lr=0.0000137, metrics:accuracy:0.5204
INFO:root:11:49:19 [Epoch 2 Batch 11600/13130] loss=1.0429, lr=0.0000055, metrics:accuracy:0.4539
INFO:root:11:49:19 [Batch 800/3750] loss=1.0083, metrics:accuracy:4550.0000
INFO:root:11:49:29 [Batch 3600/3750] loss=1.1269, metrics:accuracy:11100.0000
INFO:root:11:49:30 [Batch 1200/3750] loss=1.0039, metrics:accuracy:4950.0000
INFO:root:11:49:36 validation metrics:accuracy:0.3990
INFO:root:11:49:36 Time cost=172.02s, throughput=174.40 samples/s
INFO:root:11:49:37 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:11:49:37 Time cost=1485.76s
INFO:root:11:49:42 [Epoch 2 Batch 12000/13130] loss=1.0381, lr=0.0000055, metrics:accuracy:0.4544
INFO:root:11:49:42 [Batch 1600/3750] loss=1.1200, metrics:accuracy:5350.0000
INFO:root:11:49:47 [Epoch 4 Batch 6400/13130] loss=0.9781, lr=0.0000134, metrics:accuracy:0.5204
INFO:root:11:49:53 [Batch 2000/3750] loss=1.1339, metrics:accuracy:5750.0000
INFO:root:11:50:03 [Epoch 2 Batch 12400/13130] loss=1.0384, lr=0.0000054, metrics:accuracy:0.4546
INFO:root:11:50:04 [Batch 2400/3750] loss=1.1177, metrics:accuracy:6150.0000
INFO:root:11:50:15 [Batch 2800/3750] loss=1.0100, metrics:accuracy:6550.0000
INFO:root:11:50:17 [Epoch 4 Batch 6800/13130] loss=0.9696, lr=0.0000132, metrics:accuracy:0.5205
INFO:root:11:50:19 [Epoch 4 Batch 400/13130] loss=1.0847, lr=0.0000087, metrics:accuracy:0.4008
INFO:root:11:50:25 AMP: increasing loss scale to 32768.000000
INFO:root:11:50:26 [Epoch 2 Batch 12800/13130] loss=1.0184, lr=0.0000054, metrics:accuracy:0.4556
INFO:root:11:50:26 [Batch 3200/3750] loss=0.9533, metrics:accuracy:6950.0000
INFO:root:11:50:37 [Batch 3600/3750] loss=0.9571, metrics:accuracy:7350.0000
INFO:root:11:50:41 validation metrics:accuracy:0.4664
INFO:root:11:50:41 Time cost=104.45s, throughput=287.21 samples/s
INFO:root:11:50:42 AMP: decreasing loss scale to 16384.000000
INFO:root:11:50:43 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:11:50:43 Time cost=830.09s
INFO:root:11:50:44 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:50:47 [Epoch 4 Batch 7200/13130] loss=0.9591, lr=0.0000129, metrics:accuracy:0.5204
INFO:root:11:50:55 [Batch 400/3750] loss=1.0119, metrics:accuracy:4150.0000
INFO:root:11:51:00 [Epoch 4 Batch 800/13130] loss=1.0899, lr=0.0000086, metrics:accuracy:0.3872
INFO:root:11:51:05 [Epoch 3 Batch 400/13130] loss=1.0197, lr=0.0000066, metrics:accuracy:0.4672
INFO:root:11:51:07 [Batch 800/3750] loss=1.0219, metrics:accuracy:4550.0000
INFO:root:11:51:17 [Epoch 4 Batch 7600/13130] loss=0.9744, lr=0.0000126, metrics:accuracy:0.5198
INFO:root:11:51:18 [Batch 1200/3750] loss=1.0162, metrics:accuracy:4950.0000
INFO:root:11:51:27 [Epoch 3 Batch 800/13130] loss=1.0176, lr=0.0000065, metrics:accuracy:0.4791
INFO:root:11:51:29 [Batch 1600/3750] loss=1.0899, metrics:accuracy:5350.0000
INFO:root:11:51:40 [Batch 2000/3750] loss=1.0943, metrics:accuracy:5750.0000
INFO:root:11:51:41 [Epoch 4 Batch 1200/13130] loss=1.0767, lr=0.0000085, metrics:accuracy:0.3923
INFO:root:11:51:48 [Epoch 4 Batch 8000/13130] loss=0.9553, lr=0.0000124, metrics:accuracy:0.5199
INFO:root:11:51:49 [Epoch 3 Batch 1200/13130] loss=1.0264, lr=0.0000065, metrics:accuracy:0.4758
INFO:root:11:51:51 [Batch 2400/3750] loss=1.0813, metrics:accuracy:6150.0000
INFO:root:11:52:03 [Batch 2800/3750] loss=1.0161, metrics:accuracy:6550.0000
INFO:root:11:52:11 [Epoch 3 Batch 1600/13130] loss=1.0174, lr=0.0000064, metrics:accuracy:0.4752
INFO:root:11:52:14 [Batch 3200/3750] loss=0.9750, metrics:accuracy:6950.0000
INFO:root:11:52:18 [Epoch 4 Batch 8400/13130] loss=0.9595, lr=0.0000121, metrics:accuracy:0.5199
INFO:root:11:52:22 [Epoch 4 Batch 1600/13130] loss=1.0890, lr=0.0000083, metrics:accuracy:0.3905
INFO:root:11:52:25 [Batch 3600/3750] loss=0.9797, metrics:accuracy:7350.0000
INFO:root:11:52:29 validation metrics:accuracy:0.4698
INFO:root:11:52:29 Time cost=105.04s, throughput=285.62 samples/s
INFO:root:11:52:31 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:11:52:31 Time cost=841.18s
INFO:root:11:52:34 [Epoch 3 Batch 2000/13130] loss=1.0158, lr=0.0000063, metrics:accuracy:0.4756
INFO:root:11:52:48 [Epoch 4 Batch 8800/13130] loss=0.9624, lr=0.0000118, metrics:accuracy:0.5204
INFO:root:11:52:54 [Epoch 3 Batch 400/13130] loss=1.0211, lr=0.0000053, metrics:accuracy:0.4703
INFO:root:11:52:55 [Epoch 3 Batch 2400/13130] loss=1.0251, lr=0.0000063, metrics:accuracy:0.4752
INFO:root:11:53:04 [Epoch 4 Batch 2000/13130] loss=1.0789, lr=0.0000082, metrics:accuracy:0.3938
INFO:root:11:53:16 [Epoch 3 Batch 800/13130] loss=1.0212, lr=0.0000052, metrics:accuracy:0.4733
INFO:root:11:53:18 [Epoch 3 Batch 2800/13130] loss=1.0131, lr=0.0000062, metrics:accuracy:0.4774
INFO:root:11:53:18 [Epoch 4 Batch 9200/13130] loss=0.9674, lr=0.0000115, metrics:accuracy:0.5207
INFO:root:11:53:38 [Epoch 3 Batch 1200/13130] loss=1.0284, lr=0.0000052, metrics:accuracy:0.4751
INFO:root:11:53:40 [Epoch 3 Batch 3200/13130] loss=1.0090, lr=0.0000061, metrics:accuracy:0.4797
INFO:root:11:53:44 [Epoch 4 Batch 2400/13130] loss=1.0847, lr=0.0000081, metrics:accuracy:0.3938
INFO:root:11:53:48 [Epoch 4 Batch 9600/13130] loss=0.9772, lr=0.0000113, metrics:accuracy:0.5203
INFO:root:11:54:00 [Epoch 3 Batch 1600/13130] loss=1.0188, lr=0.0000051, metrics:accuracy:0.4747
INFO:root:11:54:02 [Epoch 3 Batch 3600/13130] loss=1.0154, lr=0.0000061, metrics:accuracy:0.4804
INFO:root:11:54:19 [Epoch 4 Batch 10000/13130] loss=0.9708, lr=0.0000110, metrics:accuracy:0.5203
INFO:root:11:54:23 [Epoch 3 Batch 2000/13130] loss=1.0205, lr=0.0000051, metrics:accuracy:0.4753
INFO:root:11:54:24 [Epoch 3 Batch 4000/13130] loss=1.0103, lr=0.0000060, metrics:accuracy:0.4819
INFO:root:11:54:25 [Epoch 4 Batch 2800/13130] loss=1.0810, lr=0.0000079, metrics:accuracy:0.3946
INFO:root:11:54:44 [Epoch 3 Batch 2400/13130] loss=1.0296, lr=0.0000050, metrics:accuracy:0.4746
INFO:root:11:54:45 [Epoch 3 Batch 4400/13130] loss=1.0261, lr=0.0000059, metrics:accuracy:0.4807
INFO:root:11:54:49 [Epoch 4 Batch 10400/13130] loss=0.9595, lr=0.0000107, metrics:accuracy:0.5209
INFO:root:11:55:06 [Epoch 4 Batch 3200/13130] loss=1.0815, lr=0.0000078, metrics:accuracy:0.3939
INFO:root:11:55:07 [Epoch 3 Batch 2800/13130] loss=1.0178, lr=0.0000050, metrics:accuracy:0.4766
INFO:root:11:55:08 [Epoch 3 Batch 4800/13130] loss=1.0135, lr=0.0000059, metrics:accuracy:0.4805
INFO:root:11:55:20 [Epoch 4 Batch 10800/13130] loss=0.9689, lr=0.0000105, metrics:accuracy:0.5207
INFO:root:11:55:29 [Epoch 3 Batch 3200/13130] loss=1.0121, lr=0.0000049, metrics:accuracy:0.4783
INFO:root:11:55:30 [Epoch 3 Batch 5200/13130] loss=1.0308, lr=0.0000058, metrics:accuracy:0.4798
INFO:root:11:55:47 [Epoch 4 Batch 3600/13130] loss=1.0856, lr=0.0000077, metrics:accuracy:0.3932
INFO:root:11:55:50 [Epoch 4 Batch 11200/13130] loss=0.9565, lr=0.0000102, metrics:accuracy:0.5211
INFO:root:11:55:52 [Epoch 3 Batch 3600/13130] loss=1.0189, lr=0.0000048, metrics:accuracy:0.4783
INFO:root:11:55:52 [Epoch 3 Batch 5600/13130] loss=1.0121, lr=0.0000057, metrics:accuracy:0.4797
INFO:root:11:56:14 [Epoch 3 Batch 4000/13130] loss=1.0153, lr=0.0000048, metrics:accuracy:0.4787
INFO:root:11:56:14 [Epoch 3 Batch 6000/13130] loss=1.0208, lr=0.0000056, metrics:accuracy:0.4797
INFO:root:11:56:21 [Epoch 4 Batch 11600/13130] loss=0.9603, lr=0.0000099, metrics:accuracy:0.5211
INFO:root:11:56:28 [Epoch 4 Batch 4000/13130] loss=1.0856, lr=0.0000075, metrics:accuracy:0.3935
INFO:root:11:56:36 [Epoch 3 Batch 4400/13130] loss=1.0312, lr=0.0000047, metrics:accuracy:0.4780
INFO:root:11:56:36 [Epoch 3 Batch 6400/13130] loss=1.0307, lr=0.0000056, metrics:accuracy:0.4792
INFO:root:11:56:51 [Epoch 4 Batch 12000/13130] loss=0.9716, lr=0.0000096, metrics:accuracy:0.5213
INFO:root:11:56:58 [Epoch 3 Batch 6800/13130] loss=1.0020, lr=0.0000055, metrics:accuracy:0.4805
INFO:root:11:56:59 [Epoch 3 Batch 4800/13130] loss=1.0137, lr=0.0000047, metrics:accuracy:0.4787
INFO:root:11:57:09 [Epoch 4 Batch 4400/13130] loss=1.0850, lr=0.0000074, metrics:accuracy:0.3925
INFO:root:11:57:20 [Epoch 3 Batch 7200/13130] loss=1.0307, lr=0.0000054, metrics:accuracy:0.4799
INFO:root:11:57:21 [Epoch 3 Batch 5200/13130] loss=1.0341, lr=0.0000046, metrics:accuracy:0.4779
INFO:root:11:57:21 [Epoch 4 Batch 12400/13130] loss=0.9519, lr=0.0000094, metrics:accuracy:0.5215
INFO:root:11:57:32 AMP: increasing loss scale to 32768.000000
INFO:root:11:57:42 [Epoch 3 Batch 7600/13130] loss=1.0234, lr=0.0000054, metrics:accuracy:0.4794
INFO:root:11:57:43 [Epoch 3 Batch 5600/13130] loss=1.0138, lr=0.0000046, metrics:accuracy:0.4779
INFO:root:11:57:51 [Epoch 4 Batch 4800/13130] loss=1.0851, lr=0.0000073, metrics:accuracy:0.3925
INFO:root:11:57:51 [Epoch 4 Batch 12800/13130] loss=0.9688, lr=0.0000091, metrics:accuracy:0.5217
INFO:root:11:58:04 [Epoch 3 Batch 8000/13130] loss=1.0196, lr=0.0000053, metrics:accuracy:0.4794
INFO:root:11:58:06 [Epoch 3 Batch 6000/13130] loss=1.0221, lr=0.0000045, metrics:accuracy:0.4781
INFO:root:11:58:16 Now we are doing evaluation on dev with gpu(0).
INFO:root:11:58:25 [Epoch 3 Batch 8400/13130] loss=1.0097, lr=0.0000052, metrics:accuracy:0.4803
INFO:root:11:58:28 [Epoch 3 Batch 6400/13130] loss=1.0334, lr=0.0000045, metrics:accuracy:0.4770
INFO:root:11:58:29 [Epoch 4 Batch 5200/13130] loss=1.0848, lr=0.0000071, metrics:accuracy:0.3926
INFO:root:11:58:32 AMP: decreasing loss scale to 16384.000000
INFO:root:11:58:33 [Batch 400/3750] loss=1.0235, metrics:accuracy:11650.0000
INFO:root:11:58:47 [Epoch 3 Batch 8800/13130] loss=1.0103, lr=0.0000052, metrics:accuracy:0.4803
INFO:root:11:58:50 [Batch 800/3750] loss=1.0475, metrics:accuracy:12050.0000
INFO:root:11:58:51 [Epoch 3 Batch 6800/13130] loss=1.0078, lr=0.0000044, metrics:accuracy:0.4778
INFO:root:11:59:03 [Epoch 4 Batch 5600/13130] loss=1.0847, lr=0.0000070, metrics:accuracy:0.3926
INFO:root:11:59:06 [Batch 1200/3750] loss=1.0379, metrics:accuracy:12450.0000
INFO:root:11:59:09 [Epoch 3 Batch 9200/13130] loss=1.0262, lr=0.0000051, metrics:accuracy:0.4796
INFO:root:11:59:14 [Epoch 3 Batch 7200/13130] loss=1.0342, lr=0.0000044, metrics:accuracy:0.4776
INFO:root:11:59:23 [Batch 1600/3750] loss=1.0569, metrics:accuracy:12850.0000
INFO:root:11:59:31 [Epoch 3 Batch 9600/13130] loss=1.0153, lr=0.0000050, metrics:accuracy:0.4796
INFO:root:11:59:35 [Epoch 3 Batch 7600/13130] loss=1.0259, lr=0.0000043, metrics:accuracy:0.4772
INFO:root:11:59:36 [Epoch 4 Batch 6000/13130] loss=1.0833, lr=0.0000069, metrics:accuracy:0.3925
INFO:root:11:59:39 [Batch 2000/3750] loss=1.0480, metrics:accuracy:13250.0000
INFO:root:11:59:53 [Epoch 3 Batch 10000/13130] loss=1.0078, lr=0.0000050, metrics:accuracy:0.4801
INFO:root:11:59:56 [Batch 2400/3750] loss=1.0434, metrics:accuracy:13650.0000
INFO:root:11:59:58 [Epoch 3 Batch 8000/13130] loss=1.0217, lr=0.0000042, metrics:accuracy:0.4774
INFO:root:12:00:10 [Epoch 4 Batch 6400/13130] loss=1.0873, lr=0.0000067, metrics:accuracy:0.3926
INFO:root:12:00:12 [Batch 2800/3750] loss=1.0301, metrics:accuracy:14050.0000
INFO:root:12:00:16 [Epoch 3 Batch 10400/13130] loss=1.0056, lr=0.0000049, metrics:accuracy:0.4805
INFO:root:12:00:20 [Epoch 3 Batch 8400/13130] loss=1.0109, lr=0.0000042, metrics:accuracy:0.4783
INFO:root:12:00:29 [Batch 3200/3750] loss=1.0164, metrics:accuracy:14450.0000
INFO:root:12:00:37 [Epoch 3 Batch 10800/13130] loss=1.0150, lr=0.0000048, metrics:accuracy:0.4805
INFO:root:12:00:42 [Epoch 3 Batch 8800/13130] loss=1.0121, lr=0.0000041, metrics:accuracy:0.4786
INFO:root:12:00:43 [Epoch 4 Batch 6800/13130] loss=1.0773, lr=0.0000066, metrics:accuracy:0.3934
INFO:root:12:00:45 [Batch 3600/3750] loss=1.0186, metrics:accuracy:14850.0000
INFO:root:12:00:52 validation metrics:accuracy:0.4757
INFO:root:12:00:52 Time cost=155.17s, throughput=193.33 samples/s
INFO:root:12:00:53 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:12:00:53 Time cost=1142.29s
INFO:root:12:00:59 [Epoch 3 Batch 11200/13130] loss=1.0234, lr=0.0000048, metrics:accuracy:0.4803
INFO:root:12:01:04 [Epoch 3 Batch 9200/13130] loss=1.0315, lr=0.0000041, metrics:accuracy:0.4779
INFO:root:12:01:20 [Epoch 4 Batch 7200/13130] loss=1.0825, lr=0.0000064, metrics:accuracy:0.3935
INFO:root:12:01:22 [Epoch 3 Batch 11600/13130] loss=1.0087, lr=0.0000047, metrics:accuracy:0.4808
INFO:root:12:01:24 [Epoch 5 Batch 400/13130] loss=0.9209, lr=0.0000086, metrics:accuracy:0.5564
INFO:root:12:01:27 [Epoch 3 Batch 9600/13130] loss=1.0178, lr=0.0000040, metrics:accuracy:0.4781
INFO:root:12:01:44 [Epoch 3 Batch 12000/13130] loss=0.9990, lr=0.0000046, metrics:accuracy:0.4815
INFO:root:12:01:44 AMP: decreasing loss scale to 8192.000000
INFO:root:12:01:49 [Epoch 3 Batch 10000/13130] loss=1.0146, lr=0.0000040, metrics:accuracy:0.4782
INFO:root:12:01:55 [Epoch 5 Batch 800/13130] loss=0.9407, lr=0.0000083, metrics:accuracy:0.5465
INFO:root:12:02:01 [Epoch 4 Batch 7600/13130] loss=1.0834, lr=0.0000063, metrics:accuracy:0.3938
INFO:root:12:02:06 [Epoch 3 Batch 12400/13130] loss=0.9999, lr=0.0000046, metrics:accuracy:0.4820
INFO:root:12:02:12 [Epoch 3 Batch 10400/13130] loss=1.0078, lr=0.0000039, metrics:accuracy:0.4784
INFO:root:12:02:26 [Epoch 5 Batch 1200/13130] loss=0.8991, lr=0.0000081, metrics:accuracy:0.5523
INFO:root:12:02:27 [Epoch 3 Batch 12800/13130] loss=0.9884, lr=0.0000045, metrics:accuracy:0.4828
INFO:root:12:02:34 [Epoch 3 Batch 10800/13130] loss=1.0179, lr=0.0000039, metrics:accuracy:0.4785
INFO:root:12:02:41 [Epoch 4 Batch 8000/13130] loss=1.0851, lr=0.0000062, metrics:accuracy:0.3937
INFO:root:12:02:46 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:02:56 [Epoch 3 Batch 11200/13130] loss=1.0252, lr=0.0000038, metrics:accuracy:0.4783
INFO:root:12:02:56 [Epoch 5 Batch 1600/13130] loss=0.9153, lr=0.0000078, metrics:accuracy:0.5545
INFO:root:12:02:57 [Batch 400/3750] loss=0.9511, metrics:accuracy:7900.0000
INFO:root:12:03:08 [Batch 800/3750] loss=0.9634, metrics:accuracy:8300.0000
INFO:root:12:03:18 [Epoch 3 Batch 11600/13130] loss=1.0147, lr=0.0000038, metrics:accuracy:0.4786
INFO:root:12:03:19 [Batch 1200/3750] loss=0.9577, metrics:accuracy:8700.0000
INFO:root:12:03:22 [Epoch 4 Batch 8400/13130] loss=1.0829, lr=0.0000060, metrics:accuracy:0.3935
INFO:root:12:03:26 [Epoch 5 Batch 2000/13130] loss=0.9214, lr=0.0000075, metrics:accuracy:0.5538
INFO:root:12:03:30 [Batch 1600/3750] loss=1.1541, metrics:accuracy:9100.0000
INFO:root:12:03:41 [Epoch 3 Batch 12000/13130] loss=1.0001, lr=0.0000037, metrics:accuracy:0.4792
INFO:root:12:03:42 [Batch 2000/3750] loss=1.1687, metrics:accuracy:9500.0000
INFO:root:12:03:53 [Batch 2400/3750] loss=1.1594, metrics:accuracy:9900.0000
INFO:root:12:03:57 [Epoch 5 Batch 2400/13130] loss=0.9267, lr=0.0000073, metrics:accuracy:0.5523
INFO:root:12:04:03 [Epoch 3 Batch 12400/13130] loss=1.0033, lr=0.0000037, metrics:accuracy:0.4800
INFO:root:12:04:04 [Epoch 4 Batch 8800/13130] loss=1.0835, lr=0.0000059, metrics:accuracy:0.3939
INFO:root:12:04:04 [Batch 2800/3750] loss=1.0126, metrics:accuracy:10300.0000
INFO:root:12:04:15 [Batch 3200/3750] loss=0.9480, metrics:accuracy:10700.0000
INFO:root:12:04:25 [Epoch 3 Batch 12800/13130] loss=0.9915, lr=0.0000036, metrics:accuracy:0.4811
INFO:root:12:04:26 [Batch 3600/3750] loss=0.9510, metrics:accuracy:11100.0000
INFO:root:12:04:27 [Epoch 5 Batch 2800/13130] loss=0.9233, lr=0.0000070, metrics:accuracy:0.5503
INFO:root:12:04:31 validation metrics:accuracy:0.4725
INFO:root:12:04:31 Time cost=104.69s, throughput=286.55 samples/s
INFO:root:12:04:32 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:12:04:32 Time cost=829.24s
INFO:root:12:04:44 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:04:45 [Epoch 4 Batch 9200/13130] loss=1.0803, lr=0.0000058, metrics:accuracy:0.3944
INFO:root:12:04:54 [Epoch 4 Batch 400/13130] loss=0.9886, lr=0.0000044, metrics:accuracy:0.5130
INFO:root:12:04:55 [Batch 400/3750] loss=1.0062, metrics:accuracy:7900.0000
INFO:root:12:04:58 [Epoch 5 Batch 3200/13130] loss=0.9235, lr=0.0000067, metrics:accuracy:0.5499
INFO:root:12:05:06 [Batch 800/3750] loss=1.0158, metrics:accuracy:8300.0000
INFO:root:12:05:16 [Epoch 4 Batch 800/13130] loss=0.9960, lr=0.0000043, metrics:accuracy:0.5054
INFO:root:12:05:17 [Batch 1200/3750] loss=1.0080, metrics:accuracy:8700.0000
INFO:root:12:05:25 [Epoch 4 Batch 9600/13130] loss=1.0806, lr=0.0000056, metrics:accuracy:0.3947
INFO:root:12:05:28 [Epoch 5 Batch 3600/13130] loss=0.9290, lr=0.0000064, metrics:accuracy:0.5495
INFO:root:12:05:29 [Batch 1600/3750] loss=1.1008, metrics:accuracy:9100.0000
INFO:root:12:05:38 [Epoch 4 Batch 1200/13130] loss=0.9931, lr=0.0000042, metrics:accuracy:0.5073
INFO:root:12:05:40 [Batch 2000/3750] loss=1.1020, metrics:accuracy:9500.0000
INFO:root:12:05:51 [Batch 2400/3750] loss=1.0913, metrics:accuracy:9900.0000
INFO:root:12:05:59 [Epoch 5 Batch 4000/13130] loss=0.9313, lr=0.0000062, metrics:accuracy:0.5494
INFO:root:12:06:01 [Epoch 4 Batch 1600/13130] loss=0.9843, lr=0.0000042, metrics:accuracy:0.5064
INFO:root:12:06:02 [Batch 2800/3750] loss=1.0002, metrics:accuracy:10300.0000
INFO:root:12:06:06 [Epoch 4 Batch 10000/13130] loss=1.0842, lr=0.0000055, metrics:accuracy:0.3944
INFO:root:12:06:13 [Batch 3200/3750] loss=0.9512, metrics:accuracy:10700.0000
INFO:root:12:06:24 [Epoch 4 Batch 2000/13130] loss=0.9739, lr=0.0000041, metrics:accuracy:0.5053
INFO:root:12:06:25 [Batch 3600/3750] loss=0.9554, metrics:accuracy:11100.0000
INFO:root:12:06:29 validation metrics:accuracy:0.4749
INFO:root:12:06:29 Time cost=104.82s, throughput=286.20 samples/s
INFO:root:12:06:29 [Epoch 5 Batch 4400/13130] loss=0.9080, lr=0.0000059, metrics:accuracy:0.5506
INFO:root:12:06:30 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:12:06:30 Time cost=839.42s
INFO:root:12:06:46 [Epoch 4 Batch 2400/13130] loss=0.9903, lr=0.0000040, metrics:accuracy:0.5069
INFO:root:12:06:47 [Epoch 4 Batch 10400/13130] loss=1.0787, lr=0.0000054, metrics:accuracy:0.3947
INFO:root:12:06:53 [Epoch 4 Batch 400/13130] loss=0.9948, lr=0.0000035, metrics:accuracy:0.5105
INFO:root:12:06:59 [Epoch 5 Batch 4800/13130] loss=0.9086, lr=0.0000056, metrics:accuracy:0.5519
INFO:root:12:07:09 [Epoch 4 Batch 2800/13130] loss=0.9844, lr=0.0000040, metrics:accuracy:0.5061
INFO:root:12:07:15 [Epoch 4 Batch 800/13130] loss=1.0010, lr=0.0000034, metrics:accuracy:0.5032
INFO:root:12:07:28 [Epoch 4 Batch 10800/13130] loss=1.0802, lr=0.0000052, metrics:accuracy:0.3944
INFO:root:12:07:30 [Epoch 5 Batch 5200/13130] loss=0.9022, lr=0.0000054, metrics:accuracy:0.5532
INFO:root:12:07:31 [Epoch 4 Batch 3200/13130] loss=0.9897, lr=0.0000039, metrics:accuracy:0.5056
INFO:root:12:07:38 [Epoch 4 Batch 1200/13130] loss=1.0007, lr=0.0000034, metrics:accuracy:0.5008
INFO:root:12:07:53 [Epoch 4 Batch 3600/13130] loss=0.9825, lr=0.0000038, metrics:accuracy:0.5063
INFO:root:12:08:00 [Epoch 5 Batch 5600/13130] loss=0.9326, lr=0.0000051, metrics:accuracy:0.5528
INFO:root:12:08:00 [Epoch 4 Batch 1600/13130] loss=0.9925, lr=0.0000033, metrics:accuracy:0.4996
INFO:root:12:08:09 [Epoch 4 Batch 11200/13130] loss=1.0830, lr=0.0000051, metrics:accuracy:0.3943
INFO:root:12:08:15 [Epoch 4 Batch 4000/13130] loss=0.9832, lr=0.0000038, metrics:accuracy:0.5069
INFO:root:12:08:23 [Epoch 4 Batch 2000/13130] loss=0.9838, lr=0.0000033, metrics:accuracy:0.5004
INFO:root:12:08:30 [Epoch 5 Batch 6000/13130] loss=0.9157, lr=0.0000048, metrics:accuracy:0.5535
INFO:root:12:08:33 AMP: increasing loss scale to 16384.000000
INFO:root:12:08:38 [Epoch 4 Batch 4400/13130] loss=0.9861, lr=0.0000037, metrics:accuracy:0.5068
INFO:root:12:08:46 [Epoch 4 Batch 2400/13130] loss=0.9979, lr=0.0000032, metrics:accuracy:0.5008
INFO:root:12:08:51 [Epoch 4 Batch 11600/13130] loss=1.0860, lr=0.0000050, metrics:accuracy:0.3943
INFO:root:12:08:59 [Epoch 4 Batch 4800/13130] loss=0.9960, lr=0.0000036, metrics:accuracy:0.5054
INFO:root:12:09:01 [Epoch 5 Batch 6400/13130] loss=0.9088, lr=0.0000045, metrics:accuracy:0.5535
INFO:root:12:09:09 [Epoch 4 Batch 2800/13130] loss=0.9929, lr=0.0000032, metrics:accuracy:0.5006
INFO:root:12:09:21 [Epoch 4 Batch 5200/13130] loss=0.9958, lr=0.0000036, metrics:accuracy:0.5048
INFO:root:12:09:31 [Epoch 4 Batch 3200/13130] loss=0.9948, lr=0.0000031, metrics:accuracy:0.4993
INFO:root:12:09:31 [Epoch 5 Batch 6800/13130] loss=0.9073, lr=0.0000043, metrics:accuracy:0.5546
INFO:root:12:09:32 [Epoch 4 Batch 12000/13130] loss=1.0808, lr=0.0000048, metrics:accuracy:0.3942
INFO:root:12:09:44 [Epoch 4 Batch 5600/13130] loss=0.9882, lr=0.0000035, metrics:accuracy:0.5053
INFO:root:12:09:54 [Epoch 4 Batch 3600/13130] loss=0.9903, lr=0.0000031, metrics:accuracy:0.5002
INFO:root:12:10:01 [Epoch 5 Batch 7200/13130] loss=0.9052, lr=0.0000040, metrics:accuracy:0.5548
INFO:root:12:10:06 [Epoch 4 Batch 6000/13130] loss=1.0014, lr=0.0000034, metrics:accuracy:0.5046
INFO:root:12:10:13 [Epoch 4 Batch 12400/13130] loss=1.0808, lr=0.0000047, metrics:accuracy:0.3946
INFO:root:12:10:16 [Epoch 4 Batch 4000/13130] loss=0.9884, lr=0.0000030, metrics:accuracy:0.5016
INFO:root:12:10:28 [Epoch 4 Batch 6400/13130] loss=0.9943, lr=0.0000034, metrics:accuracy:0.5045
INFO:root:12:10:31 [Epoch 5 Batch 7600/13130] loss=0.9299, lr=0.0000037, metrics:accuracy:0.5548
INFO:root:12:10:39 [Epoch 4 Batch 4400/13130] loss=0.9925, lr=0.0000030, metrics:accuracy:0.5018
INFO:root:12:10:50 [Epoch 4 Batch 6800/13130] loss=0.9881, lr=0.0000033, metrics:accuracy:0.5044
INFO:root:12:10:55 [Epoch 4 Batch 12800/13130] loss=1.0870, lr=0.0000045, metrics:accuracy:0.3941
INFO:root:12:11:01 [Epoch 4 Batch 4800/13130] loss=1.0004, lr=0.0000029, metrics:accuracy:0.5010
INFO:root:12:11:02 [Epoch 5 Batch 8000/13130] loss=0.9133, lr=0.0000035, metrics:accuracy:0.5550
INFO:root:12:11:11 [Epoch 4 Batch 7200/13130] loss=0.9840, lr=0.0000032, metrics:accuracy:0.5041
INFO:root:12:11:23 [Epoch 4 Batch 5200/13130] loss=1.0038, lr=0.0000028, metrics:accuracy:0.5001
INFO:root:12:11:28 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:11:31 [Epoch 5 Batch 8400/13130] loss=0.9069, lr=0.0000032, metrics:accuracy:0.5558
INFO:root:12:11:33 [Epoch 4 Batch 7600/13130] loss=0.9981, lr=0.0000032, metrics:accuracy:0.5036
INFO:root:12:11:46 [Epoch 4 Batch 5600/13130] loss=1.0013, lr=0.0000028, metrics:accuracy:0.5005
INFO:root:12:11:47 [Batch 400/3750] loss=1.0400, metrics:accuracy:11650.0000
INFO:root:12:11:55 [Epoch 4 Batch 8000/13130] loss=0.9852, lr=0.0000031, metrics:accuracy:0.5037
INFO:root:12:12:00 [Epoch 5 Batch 8800/13130] loss=0.9457, lr=0.0000029, metrics:accuracy:0.5549
INFO:root:12:12:05 [Batch 800/3750] loss=1.0460, metrics:accuracy:12050.0000
INFO:root:12:12:08 [Epoch 4 Batch 6000/13130] loss=1.0075, lr=0.0000027, metrics:accuracy:0.5006
INFO:root:12:12:17 [Epoch 4 Batch 8400/13130] loss=0.9841, lr=0.0000030, metrics:accuracy:0.5043
INFO:root:12:12:24 [Batch 1200/3750] loss=1.0467, metrics:accuracy:12450.0000
INFO:root:12:12:28 [Epoch 5 Batch 9200/13130] loss=0.9051, lr=0.0000026, metrics:accuracy:0.5554
INFO:root:12:12:30 [Epoch 4 Batch 6400/13130] loss=1.0030, lr=0.0000027, metrics:accuracy:0.5003
INFO:root:12:12:39 [Epoch 4 Batch 8800/13130] loss=0.9761, lr=0.0000030, metrics:accuracy:0.5045
INFO:root:12:12:42 [Batch 1600/3750] loss=1.0868, metrics:accuracy:12850.0000
INFO:root:12:12:52 [Epoch 4 Batch 6800/13130] loss=0.9955, lr=0.0000026, metrics:accuracy:0.5006
INFO:root:12:12:56 [Epoch 5 Batch 9600/13130] loss=0.8996, lr=0.0000024, metrics:accuracy:0.5562
INFO:root:12:13:00 [Epoch 4 Batch 9200/13130] loss=0.9890, lr=0.0000029, metrics:accuracy:0.5045
INFO:root:12:13:01 [Batch 2000/3750] loss=1.0844, metrics:accuracy:13250.0000
INFO:root:12:13:15 [Epoch 4 Batch 7200/13130] loss=0.9902, lr=0.0000026, metrics:accuracy:0.5003
INFO:root:12:13:19 [Batch 2400/3750] loss=1.0748, metrics:accuracy:13650.0000
INFO:root:12:13:22 [Epoch 4 Batch 9600/13130] loss=0.9965, lr=0.0000028, metrics:accuracy:0.5039
INFO:root:12:13:24 [Epoch 5 Batch 10000/13130] loss=0.9081, lr=0.0000021, metrics:accuracy:0.5564
INFO:root:12:13:37 [Epoch 4 Batch 7600/13130] loss=1.0050, lr=0.0000025, metrics:accuracy:0.5002
INFO:root:12:13:37 [Batch 2800/3750] loss=1.1072, metrics:accuracy:14050.0000
INFO:root:12:13:44 [Epoch 4 Batch 10000/13130] loss=0.9936, lr=0.0000027, metrics:accuracy:0.5036
INFO:root:12:13:52 [Epoch 5 Batch 10400/13130] loss=0.9162, lr=0.0000018, metrics:accuracy:0.5565
INFO:root:12:13:56 [Batch 3200/3750] loss=1.1031, metrics:accuracy:14450.0000
INFO:root:12:13:59 [Epoch 4 Batch 8000/13130] loss=0.9885, lr=0.0000025, metrics:accuracy:0.5003
INFO:root:12:14:06 [Epoch 4 Batch 10400/13130] loss=0.9777, lr=0.0000027, metrics:accuracy:0.5036
INFO:root:12:14:14 [Batch 3600/3750] loss=1.1144, metrics:accuracy:14850.0000
INFO:root:12:14:21 [Epoch 5 Batch 10800/13130] loss=0.9149, lr=0.0000016, metrics:accuracy:0.5563
INFO:root:12:14:21 [Epoch 4 Batch 8400/13130] loss=0.9939, lr=0.0000024, metrics:accuracy:0.5007
INFO:root:12:14:21 validation metrics:accuracy:0.4053
INFO:root:12:14:21 Time cost=172.67s, throughput=173.75 samples/s
INFO:root:12:14:23 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:12:14:23 Time cost=1485.29s
INFO:root:12:14:28 [Epoch 4 Batch 10800/13130] loss=0.9846, lr=0.0000026, metrics:accuracy:0.5036
INFO:root:12:14:44 [Epoch 4 Batch 8800/13130] loss=0.9817, lr=0.0000024, metrics:accuracy:0.5009
INFO:root:12:14:47 AMP: decreasing loss scale to 8192.000000
INFO:root:12:14:50 [Epoch 4 Batch 11200/13130] loss=0.9865, lr=0.0000025, metrics:accuracy:0.5040
INFO:root:12:14:50 [Epoch 5 Batch 11200/13130] loss=0.8993, lr=0.0000013, metrics:accuracy:0.5568
INFO:root:12:15:05 [Epoch 5 Batch 400/13130] loss=1.0815, lr=0.0000043, metrics:accuracy:0.4033
INFO:root:12:15:06 [Epoch 4 Batch 9200/13130] loss=0.9982, lr=0.0000023, metrics:accuracy:0.5008
INFO:root:12:15:12 [Epoch 4 Batch 11600/13130] loss=0.9829, lr=0.0000025, metrics:accuracy:0.5040
INFO:root:12:15:21 [Epoch 5 Batch 11600/13130] loss=0.9274, lr=0.0000010, metrics:accuracy:0.5566
INFO:root:12:15:28 [Epoch 4 Batch 9600/13130] loss=1.0025, lr=0.0000023, metrics:accuracy:0.5001
INFO:root:12:15:34 [Epoch 4 Batch 12000/13130] loss=0.9920, lr=0.0000024, metrics:accuracy:0.5038
INFO:root:12:15:46 [Epoch 5 Batch 800/13130] loss=1.0876, lr=0.0000042, metrics:accuracy:0.3916
INFO:root:12:15:50 [Epoch 4 Batch 10000/13130] loss=1.0001, lr=0.0000022, metrics:accuracy:0.4998
INFO:root:12:15:51 [Epoch 5 Batch 12000/13130] loss=0.9139, lr=0.0000007, metrics:accuracy:0.5565
INFO:root:12:15:56 [Epoch 4 Batch 12400/13130] loss=0.9813, lr=0.0000023, metrics:accuracy:0.5043
INFO:root:12:16:12 [Epoch 4 Batch 10400/13130] loss=0.9844, lr=0.0000021, metrics:accuracy:0.5000
INFO:root:12:16:18 [Epoch 4 Batch 12800/13130] loss=0.9882, lr=0.0000023, metrics:accuracy:0.5044
INFO:root:12:16:22 [Epoch 5 Batch 12400/13130] loss=0.9082, lr=0.0000005, metrics:accuracy:0.5567
INFO:root:12:16:28 [Epoch 5 Batch 1200/13130] loss=1.0835, lr=0.0000040, metrics:accuracy:0.3924
INFO:root:12:16:35 [Epoch 4 Batch 10800/13130] loss=0.9918, lr=0.0000021, metrics:accuracy:0.5002
INFO:root:12:16:36 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:16:47 [Batch 400/3750] loss=1.0279, metrics:accuracy:11650.0000
INFO:root:12:16:53 [Epoch 5 Batch 12800/13130] loss=0.9132, lr=0.0000002, metrics:accuracy:0.5569
INFO:root:12:16:57 [Epoch 4 Batch 11200/13130] loss=0.9900, lr=0.0000020, metrics:accuracy:0.5007
INFO:root:12:16:59 [Batch 800/3750] loss=1.0448, metrics:accuracy:12050.0000
INFO:root:12:17:09 [Epoch 5 Batch 1600/13130] loss=1.0764, lr=0.0000039, metrics:accuracy:0.3952
INFO:root:12:17:10 [Batch 1200/3750] loss=1.0354, metrics:accuracy:12450.0000
INFO:root:12:17:16 Finish training step: 32812
INFO:root:12:17:16 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:17:20 [Epoch 4 Batch 11600/13130] loss=0.9939, lr=0.0000020, metrics:accuracy:0.5007
INFO:root:12:17:21 [Batch 1600/3750] loss=1.0796, metrics:accuracy:12850.0000
INFO:root:12:17:32 [Batch 2000/3750] loss=1.0666, metrics:accuracy:13250.0000
INFO:root:12:17:33 [Batch 400/3750] loss=1.0260, metrics:accuracy:15400.0000
INFO:root:12:17:42 [Epoch 4 Batch 12000/13130] loss=0.9977, lr=0.0000019, metrics:accuracy:0.5005
INFO:root:12:17:43 [Batch 2400/3750] loss=1.0568, metrics:accuracy:13650.0000
INFO:root:12:17:44 [Epoch 5 Batch 2000/13130] loss=1.0748, lr=0.0000038, metrics:accuracy:0.3973
INFO:root:12:17:49 [Batch 800/3750] loss=1.0601, metrics:accuracy:15800.0000
INFO:root:12:17:55 [Batch 2800/3750] loss=1.0104, metrics:accuracy:14050.0000
INFO:root:12:18:04 [Epoch 4 Batch 12400/13130] loss=0.9873, lr=0.0000019, metrics:accuracy:0.5009
INFO:root:12:18:06 [Batch 3200/3750] loss=0.9765, metrics:accuracy:14450.0000
INFO:root:12:18:06 [Batch 1200/3750] loss=1.0543, metrics:accuracy:16200.0000
INFO:root:12:18:17 [Batch 3600/3750] loss=0.9820, metrics:accuracy:14850.0000
INFO:root:12:18:18 [Epoch 5 Batch 2400/13130] loss=1.0800, lr=0.0000036, metrics:accuracy:0.4005
INFO:root:12:18:21 validation metrics:accuracy:0.4772
INFO:root:12:18:21 Time cost=104.95s, throughput=285.84 samples/s
INFO:root:12:18:22 [Batch 1600/3750] loss=1.1034, metrics:accuracy:16600.0000
INFO:root:12:18:23 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:12:18:23 Time cost=830.64s
INFO:root:12:18:26 [Epoch 4 Batch 12800/13130] loss=0.9965, lr=0.0000018, metrics:accuracy:0.5008
INFO:root:12:18:39 [Batch 2000/3750] loss=1.0940, metrics:accuracy:17000.0000
INFO:root:12:18:45 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:18:45 [Epoch 5 Batch 400/13130] loss=0.9647, lr=0.0000022, metrics:accuracy:0.5219
INFO:root:12:18:51 [Epoch 5 Batch 2800/13130] loss=1.0847, lr=0.0000035, metrics:accuracy:0.3996
INFO:root:12:18:55 [Batch 2400/3750] loss=1.0905, metrics:accuracy:17400.0000
INFO:root:12:18:56 [Batch 400/3750] loss=1.0476, metrics:accuracy:11650.0000
INFO:root:12:19:07 [Batch 800/3750] loss=1.0615, metrics:accuracy:12050.0000
INFO:root:12:19:08 [Epoch 5 Batch 800/13130] loss=0.9844, lr=0.0000021, metrics:accuracy:0.5160
INFO:root:12:19:12 [Batch 2800/3750] loss=1.0829, metrics:accuracy:17800.0000
INFO:root:12:19:19 [Batch 1200/3750] loss=1.0468, metrics:accuracy:12450.0000
INFO:root:12:19:25 [Epoch 5 Batch 3200/13130] loss=1.0833, lr=0.0000034, metrics:accuracy:0.3989
INFO:root:12:19:28 [Batch 3200/3750] loss=1.0737, metrics:accuracy:18200.0000
INFO:root:12:19:29 [Epoch 5 Batch 1200/13130] loss=0.9498, lr=0.0000020, metrics:accuracy:0.5214
INFO:root:12:19:30 [Batch 1600/3750] loss=1.0617, metrics:accuracy:12850.0000
INFO:root:12:19:41 [Batch 2000/3750] loss=1.0440, metrics:accuracy:13250.0000
INFO:root:12:19:45 [Batch 3600/3750] loss=1.0755, metrics:accuracy:18600.0000
INFO:root:12:19:51 validation metrics:accuracy:0.4755
INFO:root:12:19:51 Time cost=154.73s, throughput=193.89 samples/s
INFO:root:12:19:51 [Epoch 5 Batch 1600/13130] loss=0.9642, lr=0.0000019, metrics:accuracy:0.5212
INFO:root:12:19:53 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:12:19:53 Time cost=1139.15s
INFO:root:12:19:53 [Batch 2400/3750] loss=1.0327, metrics:accuracy:13650.0000
INFO:root:12:19:53 Best model at epoch 3. Validation metrics:accuracy:0.4757
INFO:root:12:19:53 Now we are doing testing on test with gpu(0).
INFO:root:12:19:58 [Epoch 5 Batch 3600/13130] loss=1.0854, lr=0.0000032, metrics:accuracy:0.3977
INFO:root:12:20:04 [Batch 2800/3750] loss=1.0036, metrics:accuracy:14050.0000
INFO:root:12:20:13 [Epoch 5 Batch 2000/13130] loss=0.9806, lr=0.0000019, metrics:accuracy:0.5171
INFO:root:12:20:15 [Batch 3200/3750] loss=0.9728, metrics:accuracy:14450.0000
INFO:root:12:20:26 [Batch 3600/3750] loss=0.9795, metrics:accuracy:14850.0000
INFO:root:12:20:31 validation metrics:accuracy:0.4792
INFO:root:12:20:31 Time cost=105.56s, throughput=284.21 samples/s
INFO:root:12:20:33 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:12:20:33 Time cost=842.44s
INFO:root:12:20:35 [Epoch 5 Batch 4000/13130] loss=1.0829, lr=0.0000031, metrics:accuracy:0.3974
INFO:root:12:20:35 [Epoch 5 Batch 2400/13130] loss=0.9762, lr=0.0000018, metrics:accuracy:0.5179
INFO:root:12:20:55 [Epoch 5 Batch 400/13130] loss=0.9756, lr=0.0000017, metrics:accuracy:0.5119
INFO:root:12:20:57 AMP: increasing loss scale to 16384.000000
INFO:root:12:20:58 [Epoch 5 Batch 2800/13130] loss=0.9673, lr=0.0000017, metrics:accuracy:0.5175
INFO:root:12:21:08 Time cost=74.89s, throughput=200.29 samples/s
INFO:root:12:21:10 [Epoch 5 Batch 4400/13130] loss=1.0802, lr=0.0000029, metrics:accuracy:0.3980
INFO:root:12:21:18 [Epoch 5 Batch 800/13130] loss=0.9956, lr=0.0000017, metrics:accuracy:0.5046
INFO:root:12:21:20 [Epoch 5 Batch 3200/13130] loss=0.9563, lr=0.0000017, metrics:accuracy:0.5184
INFO:root:12:21:29 [Epoch 5 Batch 4800/13130] loss=1.0827, lr=0.0000028, metrics:accuracy:0.3985
INFO:root:12:21:40 [Epoch 5 Batch 1200/13130] loss=0.9617, lr=0.0000016, metrics:accuracy:0.5106
INFO:root:12:21:41 [Epoch 5 Batch 3600/13130] loss=0.9692, lr=0.0000016, metrics:accuracy:0.5180
INFO:root:12:21:48 [Epoch 5 Batch 5200/13130] loss=1.0802, lr=0.0000027, metrics:accuracy:0.3992
INFO:root:12:22:02 [Epoch 5 Batch 1600/13130] loss=0.9707, lr=0.0000016, metrics:accuracy:0.5095
INFO:root:12:22:03 [Epoch 5 Batch 4000/13130] loss=0.9715, lr=0.0000015, metrics:accuracy:0.5176
INFO:root:12:22:07 [Epoch 5 Batch 5600/13130] loss=1.0832, lr=0.0000025, metrics:accuracy:0.3986
INFO:root:12:22:24 [Epoch 5 Batch 2000/13130] loss=0.9860, lr=0.0000015, metrics:accuracy:0.5075
INFO:root:12:22:25 [Epoch 5 Batch 4400/13130] loss=0.9489, lr=0.0000015, metrics:accuracy:0.5190
INFO:root:12:22:25 [Epoch 5 Batch 6000/13130] loss=1.0856, lr=0.0000024, metrics:accuracy:0.3983
INFO:root:12:22:45 [Epoch 5 Batch 6400/13130] loss=1.0819, lr=0.0000023, metrics:accuracy:0.3985
INFO:root:12:22:47 [Epoch 5 Batch 2400/13130] loss=0.9866, lr=0.0000015, metrics:accuracy:0.5070
INFO:root:12:22:47 [Epoch 5 Batch 4800/13130] loss=0.9546, lr=0.0000014, metrics:accuracy:0.5197
INFO:root:12:23:05 [Epoch 5 Batch 6800/13130] loss=1.0801, lr=0.0000021, metrics:accuracy:0.3987
INFO:root:12:23:08 [Epoch 5 Batch 5200/13130] loss=0.9577, lr=0.0000013, metrics:accuracy:0.5208
INFO:root:12:23:09 [Epoch 5 Batch 2800/13130] loss=0.9824, lr=0.0000014, metrics:accuracy:0.5074
INFO:root:12:23:23 [Epoch 5 Batch 7200/13130] loss=1.0840, lr=0.0000020, metrics:accuracy:0.3987
INFO:root:12:23:30 [Epoch 5 Batch 5600/13130] loss=0.9729, lr=0.0000013, metrics:accuracy:0.5204
INFO:root:12:23:31 [Epoch 5 Batch 3200/13130] loss=0.9707, lr=0.0000013, metrics:accuracy:0.5091
INFO:root:12:23:42 [Epoch 5 Batch 7600/13130] loss=1.0789, lr=0.0000019, metrics:accuracy:0.3988
INFO:root:12:23:42 AMP: decreasing loss scale to 8192.000000
INFO:root:12:23:52 [Epoch 5 Batch 6000/13130] loss=0.9717, lr=0.0000012, metrics:accuracy:0.5207
INFO:root:12:23:53 [Epoch 5 Batch 3600/13130] loss=0.9808, lr=0.0000013, metrics:accuracy:0.5095
INFO:root:12:24:00 [Epoch 5 Batch 8000/13130] loss=1.0847, lr=0.0000017, metrics:accuracy:0.3993
INFO:root:12:24:13 [Epoch 5 Batch 6400/13130] loss=0.9580, lr=0.0000011, metrics:accuracy:0.5217
INFO:root:12:24:15 [Epoch 5 Batch 4000/13130] loss=0.9816, lr=0.0000012, metrics:accuracy:0.5091
INFO:root:12:24:18 [Epoch 5 Batch 8400/13130] loss=1.0774, lr=0.0000016, metrics:accuracy:0.3996
INFO:root:12:24:35 [Epoch 5 Batch 6800/13130] loss=0.9575, lr=0.0000011, metrics:accuracy:0.5226
INFO:root:12:24:36 [Epoch 5 Batch 8800/13130] loss=1.0844, lr=0.0000015, metrics:accuracy:0.3993
INFO:root:12:24:37 [Epoch 5 Batch 4400/13130] loss=0.9662, lr=0.0000012, metrics:accuracy:0.5101
INFO:root:12:24:55 [Epoch 5 Batch 9200/13130] loss=1.0790, lr=0.0000013, metrics:accuracy:0.3997
INFO:root:12:24:57 [Epoch 5 Batch 7200/13130] loss=0.9656, lr=0.0000010, metrics:accuracy:0.5223
INFO:root:12:25:00 [Epoch 5 Batch 4800/13130] loss=0.9694, lr=0.0000011, metrics:accuracy:0.5108
INFO:root:12:25:15 [Epoch 5 Batch 9600/13130] loss=1.0808, lr=0.0000012, metrics:accuracy:0.3999
INFO:root:12:25:18 [Epoch 5 Batch 7600/13130] loss=0.9671, lr=0.0000009, metrics:accuracy:0.5225
INFO:root:12:25:22 [Epoch 5 Batch 5200/13130] loss=0.9723, lr=0.0000011, metrics:accuracy:0.5117
INFO:root:12:25:33 [Epoch 5 Batch 10000/13130] loss=1.0799, lr=0.0000011, metrics:accuracy:0.4000
INFO:root:12:25:39 [Epoch 5 Batch 8000/13130] loss=0.9682, lr=0.0000009, metrics:accuracy:0.5224
INFO:root:12:25:44 [Epoch 5 Batch 5600/13130] loss=0.9836, lr=0.0000010, metrics:accuracy:0.5115
INFO:root:12:25:52 [Epoch 5 Batch 10400/13130] loss=1.0828, lr=0.0000009, metrics:accuracy:0.4002
INFO:root:12:26:01 [Epoch 5 Batch 8400/13130] loss=0.9486, lr=0.0000008, metrics:accuracy:0.5237
INFO:root:12:26:06 [Epoch 5 Batch 6000/13130] loss=0.9786, lr=0.0000010, metrics:accuracy:0.5119
INFO:root:12:26:11 [Epoch 5 Batch 10800/13130] loss=1.0821, lr=0.0000008, metrics:accuracy:0.4001
INFO:root:12:26:22 [Epoch 5 Batch 8800/13130] loss=0.9709, lr=0.0000007, metrics:accuracy:0.5232
INFO:root:12:26:28 [Epoch 5 Batch 6400/13130] loss=0.9711, lr=0.0000009, metrics:accuracy:0.5123
INFO:root:12:26:30 [Epoch 5 Batch 11200/13130] loss=1.0787, lr=0.0000006, metrics:accuracy:0.4003
INFO:root:12:26:44 [Epoch 5 Batch 9200/13130] loss=0.9603, lr=0.0000007, metrics:accuracy:0.5233
INFO:root:12:26:49 [Epoch 5 Batch 11600/13130] loss=1.0790, lr=0.0000005, metrics:accuracy:0.4009
INFO:root:12:26:49 AMP: increasing loss scale to 16384.000000
INFO:root:12:26:51 [Epoch 5 Batch 6800/13130] loss=0.9666, lr=0.0000009, metrics:accuracy:0.5138
INFO:root:12:27:05 [Epoch 5 Batch 9600/13130] loss=0.9479, lr=0.0000006, metrics:accuracy:0.5241
INFO:root:12:27:07 [Epoch 5 Batch 12000/13130] loss=1.0803, lr=0.0000004, metrics:accuracy:0.4010
INFO:root:12:27:13 [Epoch 5 Batch 7200/13130] loss=0.9769, lr=0.0000008, metrics:accuracy:0.5135
INFO:root:12:27:26 [Epoch 5 Batch 12400/13130] loss=1.0774, lr=0.0000002, metrics:accuracy:0.4012
INFO:root:12:27:27 [Epoch 5 Batch 10000/13130] loss=0.9603, lr=0.0000005, metrics:accuracy:0.5240
INFO:root:12:27:35 [Epoch 5 Batch 7600/13130] loss=0.9796, lr=0.0000007, metrics:accuracy:0.5134
INFO:root:12:27:44 [Epoch 5 Batch 12800/13130] loss=1.0857, lr=0.0000001, metrics:accuracy:0.4013
INFO:root:12:27:48 [Epoch 5 Batch 10400/13130] loss=0.9659, lr=0.0000005, metrics:accuracy:0.5241
INFO:root:12:27:56 [Epoch 5 Batch 8000/13130] loss=0.9787, lr=0.0000007, metrics:accuracy:0.5133
INFO:root:12:27:59 Finish training step: 32812
INFO:root:12:27:59 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:28:06 [Batch 400/3750] loss=1.0274, metrics:accuracy:15400.0000
INFO:root:12:28:09 [Epoch 5 Batch 10800/13130] loss=0.9685, lr=0.0000004, metrics:accuracy:0.5240
INFO:root:12:28:14 [Batch 800/3750] loss=1.0339, metrics:accuracy:15800.0000
INFO:root:12:28:18 [Epoch 5 Batch 8400/13130] loss=0.9584, lr=0.0000006, metrics:accuracy:0.5145
INFO:root:12:28:21 [Batch 1200/3750] loss=1.0348, metrics:accuracy:16200.0000
INFO:root:12:28:29 [Batch 1600/3750] loss=1.0889, metrics:accuracy:16600.0000
INFO:root:12:28:31 [Epoch 5 Batch 11200/13130] loss=0.9637, lr=0.0000003, metrics:accuracy:0.5242
INFO:root:12:28:37 [Batch 2000/3750] loss=1.0888, metrics:accuracy:17000.0000
INFO:root:12:28:40 [Epoch 5 Batch 8800/13130] loss=0.9837, lr=0.0000006, metrics:accuracy:0.5140
INFO:root:12:28:45 [Batch 2400/3750] loss=1.0794, metrics:accuracy:17400.0000
INFO:root:12:28:52 [Batch 2800/3750] loss=1.1122, metrics:accuracy:17800.0000
INFO:root:12:28:53 [Epoch 5 Batch 11600/13130] loss=0.9714, lr=0.0000003, metrics:accuracy:0.5237
INFO:root:12:29:00 [Batch 3200/3750] loss=1.1089, metrics:accuracy:18200.0000
INFO:root:12:29:03 [Epoch 5 Batch 9200/13130] loss=0.9716, lr=0.0000005, metrics:accuracy:0.5140
INFO:root:12:29:08 [Batch 3600/3750] loss=1.1201, metrics:accuracy:18600.0000
INFO:root:12:29:11 validation metrics:accuracy:0.4070
INFO:root:12:29:11 Time cost=72.08s, throughput=416.18 samples/s
INFO:root:12:29:12 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:12:29:12 Time cost=889.71s
INFO:root:12:29:13 Best model at epoch 4. Validation metrics:accuracy:0.4070
INFO:root:12:29:13 Now we are doing testing on test with gpu(0).
INFO:root:12:29:14 [Epoch 5 Batch 12000/13130] loss=0.9641, lr=0.0000002, metrics:accuracy:0.5236
INFO:root:12:29:25 [Epoch 5 Batch 9600/13130] loss=0.9645, lr=0.0000005, metrics:accuracy:0.5148
INFO:root:12:29:37 [Epoch 5 Batch 12400/13130] loss=0.9545, lr=0.0000001, metrics:accuracy:0.5239
INFO:root:12:29:47 [Epoch 5 Batch 10000/13130] loss=0.9744, lr=0.0000004, metrics:accuracy:0.5151
INFO:root:12:29:48 Time cost=34.80s, throughput=431.01 samples/s
INFO:root:12:29:58 [Epoch 5 Batch 12800/13130] loss=0.9689, lr=0.0000001, metrics:accuracy:0.5238
INFO:root:12:30:08 [Epoch 5 Batch 10400/13130] loss=0.9774, lr=0.0000004, metrics:accuracy:0.5153
INFO:root:12:30:15 Finish training step: 32812
INFO:root:12:30:15 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:30:26 [Batch 400/3750] loss=0.9745, metrics:accuracy:15400.0000
INFO:root:12:30:31 [Epoch 5 Batch 10800/13130] loss=0.9804, lr=0.0000003, metrics:accuracy:0.5153
INFO:root:12:30:37 [Batch 800/3750] loss=0.9937, metrics:accuracy:15800.0000
INFO:root:12:30:48 [Batch 1200/3750] loss=0.9855, metrics:accuracy:16200.0000
INFO:root:12:30:52 [Epoch 5 Batch 11200/13130] loss=0.9769, lr=0.0000003, metrics:accuracy:0.5154
INFO:root:12:30:59 [Batch 1600/3750] loss=1.0810, metrics:accuracy:16600.0000
INFO:root:12:31:10 [Batch 2000/3750] loss=1.0790, metrics:accuracy:17000.0000
INFO:root:12:31:14 [Epoch 5 Batch 11600/13130] loss=0.9854, lr=0.0000002, metrics:accuracy:0.5151
INFO:root:12:31:21 [Batch 2400/3750] loss=1.0644, metrics:accuracy:17400.0000
INFO:root:12:31:32 [Batch 2800/3750] loss=1.0539, metrics:accuracy:17800.0000
INFO:root:12:31:37 [Epoch 5 Batch 12000/13130] loss=0.9778, lr=0.0000001, metrics:accuracy:0.5152
INFO:root:12:31:43 [Batch 3200/3750] loss=1.0336, metrics:accuracy:18200.0000
INFO:root:12:31:53 [Batch 3600/3750] loss=1.0393, metrics:accuracy:18600.0000
INFO:root:12:31:57 validation metrics:accuracy:0.4778
INFO:root:12:31:57 Time cost=102.29s, throughput=293.28 samples/s
INFO:root:12:31:59 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:12:31:59 Time cost=816.17s
INFO:root:12:31:59 [Epoch 5 Batch 12400/13130] loss=0.9660, lr=0.0000001, metrics:accuracy:0.5155
INFO:root:12:31:59 Best model at epoch 4. Validation metrics:accuracy:0.4778
INFO:root:12:31:59 Now we are doing testing on test with gpu(0).
INFO:root:12:32:21 [Epoch 5 Batch 12800/13130] loss=0.9781, lr=0.0000000, metrics:accuracy:0.5156
INFO:root:12:32:38 Finish training step: 32812
INFO:root:12:32:38 Now we are doing evaluation on dev with gpu(0).
INFO:root:12:32:49 [Batch 400/3750] loss=0.9920, metrics:accuracy:15400.0000
INFO:root:12:32:50 Time cost=50.89s, throughput=294.74 samples/s
INFO:root:12:33:01 [Batch 800/3750] loss=1.0050, metrics:accuracy:15800.0000
INFO:root:12:33:12 [Batch 1200/3750] loss=0.9894, metrics:accuracy:16200.0000
INFO:root:12:33:23 [Batch 1600/3750] loss=1.0562, metrics:accuracy:16600.0000
INFO:root:12:33:34 [Batch 2000/3750] loss=1.0473, metrics:accuracy:17000.0000
INFO:root:12:33:45 [Batch 2400/3750] loss=1.0340, metrics:accuracy:17400.0000
INFO:root:12:33:56 [Batch 2800/3750] loss=1.0391, metrics:accuracy:17800.0000
INFO:root:12:34:07 [Batch 3200/3750] loss=1.0191, metrics:accuracy:18200.0000
INFO:root:12:34:18 [Batch 3600/3750] loss=1.0242, metrics:accuracy:18600.0000
INFO:root:12:34:22 validation metrics:accuracy:0.4815
INFO:root:12:34:22 Time cost=103.72s, throughput=289.23 samples/s
INFO:root:12:34:24 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:12:34:24 Time cost=830.82s
INFO:root:12:34:24 Best model at epoch 4. Validation metrics:accuracy:0.4815
INFO:root:12:34:24 Now we are doing testing on test with gpu(0).
INFO:root:12:35:15 Time cost=51.09s, throughput=293.59 samples/s
INFO:root:13:56:50 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float16', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:13:56:50 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:13:56:50 Using AMP
INFO:root:13:56:54 processing dataset...
INFO:root:13:57:16 Now we are doing BERT classification training on gpu(0)!
INFO:root:13:57:16 training steps=32812
WARNING:py.warnings:13:57:19 finetune_classifier.py:642: UserWarning: nan or inf is detected. Clipping results will be undefined.
  nlp.utils.clip_grad_global_norm(params, 1)

INFO:root:13:57:19 AMP: decreasing loss scale to 32768.000000
INFO:root:13:57:23 AMP: decreasing loss scale to 16384.000000
INFO:root:13:57:34 AMP: decreasing loss scale to 8192.000000
INFO:root:13:57:35 AMP: decreasing loss scale to 4096.000000
INFO:root:13:57:36 [Epoch 1 Batch 400/13130] loss=1.1410, lr=0.0000012, metrics:accuracy:0.3458
INFO:root:13:57:38 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=4e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:13:57:38 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:13:57:43 processing dataset...
INFO:root:13:57:51 [Epoch 1 Batch 800/13130] loss=1.1344, lr=0.0000024, metrics:accuracy:0.3340
INFO:root:13:58:04 Now we are doing BERT classification training on gpu(0)!
INFO:root:13:58:04 training steps=32812
INFO:root:13:58:11 [Epoch 1 Batch 1200/13130] loss=1.1180, lr=0.0000037, metrics:accuracy:0.3320
INFO:root:13:58:26 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=1e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:13:58:26 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:13:58:31 processing dataset...
INFO:root:13:58:34 [Epoch 1 Batch 400/13130] loss=1.1175, lr=0.0000024, metrics:accuracy:0.3395
INFO:root:13:58:50 [Epoch 1 Batch 1600/13130] loss=1.1060, lr=0.0000049, metrics:accuracy:0.3355
INFO:root:13:58:53 Now we are doing BERT classification training on gpu(0)!
INFO:root:13:58:54 training steps=32812
INFO:root:13:59:05 [Epoch 1 Batch 800/13130] loss=1.0980, lr=0.0000049, metrics:accuracy:0.3471
INFO:root:13:59:15 [Epoch 1 Batch 400/13130] loss=1.1195, lr=0.0000006, metrics:accuracy:0.3436
INFO:root:13:59:31 [Epoch 1 Batch 2000/13130] loss=1.1037, lr=0.0000061, metrics:accuracy:0.3396
INFO:root:13:59:35 [Epoch 1 Batch 1200/13130] loss=1.0962, lr=0.0000073, metrics:accuracy:0.3552
INFO:root:13:59:37 [Epoch 1 Batch 800/13130] loss=1.1026, lr=0.0000012, metrics:accuracy:0.3444
INFO:root:13:59:56 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=8e-06, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:13:59:56 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:13:59:58 [Epoch 1 Batch 1200/13130] loss=1.0992, lr=0.0000018, metrics:accuracy:0.3482
INFO:root:14:00:00 processing dataset...
INFO:root:14:00:06 [Epoch 1 Batch 1600/13130] loss=1.0983, lr=0.0000097, metrics:accuracy:0.3607
INFO:root:14:00:11 [Epoch 1 Batch 2400/13130] loss=1.1054, lr=0.0000073, metrics:accuracy:0.3412
INFO:root:14:00:20 [Epoch 1 Batch 1600/13130] loss=1.0997, lr=0.0000024, metrics:accuracy:0.3508
INFO:root:14:00:23 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:00:23 training steps=32812
INFO:root:14:00:36 [Epoch 1 Batch 2000/13130] loss=1.0988, lr=0.0000122, metrics:accuracy:0.3630
INFO:root:14:00:43 [Epoch 1 Batch 2000/13130] loss=1.0967, lr=0.0000030, metrics:accuracy:0.3538
INFO:root:14:00:45 [Epoch 1 Batch 400/13130] loss=1.1207, lr=0.0000005, metrics:accuracy:0.3454
INFO:root:14:00:53 [Epoch 1 Batch 2800/13130] loss=1.1062, lr=0.0000085, metrics:accuracy:0.3398
INFO:root:14:01:05 [Epoch 1 Batch 2400/13130] loss=1.0943, lr=0.0000037, metrics:accuracy:0.3568
INFO:root:14:01:06 [Epoch 1 Batch 2400/13130] loss=1.0967, lr=0.0000146, metrics:accuracy:0.3645
INFO:root:14:01:07 [Epoch 1 Batch 800/13130] loss=1.1038, lr=0.0000010, metrics:accuracy:0.3460
INFO:root:14:01:14 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:14:01:14 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:14:01:23 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:14:01:23 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:14:01:27 [Epoch 1 Batch 2800/13130] loss=1.0929, lr=0.0000043, metrics:accuracy:0.3595
INFO:root:14:01:28 processing dataset...
INFO:root:14:01:29 [Epoch 1 Batch 1200/13130] loss=1.0989, lr=0.0000015, metrics:accuracy:0.3489
INFO:root:14:01:34 [Epoch 1 Batch 3200/13130] loss=1.1036, lr=0.0000097, metrics:accuracy:0.3388
INFO:root:14:01:37 [Epoch 1 Batch 2800/13130] loss=1.0934, lr=0.0000171, metrics:accuracy:0.3665
INFO:root:14:01:50 [Epoch 1 Batch 3200/13130] loss=1.0930, lr=0.0000049, metrics:accuracy:0.3623
INFO:root:14:01:51 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:01:51 training steps=32812
INFO:root:14:01:51 [Epoch 1 Batch 1600/13130] loss=1.0972, lr=0.0000019, metrics:accuracy:0.3525
INFO:root:14:02:07 [Epoch 1 Batch 3200/13130] loss=1.0934, lr=0.0000195, metrics:accuracy:0.3685
INFO:root:14:02:11 [Epoch 1 Batch 3600/13130] loss=1.0902, lr=0.0000055, metrics:accuracy:0.3646
INFO:root:14:02:14 [Epoch 1 Batch 3600/13130] loss=1.1027, lr=0.0000110, metrics:accuracy:0.3399
INFO:root:14:02:30 [Epoch 1 Batch 2000/13130] loss=1.0964, lr=0.0000024, metrics:accuracy:0.3555
INFO:root:14:02:30 [Epoch 1 Batch 400/13130] loss=1.1171, lr=0.0000012, metrics:accuracy:0.3417
INFO:root:14:02:33 [Epoch 1 Batch 4000/13130] loss=1.0866, lr=0.0000061, metrics:accuracy:0.3679
INFO:root:14:02:37 [Epoch 1 Batch 3600/13130] loss=1.0890, lr=0.0000219, metrics:accuracy:0.3712
INFO:root:14:02:53 [Epoch 1 Batch 4000/13130] loss=1.0995, lr=0.0000122, metrics:accuracy:0.3400
INFO:root:14:02:55 [Epoch 1 Batch 4400/13130] loss=1.0851, lr=0.0000067, metrics:accuracy:0.3707
INFO:root:14:03:08 [Epoch 1 Batch 4000/13130] loss=1.0836, lr=0.0000244, metrics:accuracy:0.3740
INFO:root:14:03:09 [Epoch 1 Batch 2400/13130] loss=1.0942, lr=0.0000029, metrics:accuracy:0.3566
INFO:root:14:03:10 [Epoch 1 Batch 800/13130] loss=1.0991, lr=0.0000024, metrics:accuracy:0.3458
INFO:root:14:03:16 [Epoch 1 Batch 4800/13130] loss=1.0844, lr=0.0000073, metrics:accuracy:0.3726
INFO:root:14:03:33 AMP: increasing loss scale to 8192.000000
INFO:root:14:03:34 [Epoch 1 Batch 4400/13130] loss=1.1021, lr=0.0000134, metrics:accuracy:0.3406
INFO:root:14:03:37 [Epoch 1 Batch 5200/13130] loss=1.0909, lr=0.0000079, metrics:accuracy:0.3744
INFO:root:14:03:39 [Epoch 1 Batch 4400/13130] loss=1.0888, lr=0.0000268, metrics:accuracy:0.3754
INFO:root:14:03:48 [Epoch 1 Batch 2800/13130] loss=1.0912, lr=0.0000034, metrics:accuracy:0.3603
INFO:root:14:03:49 [Epoch 1 Batch 1200/13130] loss=1.0960, lr=0.0000037, metrics:accuracy:0.3511
INFO:root:14:03:59 [Epoch 1 Batch 5600/13130] loss=1.0793, lr=0.0000085, metrics:accuracy:0.3770
INFO:root:14:04:09 [Epoch 1 Batch 4800/13130] loss=1.0846, lr=0.0000292, metrics:accuracy:0.3772
INFO:root:14:04:15 [Epoch 1 Batch 4800/13130] loss=1.1010, lr=0.0000146, metrics:accuracy:0.3412
INFO:root:14:04:21 [Epoch 1 Batch 6000/13130] loss=1.0830, lr=0.0000091, metrics:accuracy:0.3785
INFO:root:14:04:28 [Epoch 1 Batch 1600/13130] loss=1.0974, lr=0.0000049, metrics:accuracy:0.3571
INFO:root:14:04:28 [Epoch 1 Batch 3200/13130] loss=1.0929, lr=0.0000039, metrics:accuracy:0.3626
INFO:root:14:04:39 [Epoch 1 Batch 5200/13130] loss=1.0959, lr=0.0000317, metrics:accuracy:0.3776
INFO:root:14:04:42 [Epoch 1 Batch 6400/13130] loss=1.0767, lr=0.0000098, metrics:accuracy:0.3802
INFO:root:14:04:55 [Epoch 1 Batch 5200/13130] loss=1.1015, lr=0.0000158, metrics:accuracy:0.3413
INFO:root:14:05:04 [Epoch 1 Batch 6800/13130] loss=1.0744, lr=0.0000100, metrics:accuracy:0.3828
INFO:root:14:05:07 [Epoch 1 Batch 2000/13130] loss=1.0971, lr=0.0000061, metrics:accuracy:0.3587
INFO:root:14:05:07 [Epoch 1 Batch 3600/13130] loss=1.0889, lr=0.0000044, metrics:accuracy:0.3651
INFO:root:14:05:10 [Epoch 1 Batch 5600/13130] loss=1.0805, lr=0.0000341, metrics:accuracy:0.3806
INFO:root:14:05:25 [Epoch 1 Batch 7200/13130] loss=1.0785, lr=0.0000099, metrics:accuracy:0.3842
INFO:root:14:05:36 [Epoch 1 Batch 5600/13130] loss=1.0961, lr=0.0000171, metrics:accuracy:0.3419
INFO:root:14:05:41 [Epoch 1 Batch 6000/13130] loss=1.0829, lr=0.0000366, metrics:accuracy:0.3818
INFO:root:14:05:46 [Epoch 1 Batch 2400/13130] loss=1.0961, lr=0.0000073, metrics:accuracy:0.3589
INFO:root:14:05:47 [Epoch 1 Batch 7600/13130] loss=1.0782, lr=0.0000098, metrics:accuracy:0.3859
INFO:root:14:05:47 [Epoch 1 Batch 4000/13130] loss=1.0848, lr=0.0000049, metrics:accuracy:0.3681
INFO:root:14:06:08 [Epoch 1 Batch 8000/13130] loss=1.0797, lr=0.0000098, metrics:accuracy:0.3869
INFO:root:14:06:11 [Epoch 1 Batch 6400/13130] loss=1.0808, lr=0.0000390, metrics:accuracy:0.3840
INFO:root:14:06:16 [Epoch 1 Batch 6000/13130] loss=1.1000, lr=0.0000183, metrics:accuracy:0.3424
INFO:root:14:06:26 [Epoch 1 Batch 2800/13130] loss=1.0911, lr=0.0000085, metrics:accuracy:0.3618
INFO:root:14:06:26 [Epoch 1 Batch 4400/13130] loss=1.0857, lr=0.0000054, metrics:accuracy:0.3704
INFO:root:14:06:30 [Epoch 1 Batch 8400/13130] loss=1.0793, lr=0.0000097, metrics:accuracy:0.3877
INFO:root:14:06:41 [Epoch 1 Batch 6800/13130] loss=1.0788, lr=0.0000398, metrics:accuracy:0.3861
INFO:root:14:06:52 [Epoch 1 Batch 8800/13130] loss=1.0743, lr=0.0000096, metrics:accuracy:0.3891
INFO:root:14:06:56 [Epoch 1 Batch 6400/13130] loss=1.0974, lr=0.0000195, metrics:accuracy:0.3435
INFO:root:14:07:05 [Epoch 1 Batch 4800/13130] loss=1.0836, lr=0.0000058, metrics:accuracy:0.3718
INFO:root:14:07:05 [Epoch 1 Batch 3200/13130] loss=1.0985, lr=0.0000097, metrics:accuracy:0.3635
INFO:root:14:07:11 [Epoch 1 Batch 7200/13130] loss=1.0768, lr=0.0000396, metrics:accuracy:0.3874
INFO:root:14:07:13 [Epoch 1 Batch 9200/13130] loss=1.0681, lr=0.0000096, metrics:accuracy:0.3912
INFO:root:14:07:35 [Epoch 1 Batch 9600/13130] loss=1.0675, lr=0.0000095, metrics:accuracy:0.3927
INFO:root:14:07:37 [Epoch 1 Batch 6800/13130] loss=1.0958, lr=0.0000199, metrics:accuracy:0.3448
INFO:root:14:07:41 [Epoch 1 Batch 7600/13130] loss=1.0792, lr=0.0000393, metrics:accuracy:0.3890
INFO:root:14:07:43 [Epoch 1 Batch 5200/13130] loss=1.0912, lr=0.0000063, metrics:accuracy:0.3730
INFO:root:14:07:43 [Epoch 1 Batch 3600/13130] loss=1.0967, lr=0.0000110, metrics:accuracy:0.3660
INFO:root:14:07:56 [Epoch 1 Batch 10000/13130] loss=1.0721, lr=0.0000094, metrics:accuracy:0.3935
INFO:root:14:08:11 [Epoch 1 Batch 8000/13130] loss=1.0822, lr=0.0000390, metrics:accuracy:0.3899
INFO:root:14:08:17 [Epoch 1 Batch 7200/13130] loss=1.0989, lr=0.0000198, metrics:accuracy:0.3454
INFO:root:14:08:18 [Epoch 1 Batch 10400/13130] loss=1.0599, lr=0.0000094, metrics:accuracy:0.3951
INFO:root:14:08:23 [Epoch 1 Batch 4000/13130] loss=1.0913, lr=0.0000122, metrics:accuracy:0.3678
INFO:root:14:08:23 [Epoch 1 Batch 5600/13130] loss=1.0787, lr=0.0000068, metrics:accuracy:0.3754
INFO:root:14:08:39 [Epoch 1 Batch 10800/13130] loss=1.0635, lr=0.0000093, metrics:accuracy:0.3966
INFO:root:14:08:42 [Epoch 1 Batch 8400/13130] loss=1.0800, lr=0.0000388, metrics:accuracy:0.3909
INFO:root:14:08:57 [Epoch 1 Batch 7600/13130] loss=1.0979, lr=0.0000196, metrics:accuracy:0.3462
INFO:root:14:09:00 [Epoch 1 Batch 11200/13130] loss=1.0709, lr=0.0000092, metrics:accuracy:0.3977
INFO:root:14:09:03 [Epoch 1 Batch 4400/13130] loss=1.0914, lr=0.0000134, metrics:accuracy:0.3690
INFO:root:14:09:03 [Epoch 1 Batch 6000/13130] loss=1.0834, lr=0.0000073, metrics:accuracy:0.3774
INFO:root:14:09:12 [Epoch 1 Batch 8800/13130] loss=1.0762, lr=0.0000385, metrics:accuracy:0.3918
INFO:root:14:09:22 [Epoch 1 Batch 11600/13130] loss=1.0645, lr=0.0000091, metrics:accuracy:0.3990
INFO:root:14:09:37 [Epoch 1 Batch 8000/13130] loss=1.0988, lr=0.0000195, metrics:accuracy:0.3468
INFO:root:14:09:42 [Epoch 1 Batch 4800/13130] loss=1.0887, lr=0.0000146, metrics:accuracy:0.3700
INFO:root:14:09:42 [Epoch 1 Batch 6400/13130] loss=1.0758, lr=0.0000078, metrics:accuracy:0.3800
INFO:root:14:09:42 [Epoch 1 Batch 9200/13130] loss=1.0702, lr=0.0000382, metrics:accuracy:0.3936
INFO:root:14:09:43 [Epoch 1 Batch 12000/13130] loss=1.0703, lr=0.0000091, metrics:accuracy:0.3999
INFO:root:14:10:04 [Epoch 1 Batch 12400/13130] loss=1.0638, lr=0.0000090, metrics:accuracy:0.4010
INFO:root:14:10:13 [Epoch 1 Batch 9600/13130] loss=1.0665, lr=0.0000379, metrics:accuracy:0.3953
INFO:root:14:10:16 AMP: increasing loss scale to 16384.000000
INFO:root:14:10:18 [Epoch 1 Batch 8400/13130] loss=1.0967, lr=0.0000194, metrics:accuracy:0.3471
INFO:root:14:10:21 [Epoch 1 Batch 5200/13130] loss=1.0953, lr=0.0000158, metrics:accuracy:0.3711
INFO:root:14:10:21 [Epoch 1 Batch 6800/13130] loss=1.0748, lr=0.0000080, metrics:accuracy:0.3820
INFO:root:14:10:26 [Epoch 1 Batch 12800/13130] loss=1.0645, lr=0.0000089, metrics:accuracy:0.4020
INFO:root:14:10:43 [Epoch 1 Batch 10000/13130] loss=1.0743, lr=0.0000377, metrics:accuracy:0.3960
INFO:root:14:10:44 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:10:55 [Batch 400/3750] loss=1.0828, metrics:accuracy:0.4709
INFO:root:14:10:58 [Epoch 1 Batch 8800/13130] loss=1.0935, lr=0.0000192, metrics:accuracy:0.3481
INFO:root:14:10:59 [Epoch 1 Batch 5600/13130] loss=1.0849, lr=0.0000171, metrics:accuracy:0.3736
INFO:root:14:11:00 [Epoch 1 Batch 7200/13130] loss=1.0773, lr=0.0000079, metrics:accuracy:0.3836
INFO:root:14:11:05 [Batch 800/3750] loss=1.0875, metrics:accuracy:0.4656
INFO:root:14:11:14 [Epoch 1 Batch 10400/13130] loss=1.0595, lr=0.0000374, metrics:accuracy:0.3974
INFO:root:14:11:16 [Batch 1200/3750] loss=1.0857, metrics:accuracy:0.4662
INFO:root:14:11:27 [Batch 1600/3750] loss=1.1225, metrics:accuracy:0.4209
INFO:root:14:11:38 [Batch 2000/3750] loss=1.1262, metrics:accuracy:0.3911
INFO:root:14:11:38 [Epoch 1 Batch 9200/13130] loss=1.0957, lr=0.0000191, metrics:accuracy:0.3488
INFO:root:14:11:39 [Epoch 1 Batch 6000/13130] loss=1.0848, lr=0.0000183, metrics:accuracy:0.3754
INFO:root:14:11:39 [Epoch 1 Batch 7600/13130] loss=1.0760, lr=0.0000079, metrics:accuracy:0.3852
INFO:root:14:11:44 [Epoch 1 Batch 10800/13130] loss=1.0586, lr=0.0000371, metrics:accuracy:0.3994
INFO:root:14:11:49 [Batch 2400/3750] loss=1.1153, metrics:accuracy:0.3726
INFO:root:14:12:00 [Batch 2800/3750] loss=0.9916, metrics:accuracy:0.3927
INFO:root:14:12:11 [Batch 3200/3750] loss=0.9378, metrics:accuracy:0.4179
INFO:root:14:12:13 [Epoch 1 Batch 11200/13130] loss=1.0748, lr=0.0000369, metrics:accuracy:0.4004
INFO:root:14:12:18 [Epoch 1 Batch 6400/13130] loss=1.0897, lr=0.0000195, metrics:accuracy:0.3764
INFO:root:14:12:18 [Epoch 1 Batch 8000/13130] loss=1.0816, lr=0.0000078, metrics:accuracy:0.3863
INFO:root:14:12:18 [Epoch 1 Batch 9600/13130] loss=1.0926, lr=0.0000190, metrics:accuracy:0.3494
INFO:root:14:12:22 [Batch 3600/3750] loss=0.9411, metrics:accuracy:0.4398
INFO:root:14:12:26 validation metrics:accuracy:0.4468
INFO:root:14:12:26 Time cost=102.49s, throughput=292.71 samples/s
INFO:root:14:12:28 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:14:12:28 Time cost=814.47s
INFO:root:14:12:44 [Epoch 1 Batch 11600/13130] loss=1.0592, lr=0.0000366, metrics:accuracy:0.4017
INFO:root:14:12:49 [Epoch 2 Batch 400/13130] loss=1.0529, lr=0.0000088, metrics:accuracy:0.4522
INFO:root:14:12:57 [Epoch 1 Batch 6800/13130] loss=1.0817, lr=0.0000199, metrics:accuracy:0.3783
INFO:root:14:12:57 [Epoch 1 Batch 8400/13130] loss=1.0773, lr=0.0000078, metrics:accuracy:0.3876
INFO:root:14:12:59 [Epoch 1 Batch 10000/13130] loss=1.0964, lr=0.0000188, metrics:accuracy:0.3499
INFO:root:14:13:11 [Epoch 2 Batch 800/13130] loss=1.0530, lr=0.0000088, metrics:accuracy:0.4462
INFO:root:14:13:14 [Epoch 1 Batch 12000/13130] loss=1.0717, lr=0.0000363, metrics:accuracy:0.4026
INFO:root:14:13:32 [Epoch 2 Batch 1200/13130] loss=1.0462, lr=0.0000087, metrics:accuracy:0.4484
INFO:root:14:13:36 [Epoch 1 Batch 7200/13130] loss=1.0828, lr=0.0000198, metrics:accuracy:0.3799
INFO:root:14:13:36 [Epoch 1 Batch 8800/13130] loss=1.0740, lr=0.0000077, metrics:accuracy:0.3887
INFO:root:14:13:39 [Epoch 1 Batch 10400/13130] loss=1.0923, lr=0.0000187, metrics:accuracy:0.3508
INFO:root:14:13:45 [Epoch 1 Batch 12400/13130] loss=1.0653, lr=0.0000360, metrics:accuracy:0.4036
INFO:root:14:13:53 [Epoch 2 Batch 1600/13130] loss=1.0411, lr=0.0000086, metrics:accuracy:0.4505
INFO:root:14:14:14 [Epoch 2 Batch 2000/13130] loss=1.0401, lr=0.0000085, metrics:accuracy:0.4531
INFO:root:14:14:14 [Epoch 1 Batch 7600/13130] loss=1.0814, lr=0.0000196, metrics:accuracy:0.3810
INFO:root:14:14:14 [Epoch 1 Batch 9200/13130] loss=1.0674, lr=0.0000076, metrics:accuracy:0.3908
INFO:root:14:14:15 [Epoch 1 Batch 12800/13130] loss=1.0663, lr=0.0000358, metrics:accuracy:0.4046
INFO:root:14:14:19 [Epoch 1 Batch 10800/13130] loss=1.0946, lr=0.0000186, metrics:accuracy:0.3513
INFO:root:14:14:36 [Epoch 2 Batch 2400/13130] loss=1.0404, lr=0.0000085, metrics:accuracy:0.4542
INFO:root:14:14:40 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:14:53 [Epoch 1 Batch 8000/13130] loss=1.0869, lr=0.0000195, metrics:accuracy:0.3817
INFO:root:14:14:54 [Epoch 1 Batch 9600/13130] loss=1.0677, lr=0.0000076, metrics:accuracy:0.3924
INFO:root:14:14:56 [Epoch 1 Batch 11200/13130] loss=1.0973, lr=0.0000184, metrics:accuracy:0.3512
INFO:root:14:14:56 [Batch 400/3750] loss=1.1322, metrics:accuracy:0.4381
INFO:root:14:14:57 [Epoch 2 Batch 2800/13130] loss=1.0466, lr=0.0000084, metrics:accuracy:0.4544
INFO:root:14:15:12 [Batch 800/3750] loss=1.1377, metrics:accuracy:0.4333
INFO:root:14:15:19 [Epoch 2 Batch 3200/13130] loss=1.0496, lr=0.0000083, metrics:accuracy:0.4542
INFO:root:14:15:28 [Batch 1200/3750] loss=1.1360, metrics:accuracy:0.4322
INFO:root:14:15:29 [Epoch 1 Batch 11600/13130] loss=1.0934, lr=0.0000183, metrics:accuracy:0.3516
INFO:root:14:15:33 [Epoch 1 Batch 8400/13130] loss=1.0861, lr=0.0000194, metrics:accuracy:0.3821
INFO:root:14:15:33 [Epoch 1 Batch 10000/13130] loss=1.0714, lr=0.0000075, metrics:accuracy:0.3931
INFO:root:14:15:40 [Epoch 2 Batch 3600/13130] loss=1.0447, lr=0.0000083, metrics:accuracy:0.4539
INFO:root:14:15:45 [Batch 1600/3750] loss=1.1347, metrics:accuracy:0.3967
INFO:root:14:16:01 [Batch 2000/3750] loss=1.1293, metrics:accuracy:0.3734
INFO:root:14:16:02 [Epoch 2 Batch 4000/13130] loss=1.0402, lr=0.0000082, metrics:accuracy:0.4546
INFO:root:14:16:03 [Epoch 1 Batch 12000/13130] loss=1.0931, lr=0.0000182, metrics:accuracy:0.3522
INFO:root:14:16:12 [Epoch 1 Batch 8800/13130] loss=1.0771, lr=0.0000192, metrics:accuracy:0.3836
INFO:root:14:16:12 [Epoch 1 Batch 10400/13130] loss=1.0601, lr=0.0000075, metrics:accuracy:0.3947
INFO:root:14:16:17 [Batch 2400/3750] loss=1.1216, metrics:accuracy:0.3590
INFO:root:14:16:23 [Epoch 2 Batch 4400/13130] loss=1.0495, lr=0.0000081, metrics:accuracy:0.4545
INFO:root:14:16:33 [Batch 2800/3750] loss=0.9595, metrics:accuracy:0.3838
INFO:root:14:16:34 AMP: increasing loss scale to 32768.000000
INFO:root:14:16:36 [Epoch 1 Batch 12400/13130] loss=1.0905, lr=0.0000180, metrics:accuracy:0.3531
INFO:root:14:16:36 AMP: decreasing loss scale to 16384.000000
INFO:root:14:16:45 [Epoch 2 Batch 4800/13130] loss=1.0434, lr=0.0000081, metrics:accuracy:0.4541
INFO:root:14:16:50 [Batch 3200/3750] loss=0.8933, metrics:accuracy:0.4142
INFO:root:14:16:50 [Epoch 1 Batch 9200/13130] loss=1.0681, lr=0.0000191, metrics:accuracy:0.3857
INFO:root:14:16:51 [Epoch 1 Batch 10800/13130] loss=1.0623, lr=0.0000074, metrics:accuracy:0.3963
INFO:root:14:17:06 [Batch 3600/3750] loss=0.8955, metrics:accuracy:0.4377
INFO:root:14:17:07 [Epoch 2 Batch 5200/13130] loss=1.0504, lr=0.0000080, metrics:accuracy:0.4540
INFO:root:14:17:09 [Epoch 1 Batch 12800/13130] loss=1.0972, lr=0.0000179, metrics:accuracy:0.3533
INFO:root:14:17:12 validation metrics:accuracy:0.4463
INFO:root:14:17:12 Time cost=152.32s, throughput=196.95 samples/s
INFO:root:14:17:14 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:14:17:14 Time cost=1149.62s
INFO:root:14:17:28 [Epoch 2 Batch 5600/13130] loss=1.0421, lr=0.0000079, metrics:accuracy:0.4538
INFO:root:14:17:29 [Epoch 1 Batch 9600/13130] loss=1.0690, lr=0.0000190, metrics:accuracy:0.3873
INFO:root:14:17:29 [Epoch 1 Batch 11200/13130] loss=1.0707, lr=0.0000074, metrics:accuracy:0.3973
INFO:root:14:17:39 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:17:43 [Epoch 2 Batch 400/13130] loss=1.0473, lr=0.0000353, metrics:accuracy:0.4537
INFO:root:14:17:50 [Epoch 2 Batch 6000/13130] loss=1.0522, lr=0.0000079, metrics:accuracy:0.4538
INFO:root:14:17:57 [Batch 400/3750] loss=1.0495, metrics:accuracy:0.5766
INFO:root:14:18:08 [Epoch 1 Batch 10000/13130] loss=1.0743, lr=0.0000188, metrics:accuracy:0.3883
INFO:root:14:18:09 [Epoch 1 Batch 11600/13130] loss=1.0650, lr=0.0000073, metrics:accuracy:0.3986
INFO:root:14:18:11 [Epoch 2 Batch 800/13130] loss=1.0639, lr=0.0000350, metrics:accuracy:0.4427
INFO:root:14:18:11 [Epoch 2 Batch 6400/13130] loss=1.0261, lr=0.0000078, metrics:accuracy:0.4551
INFO:root:14:18:15 [Batch 800/3750] loss=1.0510, metrics:accuracy:0.5737
INFO:root:14:18:33 [Batch 1200/3750] loss=1.0514, metrics:accuracy:0.5719
INFO:root:14:18:33 [Epoch 2 Batch 6800/13130] loss=1.0416, lr=0.0000077, metrics:accuracy:0.4549
INFO:root:14:18:39 [Epoch 2 Batch 1200/13130] loss=1.0435, lr=0.0000347, metrics:accuracy:0.4510
INFO:root:14:18:47 [Epoch 1 Batch 10400/13130] loss=1.0600, lr=0.0000187, metrics:accuracy:0.3901
INFO:root:14:18:48 [Epoch 1 Batch 12000/13130] loss=1.0684, lr=0.0000073, metrics:accuracy:0.3990
INFO:root:14:18:52 [Batch 1600/3750] loss=1.0884, metrics:accuracy:0.5084
INFO:root:14:18:55 [Epoch 2 Batch 7200/13130] loss=1.0590, lr=0.0000077, metrics:accuracy:0.4541
INFO:root:14:19:06 [Epoch 2 Batch 1600/13130] loss=1.0405, lr=0.0000345, metrics:accuracy:0.4533
INFO:root:14:19:10 [Batch 2000/3750] loss=1.0892, metrics:accuracy:0.4644
INFO:root:14:19:16 [Epoch 2 Batch 7600/13130] loss=1.0407, lr=0.0000076, metrics:accuracy:0.4544
INFO:root:14:19:26 [Epoch 1 Batch 10800/13130] loss=1.0616, lr=0.0000186, metrics:accuracy:0.3921
INFO:root:14:19:27 [Epoch 1 Batch 12400/13130] loss=1.0632, lr=0.0000072, metrics:accuracy:0.4000
INFO:root:14:19:28 [Batch 2400/3750] loss=1.0829, metrics:accuracy:0.4373
INFO:root:14:19:33 [Epoch 2 Batch 2000/13130] loss=1.0344, lr=0.0000342, metrics:accuracy:0.4568
INFO:root:14:19:37 [Epoch 2 Batch 8000/13130] loss=1.0464, lr=0.0000075, metrics:accuracy:0.4542
INFO:root:14:19:46 [Batch 2800/3750] loss=1.1212, metrics:accuracy:0.4146
INFO:root:14:19:59 [Epoch 2 Batch 8400/13130] loss=1.0359, lr=0.0000075, metrics:accuracy:0.4551
INFO:root:14:20:02 [Epoch 2 Batch 2400/13130] loss=1.0393, lr=0.0000339, metrics:accuracy:0.4588
INFO:root:14:20:05 [Epoch 1 Batch 11200/13130] loss=1.0743, lr=0.0000184, metrics:accuracy:0.3931
INFO:root:14:20:05 [Batch 3200/3750] loss=1.1241, metrics:accuracy:0.3991
INFO:root:14:20:05 [Epoch 1 Batch 12800/13130] loss=1.0671, lr=0.0000072, metrics:accuracy:0.4009
INFO:root:14:20:20 [Epoch 2 Batch 8800/13130] loss=1.0577, lr=0.0000074, metrics:accuracy:0.4540
INFO:root:14:20:23 [Batch 3600/3750] loss=1.1306, metrics:accuracy:0.3857
INFO:root:14:20:30 [Epoch 2 Batch 2800/13130] loss=1.0413, lr=0.0000337, metrics:accuracy:0.4598
INFO:root:14:20:30 validation metrics:accuracy:0.3827
INFO:root:14:20:30 Time cost=170.76s, throughput=175.69 samples/s
INFO:root:14:20:32 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:14:20:32 Time cost=1395.37s
INFO:root:14:20:38 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:20:42 [Epoch 2 Batch 9200/13130] loss=1.0494, lr=0.0000073, metrics:accuracy:0.4542
INFO:root:14:20:43 [Epoch 1 Batch 11600/13130] loss=1.0630, lr=0.0000183, metrics:accuracy:0.3945
INFO:root:14:20:59 [Batch 400/3750] loss=1.1223, metrics:accuracy:0.4234
INFO:root:14:20:59 [Epoch 2 Batch 3200/13130] loss=1.0487, lr=0.0000334, metrics:accuracy:0.4582
INFO:root:14:21:04 [Epoch 2 Batch 9600/13130] loss=1.0352, lr=0.0000073, metrics:accuracy:0.4543
INFO:root:14:21:13 [Epoch 2 Batch 400/13130] loss=1.0943, lr=0.0000176, metrics:accuracy:0.3663
INFO:root:14:21:17 [Epoch 1 Batch 12000/13130] loss=1.0732, lr=0.0000182, metrics:accuracy:0.3953
INFO:root:14:21:20 [Batch 800/3750] loss=1.1316, metrics:accuracy:0.4161
INFO:root:14:21:26 [Epoch 2 Batch 10000/13130] loss=1.0469, lr=0.0000072, metrics:accuracy:0.4542
INFO:root:14:21:29 [Epoch 2 Batch 3600/13130] loss=1.0425, lr=0.0000331, metrics:accuracy:0.4573
INFO:root:14:21:40 [Batch 1200/3750] loss=1.1287, metrics:accuracy:0.4142
INFO:root:14:21:48 [Epoch 2 Batch 10400/13130] loss=1.0336, lr=0.0000071, metrics:accuracy:0.4545
INFO:root:14:21:52 [Epoch 1 Batch 12400/13130] loss=1.0641, lr=0.0000180, metrics:accuracy:0.3966
INFO:root:14:21:53 [Epoch 2 Batch 800/13130] loss=1.0942, lr=0.0000175, metrics:accuracy:0.3642
INFO:root:14:22:00 [Epoch 2 Batch 4000/13130] loss=1.0420, lr=0.0000328, metrics:accuracy:0.4580
INFO:root:14:22:01 [Batch 1600/3750] loss=1.1208, metrics:accuracy:0.3832
INFO:root:14:22:09 [Epoch 2 Batch 10800/13130] loss=1.0431, lr=0.0000071, metrics:accuracy:0.4547
INFO:root:14:22:22 [Batch 2000/3750] loss=1.1182, metrics:accuracy:0.3604
INFO:root:14:22:27 [Epoch 1 Batch 12800/13130] loss=1.0665, lr=0.0000179, metrics:accuracy:0.3977
INFO:root:14:22:30 [Epoch 2 Batch 4400/13130] loss=1.0470, lr=0.0000326, metrics:accuracy:0.4578
INFO:root:14:22:31 [Epoch 2 Batch 11200/13130] loss=1.0309, lr=0.0000070, metrics:accuracy:0.4553
INFO:root:14:22:34 [Epoch 2 Batch 1200/13130] loss=1.0919, lr=0.0000174, metrics:accuracy:0.3673
INFO:root:14:22:43 [Batch 2400/3750] loss=1.1049, metrics:accuracy:0.3480
INFO:root:14:22:54 [Epoch 2 Batch 11600/13130] loss=1.0407, lr=0.0000069, metrics:accuracy:0.4558
INFO:root:14:22:55 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:23:00 [Epoch 2 Batch 4800/13130] loss=1.0761, lr=0.0000323, metrics:accuracy:0.4533
INFO:root:14:23:04 [Batch 2800/3750] loss=0.9709, metrics:accuracy:0.3767
INFO:root:14:23:14 [Epoch 2 Batch 1600/13130] loss=1.0940, lr=0.0000172, metrics:accuracy:0.3653
INFO:root:14:23:16 [Batch 400/3750] loss=1.1066, metrics:accuracy:0.4531
INFO:root:14:23:16 [Epoch 2 Batch 12000/13130] loss=1.0381, lr=0.0000069, metrics:accuracy:0.4559
INFO:root:14:23:24 [Batch 3200/3750] loss=0.9118, metrics:accuracy:0.4101
INFO:root:14:23:31 [Epoch 2 Batch 5200/13130] loss=1.0639, lr=0.0000320, metrics:accuracy:0.4524
INFO:root:14:23:37 [Batch 800/3750] loss=1.1112, metrics:accuracy:0.4456
INFO:root:14:23:38 [Epoch 2 Batch 12400/13130] loss=1.0359, lr=0.0000068, metrics:accuracy:0.4562
INFO:root:14:23:45 [Batch 3600/3750] loss=0.9159, metrics:accuracy:0.4364
INFO:root:14:23:53 validation metrics:accuracy:0.4456
INFO:root:14:23:53 Time cost=195.08s, throughput=153.79 samples/s
INFO:root:14:23:55 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:14:23:55 Time cost=1412.05s
INFO:root:14:23:55 [Epoch 2 Batch 2000/13130] loss=1.0900, lr=0.0000171, metrics:accuracy:0.3688
INFO:root:14:23:56 [Batch 1200/3750] loss=1.1097, metrics:accuracy:0.4425
INFO:root:14:24:00 [Epoch 2 Batch 12800/13130] loss=1.0168, lr=0.0000067, metrics:accuracy:0.4569
INFO:root:14:24:02 [Epoch 2 Batch 5600/13130] loss=1.0462, lr=0.0000318, metrics:accuracy:0.4526
INFO:root:14:24:16 [Batch 1600/3750] loss=1.1550, metrics:accuracy:0.3941
INFO:root:14:24:18 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:24:29 [Batch 400/3750] loss=0.9980, metrics:accuracy:0.5519
INFO:root:14:24:29 [Epoch 2 Batch 400/13130] loss=1.0544, lr=0.0000071, metrics:accuracy:0.4494
INFO:root:14:24:32 [Epoch 2 Batch 6000/13130] loss=1.0588, lr=0.0000315, metrics:accuracy:0.4525
INFO:root:14:24:36 [Epoch 2 Batch 2400/13130] loss=1.0946, lr=0.0000170, metrics:accuracy:0.3681
INFO:root:14:24:36 [Batch 2000/3750] loss=1.1583, metrics:accuracy:0.3608
INFO:root:14:24:39 [Batch 800/3750] loss=1.0083, metrics:accuracy:0.5463
INFO:root:14:24:43 AMP: decreasing loss scale to 8192.000000
INFO:root:14:24:51 [Batch 1200/3750] loss=1.0039, metrics:accuracy:0.5449
INFO:root:14:24:57 [Batch 2400/3750] loss=1.1478, metrics:accuracy:0.3403
INFO:root:14:25:02 [Batch 1600/3750] loss=1.1200, metrics:accuracy:0.4961
INFO:root:14:25:03 [Epoch 2 Batch 6400/13130] loss=1.0381, lr=0.0000312, metrics:accuracy:0.4535
INFO:root:14:25:04 [Epoch 2 Batch 800/13130] loss=1.0564, lr=0.0000070, metrics:accuracy:0.4448
INFO:root:14:25:14 [Batch 2000/3750] loss=1.1339, metrics:accuracy:0.4616
INFO:root:14:25:17 [Epoch 2 Batch 2800/13130] loss=1.0942, lr=0.0000168, metrics:accuracy:0.3682
INFO:root:14:25:18 [Batch 2800/3750] loss=0.9681, metrics:accuracy:0.3707
INFO:root:14:25:25 [Batch 2400/3750] loss=1.1177, metrics:accuracy:0.4398
INFO:root:14:25:34 [Epoch 2 Batch 6800/13130] loss=1.0449, lr=0.0000309, metrics:accuracy:0.4534
INFO:root:14:25:36 [Batch 2800/3750] loss=1.0100, metrics:accuracy:0.4439
INFO:root:14:25:39 [Epoch 2 Batch 1200/13130] loss=1.0471, lr=0.0000069, metrics:accuracy:0.4462
INFO:root:14:25:39 [Batch 3200/3750] loss=0.8997, metrics:accuracy:0.4063
INFO:root:14:25:48 [Batch 3200/3750] loss=0.9533, metrics:accuracy:0.4550
INFO:root:14:25:59 [Epoch 2 Batch 3200/13130] loss=1.0910, lr=0.0000167, metrics:accuracy:0.3690
INFO:root:14:25:59 [Batch 3600/3750] loss=0.9571, metrics:accuracy:0.4632
INFO:root:14:26:00 [Batch 3600/3750] loss=0.8986, metrics:accuracy:0.4343
INFO:root:14:26:04 validation metrics:accuracy:0.4664
INFO:root:14:26:04 Time cost=105.96s, throughput=283.13 samples/s
INFO:root:14:26:04 [Epoch 2 Batch 7200/13130] loss=1.0612, lr=0.0000307, metrics:accuracy:0.4527
INFO:root:14:26:05 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:14:26:05 Time cost=817.18s
INFO:root:14:26:07 validation metrics:accuracy:0.4440
INFO:root:14:26:07 Time cost=192.24s, throughput=156.06 samples/s
INFO:root:14:26:09 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:14:26:09 Time cost=1458.57s
INFO:root:14:26:13 [Epoch 2 Batch 1600/13130] loss=1.0422, lr=0.0000069, metrics:accuracy:0.4508
INFO:root:14:26:27 [Epoch 3 Batch 400/13130] loss=1.0197, lr=0.0000066, metrics:accuracy:0.4672
INFO:root:14:26:34 [Epoch 2 Batch 7600/13130] loss=1.0416, lr=0.0000304, metrics:accuracy:0.4535
INFO:root:14:26:39 [Epoch 2 Batch 3600/13130] loss=1.0951, lr=0.0000166, metrics:accuracy:0.3690
INFO:root:14:26:48 [Epoch 2 Batch 400/13130] loss=1.0514, lr=0.0000176, metrics:accuracy:0.4528
INFO:root:14:26:48 [Epoch 3 Batch 800/13130] loss=1.0176, lr=0.0000065, metrics:accuracy:0.4791
INFO:root:14:26:51 [Epoch 2 Batch 2000/13130] loss=1.0416, lr=0.0000068, metrics:accuracy:0.4531
INFO:root:14:27:03 [Epoch 2 Batch 8000/13130] loss=1.0480, lr=0.0000301, metrics:accuracy:0.4531
INFO:root:14:27:10 [Epoch 3 Batch 1200/13130] loss=1.0264, lr=0.0000065, metrics:accuracy:0.4758
INFO:root:14:27:19 [Epoch 2 Batch 4000/13130] loss=1.0903, lr=0.0000164, metrics:accuracy:0.3700
INFO:root:14:27:27 [Epoch 2 Batch 800/13130] loss=1.0555, lr=0.0000175, metrics:accuracy:0.4503
INFO:root:14:27:30 [Epoch 2 Batch 2400/13130] loss=1.0412, lr=0.0000068, metrics:accuracy:0.4536
INFO:root:14:27:32 [Epoch 3 Batch 1600/13130] loss=1.0174, lr=0.0000064, metrics:accuracy:0.4752
INFO:root:14:27:34 [Epoch 2 Batch 8400/13130] loss=1.0438, lr=0.0000299, metrics:accuracy:0.4536
INFO:root:14:27:53 [Epoch 3 Batch 2000/13130] loss=1.0158, lr=0.0000063, metrics:accuracy:0.4756
INFO:root:14:27:59 [Epoch 2 Batch 4400/13130] loss=1.0889, lr=0.0000163, metrics:accuracy:0.3716
INFO:root:14:28:04 [Epoch 2 Batch 8800/13130] loss=1.0535, lr=0.0000296, metrics:accuracy:0.4531
INFO:root:14:28:06 [Epoch 2 Batch 1200/13130] loss=1.0448, lr=0.0000174, metrics:accuracy:0.4517
INFO:root:14:28:09 [Epoch 2 Batch 2800/13130] loss=1.0514, lr=0.0000067, metrics:accuracy:0.4529
INFO:root:14:28:14 [Epoch 3 Batch 2400/13130] loss=1.0251, lr=0.0000063, metrics:accuracy:0.4752
INFO:root:14:28:35 [Epoch 2 Batch 9200/13130] loss=1.0511, lr=0.0000293, metrics:accuracy:0.4528
INFO:root:14:28:36 [Epoch 3 Batch 2800/13130] loss=1.0131, lr=0.0000062, metrics:accuracy:0.4774
INFO:root:14:28:40 [Epoch 2 Batch 4800/13130] loss=1.0885, lr=0.0000162, metrics:accuracy:0.3725
INFO:root:14:28:45 [Epoch 2 Batch 1600/13130] loss=1.0406, lr=0.0000172, metrics:accuracy:0.4554
INFO:root:14:28:48 [Epoch 2 Batch 3200/13130] loss=1.0517, lr=0.0000067, metrics:accuracy:0.4527
INFO:root:14:28:58 [Epoch 3 Batch 3200/13130] loss=1.0090, lr=0.0000061, metrics:accuracy:0.4797
INFO:root:14:29:05 [Epoch 2 Batch 9600/13130] loss=1.0413, lr=0.0000291, metrics:accuracy:0.4527
INFO:root:14:29:19 [Epoch 3 Batch 3600/13130] loss=1.0154, lr=0.0000061, metrics:accuracy:0.4804
INFO:root:14:29:21 [Epoch 2 Batch 5200/13130] loss=1.0968, lr=0.0000160, metrics:accuracy:0.3721
INFO:root:14:29:23 [Epoch 2 Batch 2000/13130] loss=1.0369, lr=0.0000171, metrics:accuracy:0.4569
INFO:root:14:29:26 [Epoch 2 Batch 3600/13130] loss=1.0462, lr=0.0000066, metrics:accuracy:0.4526
INFO:root:14:29:36 [Epoch 2 Batch 10000/13130] loss=1.0500, lr=0.0000288, metrics:accuracy:0.4528
INFO:root:14:29:41 [Epoch 3 Batch 4000/13130] loss=1.0103, lr=0.0000060, metrics:accuracy:0.4819
INFO:root:14:30:02 [Epoch 3 Batch 4400/13130] loss=1.0261, lr=0.0000059, metrics:accuracy:0.4807
INFO:root:14:30:02 [Epoch 2 Batch 5600/13130] loss=1.0908, lr=0.0000159, metrics:accuracy:0.3718
INFO:root:14:30:03 [Epoch 2 Batch 2400/13130] loss=1.0409, lr=0.0000170, metrics:accuracy:0.4562
INFO:root:14:30:06 [Epoch 2 Batch 4000/13130] loss=1.0428, lr=0.0000066, metrics:accuracy:0.4536
INFO:root:14:30:06 [Epoch 2 Batch 10400/13130] loss=1.0408, lr=0.0000285, metrics:accuracy:0.4530
INFO:root:14:30:24 [Epoch 3 Batch 4800/13130] loss=1.0135, lr=0.0000059, metrics:accuracy:0.4805
INFO:root:14:30:36 [Epoch 2 Batch 10800/13130] loss=1.0427, lr=0.0000282, metrics:accuracy:0.4530
INFO:root:14:30:42 [Epoch 2 Batch 6000/13130] loss=1.0935, lr=0.0000157, metrics:accuracy:0.3717
INFO:root:14:30:42 [Epoch 2 Batch 2800/13130] loss=1.0448, lr=0.0000168, metrics:accuracy:0.4567
INFO:root:14:30:45 [Epoch 2 Batch 4400/13130] loss=1.0524, lr=0.0000065, metrics:accuracy:0.4537
INFO:root:14:30:45 [Epoch 3 Batch 5200/13130] loss=1.0308, lr=0.0000058, metrics:accuracy:0.4798
INFO:root:14:31:06 [Epoch 2 Batch 11200/13130] loss=1.0285, lr=0.0000280, metrics:accuracy:0.4537
INFO:root:14:31:07 [Epoch 3 Batch 5600/13130] loss=1.0121, lr=0.0000057, metrics:accuracy:0.4797
INFO:root:14:31:22 [Epoch 2 Batch 3200/13130] loss=1.0497, lr=0.0000167, metrics:accuracy:0.4543
INFO:root:14:31:22 [Epoch 2 Batch 6400/13130] loss=1.0887, lr=0.0000156, metrics:accuracy:0.3720
INFO:root:14:31:25 [Epoch 2 Batch 4800/13130] loss=1.0438, lr=0.0000065, metrics:accuracy:0.4535
INFO:root:14:31:29 [Epoch 3 Batch 6000/13130] loss=1.0208, lr=0.0000056, metrics:accuracy:0.4797
INFO:root:14:31:29 AMP: increasing loss scale to 16384.000000
INFO:root:14:31:37 [Epoch 2 Batch 11600/13130] loss=1.0400, lr=0.0000277, metrics:accuracy:0.4540
INFO:root:14:31:50 [Epoch 3 Batch 6400/13130] loss=1.0307, lr=0.0000056, metrics:accuracy:0.4792
INFO:root:14:32:01 [Epoch 2 Batch 3600/13130] loss=1.0445, lr=0.0000166, metrics:accuracy:0.4540
INFO:root:14:32:04 [Epoch 2 Batch 5200/13130] loss=1.0540, lr=0.0000064, metrics:accuracy:0.4522
INFO:root:14:32:04 [Epoch 2 Batch 6800/13130] loss=1.0881, lr=0.0000155, metrics:accuracy:0.3725
INFO:root:14:32:07 [Epoch 2 Batch 12000/13130] loss=1.0422, lr=0.0000274, metrics:accuracy:0.4543
INFO:root:14:32:12 [Epoch 3 Batch 6800/13130] loss=1.0020, lr=0.0000055, metrics:accuracy:0.4805
INFO:root:14:32:34 [Epoch 3 Batch 7200/13130] loss=1.0307, lr=0.0000054, metrics:accuracy:0.4799
INFO:root:14:32:37 [Epoch 2 Batch 12400/13130] loss=1.0374, lr=0.0000272, metrics:accuracy:0.4547
INFO:root:14:32:40 [Epoch 2 Batch 4000/13130] loss=1.0423, lr=0.0000164, metrics:accuracy:0.4539
INFO:root:14:32:43 [Epoch 2 Batch 5600/13130] loss=1.0405, lr=0.0000064, metrics:accuracy:0.4522
INFO:root:14:32:44 [Epoch 2 Batch 7200/13130] loss=1.0977, lr=0.0000153, metrics:accuracy:0.3715
INFO:root:14:32:55 [Epoch 3 Batch 7600/13130] loss=1.0234, lr=0.0000054, metrics:accuracy:0.4794
INFO:root:14:33:07 AMP: decreasing loss scale to 8192.000000
INFO:root:14:33:08 [Epoch 2 Batch 12800/13130] loss=1.0250, lr=0.0000269, metrics:accuracy:0.4554
INFO:root:14:33:17 [Epoch 3 Batch 8000/13130] loss=1.0196, lr=0.0000053, metrics:accuracy:0.4794
INFO:root:14:33:19 [Epoch 2 Batch 4400/13130] loss=1.0517, lr=0.0000163, metrics:accuracy:0.4544
INFO:root:14:33:22 [Epoch 2 Batch 6000/13130] loss=1.0530, lr=0.0000063, metrics:accuracy:0.4518
INFO:root:14:33:25 [Epoch 2 Batch 7600/13130] loss=1.0874, lr=0.0000152, metrics:accuracy:0.3717
INFO:root:14:33:32 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:33:38 [Epoch 3 Batch 8400/13130] loss=1.0097, lr=0.0000052, metrics:accuracy:0.4803
INFO:root:14:33:48 [Batch 400/3750] loss=1.0309, metrics:accuracy:0.5497
INFO:root:14:33:59 [Epoch 2 Batch 4800/13130] loss=1.0392, lr=0.0000162, metrics:accuracy:0.4546
INFO:root:14:34:00 [Epoch 3 Batch 8800/13130] loss=1.0103, lr=0.0000052, metrics:accuracy:0.4803
INFO:root:14:34:00 [Epoch 2 Batch 8000/13130] loss=1.0887, lr=0.0000151, metrics:accuracy:0.3724
INFO:root:14:34:02 [Epoch 2 Batch 6400/13130] loss=1.0266, lr=0.0000062, metrics:accuracy:0.4532
INFO:root:14:34:05 [Batch 800/3750] loss=1.0367, metrics:accuracy:0.5459
INFO:root:14:34:21 [Epoch 3 Batch 9200/13130] loss=1.0262, lr=0.0000051, metrics:accuracy:0.4796
INFO:root:14:34:21 [Batch 1200/3750] loss=1.0247, metrics:accuracy:0.5486
INFO:root:14:34:33 [Epoch 2 Batch 8400/13130] loss=1.0891, lr=0.0000149, metrics:accuracy:0.3730
INFO:root:14:34:37 [Batch 1600/3750] loss=1.1553, metrics:accuracy:0.4955
INFO:root:14:34:39 [Epoch 2 Batch 5200/13130] loss=1.0510, lr=0.0000160, metrics:accuracy:0.4543
INFO:root:14:34:42 [Epoch 2 Batch 6800/13130] loss=1.0424, lr=0.0000062, metrics:accuracy:0.4534
INFO:root:14:34:42 [Epoch 3 Batch 9600/13130] loss=1.0153, lr=0.0000050, metrics:accuracy:0.4796
INFO:root:14:34:54 [Batch 2000/3750] loss=1.1732, metrics:accuracy:0.4584
INFO:root:14:35:04 [Epoch 3 Batch 10000/13130] loss=1.0078, lr=0.0000050, metrics:accuracy:0.4801
INFO:root:14:35:06 [Epoch 2 Batch 8800/13130] loss=1.0895, lr=0.0000148, metrics:accuracy:0.3734
INFO:root:14:35:10 [Batch 2400/3750] loss=1.1548, metrics:accuracy:0.4348
INFO:root:14:35:18 [Epoch 2 Batch 5600/13130] loss=1.0423, lr=0.0000159, metrics:accuracy:0.4543
INFO:root:14:35:21 [Epoch 2 Batch 7200/13130] loss=1.0596, lr=0.0000061, metrics:accuracy:0.4525
INFO:root:14:35:26 [Epoch 3 Batch 10400/13130] loss=1.0056, lr=0.0000049, metrics:accuracy:0.4805
INFO:root:14:35:27 [Batch 2800/3750] loss=1.0016, metrics:accuracy:0.4408
INFO:root:14:35:39 [Epoch 2 Batch 9200/13130] loss=1.0927, lr=0.0000147, metrics:accuracy:0.3733
INFO:root:14:35:44 [Batch 3200/3750] loss=0.9306, metrics:accuracy:0.4521
INFO:root:14:35:47 [Epoch 3 Batch 10800/13130] loss=1.0150, lr=0.0000048, metrics:accuracy:0.4805
INFO:root:14:35:56 [Epoch 2 Batch 6000/13130] loss=1.0506, lr=0.0000157, metrics:accuracy:0.4552
INFO:root:14:36:00 [Batch 3600/3750] loss=0.9329, metrics:accuracy:0.4610
INFO:root:14:36:00 [Epoch 2 Batch 7600/13130] loss=1.0415, lr=0.0000061, metrics:accuracy:0.4529
INFO:root:14:36:06 validation metrics:accuracy:0.4637
INFO:root:14:36:06 Time cost=153.55s, throughput=195.37 samples/s
INFO:root:14:36:08 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:14:36:08 Time cost=1133.71s
INFO:root:14:36:08 [Epoch 3 Batch 11200/13130] loss=1.0234, lr=0.0000048, metrics:accuracy:0.4803
INFO:root:14:36:11 [Epoch 2 Batch 9600/13130] loss=1.0961, lr=0.0000145, metrics:accuracy:0.3730
INFO:root:14:36:30 [Epoch 3 Batch 11600/13130] loss=1.0087, lr=0.0000047, metrics:accuracy:0.4808
INFO:root:14:36:36 [Epoch 2 Batch 6400/13130] loss=1.0242, lr=0.0000156, metrics:accuracy:0.4568
INFO:root:14:36:38 [Epoch 3 Batch 400/13130] loss=1.0160, lr=0.0000264, metrics:accuracy:0.4784
INFO:root:14:36:39 [Epoch 2 Batch 8000/13130] loss=1.0476, lr=0.0000060, metrics:accuracy:0.4529
INFO:root:14:36:51 [Epoch 3 Batch 12000/13130] loss=0.9990, lr=0.0000046, metrics:accuracy:0.4815
INFO:root:14:36:51 [Epoch 2 Batch 10000/13130] loss=1.0906, lr=0.0000144, metrics:accuracy:0.3731
INFO:root:14:37:09 [Epoch 3 Batch 800/13130] loss=1.0133, lr=0.0000261, metrics:accuracy:0.4805
INFO:root:14:37:13 [Epoch 3 Batch 12400/13130] loss=0.9999, lr=0.0000046, metrics:accuracy:0.4820
INFO:root:14:37:15 [Epoch 2 Batch 6800/13130] loss=1.0431, lr=0.0000155, metrics:accuracy:0.4567
INFO:root:14:37:19 [Epoch 2 Batch 8400/13130] loss=1.0373, lr=0.0000060, metrics:accuracy:0.4535
INFO:root:14:37:31 [Epoch 2 Batch 10400/13130] loss=1.0879, lr=0.0000143, metrics:accuracy:0.3735
INFO:root:14:37:34 [Epoch 3 Batch 12800/13130] loss=0.9884, lr=0.0000045, metrics:accuracy:0.4828
INFO:root:14:37:39 [Epoch 3 Batch 1200/13130] loss=1.0228, lr=0.0000258, metrics:accuracy:0.4794
INFO:root:14:37:52 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:37:54 [Epoch 2 Batch 7200/13130] loss=1.0590, lr=0.0000153, metrics:accuracy:0.4559
INFO:root:14:37:58 [Epoch 2 Batch 8800/13130] loss=1.0575, lr=0.0000059, metrics:accuracy:0.4528
INFO:root:14:38:03 [Batch 400/3750] loss=0.9511, metrics:accuracy:0.6041
INFO:root:14:38:10 [Epoch 3 Batch 1600/13130] loss=1.0055, lr=0.0000256, metrics:accuracy:0.4833
INFO:root:14:38:12 [Epoch 2 Batch 10800/13130] loss=1.0888, lr=0.0000141, metrics:accuracy:0.3738
INFO:root:14:38:14 [Batch 800/3750] loss=0.9634, metrics:accuracy:0.5945
INFO:root:14:38:25 [Batch 1200/3750] loss=0.9577, metrics:accuracy:0.5940
INFO:root:14:38:33 [Epoch 2 Batch 7600/13130] loss=1.0378, lr=0.0000152, metrics:accuracy:0.4570
INFO:root:14:38:36 [Batch 1600/3750] loss=1.1541, metrics:accuracy:0.5250
INFO:root:14:38:37 [Epoch 2 Batch 9200/13130] loss=1.0471, lr=0.0000059, metrics:accuracy:0.4531
INFO:root:14:38:40 [Epoch 3 Batch 2000/13130] loss=1.0060, lr=0.0000253, metrics:accuracy:0.4846
INFO:root:14:38:47 [Batch 2000/3750] loss=1.1687, metrics:accuracy:0.4784
INFO:root:14:38:53 [Epoch 2 Batch 11200/13130] loss=1.0849, lr=0.0000140, metrics:accuracy:0.3746
INFO:root:14:38:58 [Batch 2400/3750] loss=1.1594, metrics:accuracy:0.4488
INFO:root:14:39:09 [Batch 2800/3750] loss=1.0126, metrics:accuracy:0.4518
INFO:root:14:39:10 [Epoch 3 Batch 2400/13130] loss=1.0245, lr=0.0000250, metrics:accuracy:0.4828
INFO:root:14:39:12 [Epoch 2 Batch 8000/13130] loss=1.0429, lr=0.0000151, metrics:accuracy:0.4567
INFO:root:14:39:16 [Epoch 2 Batch 9600/13130] loss=1.0363, lr=0.0000058, metrics:accuracy:0.4532
INFO:root:14:39:16 AMP: increasing loss scale to 16384.000000
INFO:root:14:39:20 [Batch 3200/3750] loss=0.9480, metrics:accuracy:0.4624
INFO:root:14:39:31 [Batch 3600/3750] loss=0.9510, metrics:accuracy:0.4695
INFO:root:14:39:34 [Epoch 2 Batch 11600/13130] loss=1.0929, lr=0.0000138, metrics:accuracy:0.3745
INFO:root:14:39:35 validation metrics:accuracy:0.4725
INFO:root:14:39:35 Time cost=103.29s, throughput=290.45 samples/s
INFO:root:14:39:37 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:14:39:37 Time cost=811.70s
INFO:root:14:39:41 [Epoch 3 Batch 2800/13130] loss=1.0053, lr=0.0000248, metrics:accuracy:0.4855
INFO:root:14:39:51 [Epoch 2 Batch 8400/13130] loss=1.0382, lr=0.0000149, metrics:accuracy:0.4573
INFO:root:14:39:55 [Epoch 2 Batch 10000/13130] loss=1.0476, lr=0.0000058, metrics:accuracy:0.4530
INFO:root:14:39:58 [Epoch 4 Batch 400/13130] loss=0.9886, lr=0.0000044, metrics:accuracy:0.5130
INFO:root:14:40:11 [Epoch 3 Batch 3200/13130] loss=1.0021, lr=0.0000245, metrics:accuracy:0.4873
INFO:root:14:40:15 [Epoch 2 Batch 12000/13130] loss=1.0897, lr=0.0000137, metrics:accuracy:0.3745
INFO:root:14:40:20 [Epoch 4 Batch 800/13130] loss=0.9960, lr=0.0000043, metrics:accuracy:0.5054
INFO:root:14:40:31 [Epoch 2 Batch 8800/13130] loss=1.0539, lr=0.0000148, metrics:accuracy:0.4561
INFO:root:14:40:35 [Epoch 2 Batch 10400/13130] loss=1.0369, lr=0.0000057, metrics:accuracy:0.4535
INFO:root:14:40:41 [Epoch 4 Batch 1200/13130] loss=0.9931, lr=0.0000042, metrics:accuracy:0.5073
INFO:root:14:40:42 [Epoch 3 Batch 3600/13130] loss=1.0096, lr=0.0000242, metrics:accuracy:0.4867
INFO:root:14:40:56 [Epoch 2 Batch 12400/13130] loss=1.0882, lr=0.0000136, metrics:accuracy:0.3751
INFO:root:14:41:04 [Epoch 4 Batch 1600/13130] loss=0.9843, lr=0.0000042, metrics:accuracy:0.5064
INFO:root:14:41:10 [Epoch 2 Batch 9200/13130] loss=1.0451, lr=0.0000147, metrics:accuracy:0.4562
INFO:root:14:41:12 [Epoch 3 Batch 4000/13130] loss=1.0074, lr=0.0000240, metrics:accuracy:0.4870
INFO:root:14:41:13 [Epoch 2 Batch 10800/13130] loss=1.0426, lr=0.0000056, metrics:accuracy:0.4535
INFO:root:14:41:26 [Epoch 4 Batch 2000/13130] loss=0.9739, lr=0.0000041, metrics:accuracy:0.5053
INFO:root:14:41:36 [Epoch 2 Batch 12800/13130] loss=1.0824, lr=0.0000134, metrics:accuracy:0.3760
INFO:root:14:41:41 [Epoch 3 Batch 4400/13130] loss=1.0193, lr=0.0000237, metrics:accuracy:0.4864
INFO:root:14:41:47 [Epoch 4 Batch 2400/13130] loss=0.9903, lr=0.0000040, metrics:accuracy:0.5069
INFO:root:14:41:49 [Epoch 2 Batch 9600/13130] loss=1.0327, lr=0.0000145, metrics:accuracy:0.4564
INFO:root:14:41:53 [Epoch 2 Batch 11200/13130] loss=1.0326, lr=0.0000056, metrics:accuracy:0.4538
INFO:root:14:42:09 [Epoch 4 Batch 2800/13130] loss=0.9844, lr=0.0000040, metrics:accuracy:0.5061
INFO:root:14:42:10 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:42:12 [Epoch 3 Batch 4800/13130] loss=1.0129, lr=0.0000234, metrics:accuracy:0.4870
INFO:root:14:42:28 [Epoch 2 Batch 10000/13130] loss=1.0446, lr=0.0000144, metrics:accuracy:0.4567
INFO:root:14:42:29 [Batch 400/3750] loss=1.0802, metrics:accuracy:0.4241
INFO:root:14:42:31 [Epoch 4 Batch 3200/13130] loss=0.9897, lr=0.0000039, metrics:accuracy:0.5056
INFO:root:14:42:33 [Epoch 2 Batch 11600/13130] loss=1.0429, lr=0.0000055, metrics:accuracy:0.4539
INFO:root:14:42:40 [Epoch 3 Batch 5200/13130] loss=1.0319, lr=0.0000231, metrics:accuracy:0.4856
INFO:root:14:42:47 [Batch 800/3750] loss=1.0836, metrics:accuracy:0.4208
INFO:root:14:42:52 [Epoch 4 Batch 3600/13130] loss=0.9825, lr=0.0000038, metrics:accuracy:0.5063
INFO:root:14:43:05 [Batch 1200/3750] loss=1.0846, metrics:accuracy:0.4203
INFO:root:14:43:08 [Epoch 2 Batch 10400/13130] loss=1.0309, lr=0.0000143, metrics:accuracy:0.4572
INFO:root:14:43:09 [Epoch 3 Batch 5600/13130] loss=1.0078, lr=0.0000229, metrics:accuracy:0.4857
INFO:root:14:43:12 [Epoch 2 Batch 12000/13130] loss=1.0381, lr=0.0000055, metrics:accuracy:0.4544
INFO:root:14:43:14 [Epoch 4 Batch 4000/13130] loss=0.9832, lr=0.0000038, metrics:accuracy:0.5069
INFO:root:14:43:24 [Batch 1600/3750] loss=1.0807, metrics:accuracy:0.3975
INFO:root:14:43:36 [Epoch 4 Batch 4400/13130] loss=0.9861, lr=0.0000037, metrics:accuracy:0.5068
INFO:root:14:43:37 [Epoch 3 Batch 6000/13130] loss=1.0180, lr=0.0000226, metrics:accuracy:0.4858
INFO:root:14:43:42 [Batch 2000/3750] loss=1.0734, metrics:accuracy:0.3832
INFO:root:14:43:46 [Epoch 2 Batch 10800/13130] loss=1.0414, lr=0.0000141, metrics:accuracy:0.4571
INFO:root:14:43:50 [Epoch 2 Batch 12400/13130] loss=1.0384, lr=0.0000054, metrics:accuracy:0.4546
INFO:root:14:43:57 [Epoch 4 Batch 4800/13130] loss=0.9960, lr=0.0000036, metrics:accuracy:0.5054
INFO:root:14:44:01 [Batch 2400/3750] loss=1.0633, metrics:accuracy:0.3775
INFO:root:14:44:05 [Epoch 3 Batch 6400/13130] loss=1.0165, lr=0.0000223, metrics:accuracy:0.4851
INFO:root:14:44:19 [Epoch 4 Batch 5200/13130] loss=0.9958, lr=0.0000036, metrics:accuracy:0.5048
INFO:root:14:44:19 [Batch 2800/3750] loss=1.0956, metrics:accuracy:0.3796
INFO:root:14:44:25 [Epoch 2 Batch 11200/13130] loss=1.0263, lr=0.0000140, metrics:accuracy:0.4577
INFO:root:14:44:29 [Epoch 2 Batch 12800/13130] loss=1.0184, lr=0.0000054, metrics:accuracy:0.4556
INFO:root:14:44:33 [Epoch 3 Batch 6800/13130] loss=1.0043, lr=0.0000221, metrics:accuracy:0.4854
INFO:root:14:44:38 [Batch 3200/3750] loss=1.0901, metrics:accuracy:0.3878
INFO:root:14:44:41 [Epoch 4 Batch 5600/13130] loss=0.9882, lr=0.0000035, metrics:accuracy:0.5053
INFO:root:14:44:56 [Batch 3600/3750] loss=1.1018, metrics:accuracy:0.3911
INFO:root:14:45:01 [Epoch 3 Batch 7200/13130] loss=1.0229, lr=0.0000218, metrics:accuracy:0.4848
INFO:root:14:45:02 [Epoch 4 Batch 6000/13130] loss=1.0014, lr=0.0000034, metrics:accuracy:0.5046
INFO:root:14:45:02 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:45:03 validation metrics:accuracy:0.3931
INFO:root:14:45:03 Time cost=172.30s, throughput=174.11 samples/s
INFO:root:14:45:05 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:14:45:05 Time cost=1472.93s
INFO:root:14:45:05 [Epoch 2 Batch 11600/13130] loss=1.0358, lr=0.0000138, metrics:accuracy:0.4581
INFO:root:14:45:24 [Batch 400/3750] loss=1.0119, metrics:accuracy:0.5475
INFO:root:14:45:24 [Epoch 4 Batch 6400/13130] loss=0.9943, lr=0.0000034, metrics:accuracy:0.5045
INFO:root:14:45:30 [Epoch 3 Batch 7600/13130] loss=1.0157, lr=0.0000215, metrics:accuracy:0.4847
INFO:root:14:45:40 [Epoch 2 Batch 12000/13130] loss=1.0404, lr=0.0000137, metrics:accuracy:0.4583
INFO:root:14:45:45 [Batch 800/3750] loss=1.0219, metrics:accuracy:0.5423
INFO:root:14:45:45 [Epoch 3 Batch 400/13130] loss=1.0898, lr=0.0000132, metrics:accuracy:0.3691
INFO:root:14:45:45 [Epoch 4 Batch 6800/13130] loss=0.9881, lr=0.0000033, metrics:accuracy:0.5044
INFO:root:14:46:00 [Epoch 3 Batch 8000/13130] loss=1.0161, lr=0.0000212, metrics:accuracy:0.4848
INFO:root:14:46:05 [Batch 1200/3750] loss=1.0162, metrics:accuracy:0.5391
INFO:root:14:46:07 [Epoch 4 Batch 7200/13130] loss=0.9840, lr=0.0000032, metrics:accuracy:0.5041
INFO:root:14:46:14 [Epoch 2 Batch 12400/13130] loss=1.0396, lr=0.0000136, metrics:accuracy:0.4584
INFO:root:14:46:26 [Batch 1600/3750] loss=1.0899, metrics:accuracy:0.4998
INFO:root:14:46:26 [Epoch 3 Batch 800/13130] loss=1.0829, lr=0.0000131, metrics:accuracy:0.3758
INFO:root:14:46:29 [Epoch 4 Batch 7600/13130] loss=0.9981, lr=0.0000032, metrics:accuracy:0.5036
INFO:root:14:46:30 [Epoch 3 Batch 8400/13130] loss=1.0038, lr=0.0000210, metrics:accuracy:0.4851
INFO:root:14:46:47 [Batch 2000/3750] loss=1.0943, metrics:accuracy:0.4731
INFO:root:14:46:50 [Epoch 2 Batch 12800/13130] loss=1.0171, lr=0.0000134, metrics:accuracy:0.4594
INFO:root:14:46:51 [Epoch 4 Batch 8000/13130] loss=0.9852, lr=0.0000031, metrics:accuracy:0.5037
INFO:root:14:47:01 [Epoch 3 Batch 8800/13130] loss=1.0063, lr=0.0000207, metrics:accuracy:0.4857
INFO:root:14:47:06 [Epoch 3 Batch 1200/13130] loss=1.0859, lr=0.0000129, metrics:accuracy:0.3820
INFO:root:14:47:07 [Batch 2400/3750] loss=1.0813, metrics:accuracy:0.4584
INFO:root:14:47:12 [Epoch 4 Batch 8400/13130] loss=0.9841, lr=0.0000030, metrics:accuracy:0.5043
INFO:root:14:47:19 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:47:28 [Batch 2800/3750] loss=1.0161, metrics:accuracy:0.4592
INFO:root:14:47:31 [Epoch 3 Batch 9200/13130] loss=1.0194, lr=0.0000204, metrics:accuracy:0.4852
INFO:root:14:47:34 [Epoch 4 Batch 8800/13130] loss=0.9761, lr=0.0000030, metrics:accuracy:0.5045
INFO:root:14:47:39 [Batch 400/3750] loss=1.0040, metrics:accuracy:0.5353
INFO:root:14:47:48 [Epoch 3 Batch 1600/13130] loss=1.0876, lr=0.0000128, metrics:accuracy:0.3841
INFO:root:14:47:49 [Batch 3200/3750] loss=0.9750, metrics:accuracy:0.4646
INFO:root:14:47:55 [Epoch 4 Batch 9200/13130] loss=0.9890, lr=0.0000029, metrics:accuracy:0.5045
INFO:root:14:48:00 [Batch 800/3750] loss=1.0078, metrics:accuracy:0.5348
INFO:root:14:48:01 [Epoch 3 Batch 9600/13130] loss=1.0086, lr=0.0000202, metrics:accuracy:0.4857
INFO:root:14:48:10 [Batch 3600/3750] loss=0.9797, metrics:accuracy:0.4679
INFO:root:14:48:17 [Epoch 4 Batch 9600/13130] loss=0.9965, lr=0.0000028, metrics:accuracy:0.5039
INFO:root:14:48:17 validation metrics:accuracy:0.4698
INFO:root:14:48:17 Time cost=194.90s, throughput=153.92 samples/s
INFO:root:14:48:19 [Batch 1200/3750] loss=1.0044, metrics:accuracy:0.5331
INFO:root:14:48:19 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:14:48:19 Time cost=1464.37s
INFO:root:14:48:29 [Epoch 3 Batch 2000/13130] loss=1.0806, lr=0.0000127, metrics:accuracy:0.3869
INFO:root:14:48:32 [Epoch 3 Batch 10000/13130] loss=1.0028, lr=0.0000199, metrics:accuracy:0.4859
INFO:root:14:48:39 [Epoch 4 Batch 10000/13130] loss=0.9936, lr=0.0000027, metrics:accuracy:0.5036
INFO:root:14:48:40 [Batch 1600/3750] loss=1.1331, metrics:accuracy:0.4856
INFO:root:14:48:55 [Epoch 3 Batch 400/13130] loss=1.0211, lr=0.0000053, metrics:accuracy:0.4703
INFO:root:14:48:59 AMP: increasing loss scale to 32768.000000
INFO:root:14:49:00 [Batch 2000/3750] loss=1.1448, metrics:accuracy:0.4541
INFO:root:14:49:01 [Epoch 4 Batch 10400/13130] loss=0.9777, lr=0.0000027, metrics:accuracy:0.5036
INFO:root:14:49:02 [Epoch 3 Batch 10400/13130] loss=1.0065, lr=0.0000196, metrics:accuracy:0.4863
INFO:root:14:49:07 AMP: decreasing loss scale to 16384.000000
INFO:root:14:49:10 [Epoch 3 Batch 2400/13130] loss=1.0873, lr=0.0000125, metrics:accuracy:0.3868
INFO:root:14:49:21 [Batch 2400/3750] loss=1.1318, metrics:accuracy:0.4344
INFO:root:14:49:23 [Epoch 4 Batch 10800/13130] loss=0.9846, lr=0.0000026, metrics:accuracy:0.5036
INFO:root:14:49:29 [Epoch 3 Batch 800/13130] loss=1.0212, lr=0.0000052, metrics:accuracy:0.4733
INFO:root:14:49:33 [Epoch 3 Batch 10800/13130] loss=1.0112, lr=0.0000193, metrics:accuracy:0.4863
INFO:root:14:49:42 [Batch 2800/3750] loss=1.0089, metrics:accuracy:0.4416
INFO:root:14:49:45 [Epoch 4 Batch 11200/13130] loss=0.9865, lr=0.0000025, metrics:accuracy:0.5040
INFO:root:14:49:51 [Epoch 3 Batch 2800/13130] loss=1.0887, lr=0.0000124, metrics:accuracy:0.3848
INFO:root:14:50:02 [Epoch 3 Batch 11200/13130] loss=1.0143, lr=0.0000191, metrics:accuracy:0.4864
INFO:root:14:50:03 [Batch 3200/3750] loss=0.9488, metrics:accuracy:0.4546
INFO:root:14:50:04 [Epoch 3 Batch 1200/13130] loss=1.0284, lr=0.0000052, metrics:accuracy:0.4751
INFO:root:14:50:07 [Epoch 4 Batch 11600/13130] loss=0.9829, lr=0.0000025, metrics:accuracy:0.5040
INFO:root:14:50:24 [Batch 3600/3750] loss=0.9517, metrics:accuracy:0.4639
INFO:root:14:50:29 [Epoch 4 Batch 12000/13130] loss=0.9920, lr=0.0000024, metrics:accuracy:0.5038
INFO:root:14:50:32 [Epoch 3 Batch 3200/13130] loss=1.0831, lr=0.0000122, metrics:accuracy:0.3860
INFO:root:14:50:32 validation metrics:accuracy:0.4668
INFO:root:14:50:32 Time cost=193.30s, throughput=155.20 samples/s
INFO:root:14:50:33 [Epoch 3 Batch 11600/13130] loss=1.0081, lr=0.0000188, metrics:accuracy:0.4866
INFO:root:14:50:34 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:14:50:34 Time cost=1464.43s
INFO:root:14:50:39 [Epoch 3 Batch 1600/13130] loss=1.0188, lr=0.0000051, metrics:accuracy:0.4747
INFO:root:14:50:50 [Epoch 4 Batch 12400/13130] loss=0.9813, lr=0.0000023, metrics:accuracy:0.5043
INFO:root:14:51:04 [Epoch 3 Batch 12000/13130] loss=0.9981, lr=0.0000185, metrics:accuracy:0.4874
INFO:root:14:51:12 [Epoch 4 Batch 12800/13130] loss=0.9882, lr=0.0000023, metrics:accuracy:0.5044
INFO:root:14:51:12 [Epoch 3 Batch 3600/13130] loss=1.0890, lr=0.0000121, metrics:accuracy:0.3862
INFO:root:14:51:14 [Epoch 3 Batch 400/13130] loss=1.0111, lr=0.0000132, metrics:accuracy:0.4691
INFO:root:14:51:18 [Epoch 3 Batch 2000/13130] loss=1.0205, lr=0.0000051, metrics:accuracy:0.4753
INFO:root:14:51:30 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:51:34 [Epoch 3 Batch 12400/13130] loss=0.9957, lr=0.0000183, metrics:accuracy:0.4882
INFO:root:14:51:40 [Batch 400/3750] loss=1.0279, metrics:accuracy:0.5444
INFO:root:14:51:51 [Batch 800/3750] loss=1.0448, metrics:accuracy:0.5437
INFO:root:14:51:52 [Epoch 3 Batch 800/13130] loss=1.0127, lr=0.0000131, metrics:accuracy:0.4779
INFO:root:14:51:52 [Epoch 3 Batch 4000/13130] loss=1.0841, lr=0.0000120, metrics:accuracy:0.3863
INFO:root:14:51:57 [Epoch 3 Batch 2400/13130] loss=1.0296, lr=0.0000050, metrics:accuracy:0.4746
INFO:root:14:52:02 [Batch 1200/3750] loss=1.0354, metrics:accuracy:0.5401
INFO:root:14:52:05 [Epoch 3 Batch 12800/13130] loss=0.9829, lr=0.0000180, metrics:accuracy:0.4888
INFO:root:14:52:13 [Batch 1600/3750] loss=1.0796, metrics:accuracy:0.5086
INFO:root:14:52:24 [Batch 2000/3750] loss=1.0666, metrics:accuracy:0.4891
INFO:root:14:52:30 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:52:31 [Epoch 3 Batch 1200/13130] loss=1.0190, lr=0.0000129, metrics:accuracy:0.4801
INFO:root:14:52:33 [Epoch 3 Batch 4400/13130] loss=1.0925, lr=0.0000118, metrics:accuracy:0.3855
INFO:root:14:52:35 [Batch 2400/3750] loss=1.0568, metrics:accuracy:0.4782
INFO:root:14:52:37 [Epoch 3 Batch 2800/13130] loss=1.0178, lr=0.0000050, metrics:accuracy:0.4766
INFO:root:14:52:46 [Batch 2800/3750] loss=1.0104, metrics:accuracy:0.4756
INFO:root:14:52:46 [Batch 400/3750] loss=0.9708, metrics:accuracy:0.5925
INFO:root:14:52:57 [Batch 3200/3750] loss=0.9765, metrics:accuracy:0.4765
INFO:root:14:53:03 [Batch 800/3750] loss=0.9842, metrics:accuracy:0.5875
INFO:root:14:53:06 [Epoch 3 Batch 4800/13130] loss=1.0835, lr=0.0000117, metrics:accuracy:0.3862
INFO:root:14:53:08 [Batch 3600/3750] loss=0.9820, metrics:accuracy:0.4763
INFO:root:14:53:11 [Epoch 3 Batch 1600/13130] loss=1.0073, lr=0.0000128, metrics:accuracy:0.4806
INFO:root:14:53:12 validation metrics:accuracy:0.4772
INFO:root:14:53:12 Time cost=102.05s, throughput=293.98 samples/s
INFO:root:14:53:13 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:14:53:13 Time cost=816.26s
INFO:root:14:53:16 [Epoch 3 Batch 3200/13130] loss=1.0121, lr=0.0000049, metrics:accuracy:0.4783
INFO:root:14:53:19 [Batch 1200/3750] loss=0.9796, metrics:accuracy:0.5855
INFO:root:14:53:35 [Epoch 5 Batch 400/13130] loss=0.9647, lr=0.0000022, metrics:accuracy:0.5219
INFO:root:14:53:35 [Batch 1600/3750] loss=1.1310, metrics:accuracy:0.5302
INFO:root:14:53:39 [Epoch 3 Batch 5200/13130] loss=1.0926, lr=0.0000116, metrics:accuracy:0.3859
INFO:root:14:53:50 [Epoch 3 Batch 2000/13130] loss=1.0080, lr=0.0000127, metrics:accuracy:0.4816
INFO:root:14:53:52 [Batch 2000/3750] loss=1.1465, metrics:accuracy:0.4934
INFO:root:14:53:55 [Epoch 3 Batch 3600/13130] loss=1.0189, lr=0.0000048, metrics:accuracy:0.4783
INFO:root:14:53:57 [Epoch 5 Batch 800/13130] loss=0.9844, lr=0.0000021, metrics:accuracy:0.5160
INFO:root:14:54:08 [Batch 2400/3750] loss=1.1414, metrics:accuracy:0.4696
INFO:root:14:54:11 AMP: decreasing loss scale to 8192.000000
INFO:root:14:54:12 [Epoch 3 Batch 5600/13130] loss=1.0854, lr=0.0000114, metrics:accuracy:0.3862
INFO:root:14:54:19 [Epoch 5 Batch 1200/13130] loss=0.9498, lr=0.0000020, metrics:accuracy:0.5214
INFO:root:14:54:25 [Batch 2800/3750] loss=1.0103, metrics:accuracy:0.4671
INFO:root:14:54:29 [Epoch 3 Batch 2400/13130] loss=1.0271, lr=0.0000125, metrics:accuracy:0.4809
INFO:root:14:54:34 [Epoch 3 Batch 4000/13130] loss=1.0153, lr=0.0000048, metrics:accuracy:0.4787
INFO:root:14:54:40 [Epoch 5 Batch 1600/13130] loss=0.9642, lr=0.0000019, metrics:accuracy:0.5212
INFO:root:14:54:41 [Batch 3200/3750] loss=0.9621, metrics:accuracy:0.4698
INFO:root:14:54:46 [Epoch 3 Batch 6000/13130] loss=1.0892, lr=0.0000113, metrics:accuracy:0.3856
INFO:root:14:54:57 [Batch 3600/3750] loss=0.9611, metrics:accuracy:0.4728
INFO:root:14:55:02 [Epoch 5 Batch 2000/13130] loss=0.9806, lr=0.0000019, metrics:accuracy:0.5171
INFO:root:14:55:03 validation metrics:accuracy:0.4735
INFO:root:14:55:03 Time cost=153.72s, throughput=195.17 samples/s
INFO:root:14:55:05 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:14:55:05 Time cost=1137.44s
INFO:root:14:55:08 [Epoch 3 Batch 2800/13130] loss=1.0048, lr=0.0000124, metrics:accuracy:0.4835
INFO:root:14:55:13 [Epoch 3 Batch 4400/13130] loss=1.0312, lr=0.0000047, metrics:accuracy:0.4780
INFO:root:14:55:21 [Epoch 3 Batch 6400/13130] loss=1.0871, lr=0.0000112, metrics:accuracy:0.3852
INFO:root:14:55:23 [Epoch 5 Batch 2400/13130] loss=0.9762, lr=0.0000018, metrics:accuracy:0.5179
INFO:root:14:55:35 [Epoch 4 Batch 400/13130] loss=0.9754, lr=0.0000175, metrics:accuracy:0.5311
INFO:root:14:55:45 [Epoch 5 Batch 2800/13130] loss=0.9673, lr=0.0000017, metrics:accuracy:0.5175
INFO:root:14:55:48 [Epoch 3 Batch 3200/13130] loss=1.0014, lr=0.0000122, metrics:accuracy:0.4859
INFO:root:14:55:53 [Epoch 3 Batch 4800/13130] loss=1.0137, lr=0.0000047, metrics:accuracy:0.4787
INFO:root:14:56:01 [Epoch 3 Batch 6800/13130] loss=1.0811, lr=0.0000110, metrics:accuracy:0.3858
INFO:root:14:56:05 [Epoch 4 Batch 800/13130] loss=0.9854, lr=0.0000172, metrics:accuracy:0.5157
INFO:root:14:56:06 [Epoch 5 Batch 3200/13130] loss=0.9563, lr=0.0000017, metrics:accuracy:0.5184
INFO:root:14:56:27 [Epoch 3 Batch 3600/13130] loss=1.0052, lr=0.0000121, metrics:accuracy:0.4850
INFO:root:14:56:28 [Epoch 5 Batch 3600/13130] loss=0.9692, lr=0.0000016, metrics:accuracy:0.5180
INFO:root:14:56:32 [Epoch 3 Batch 5200/13130] loss=1.0341, lr=0.0000046, metrics:accuracy:0.4779
INFO:root:14:56:36 [Epoch 4 Batch 1200/13130] loss=0.9700, lr=0.0000170, metrics:accuracy:0.5186
INFO:root:14:56:43 [Epoch 3 Batch 7200/13130] loss=1.0823, lr=0.0000109, metrics:accuracy:0.3860
INFO:root:14:56:50 [Epoch 5 Batch 4000/13130] loss=0.9715, lr=0.0000015, metrics:accuracy:0.5176
INFO:root:14:57:07 [Epoch 3 Batch 4000/13130] loss=1.0063, lr=0.0000120, metrics:accuracy:0.4863
INFO:root:14:57:07 [Epoch 4 Batch 1600/13130] loss=0.9677, lr=0.0000167, metrics:accuracy:0.5182
INFO:root:14:57:11 [Epoch 5 Batch 4400/13130] loss=0.9489, lr=0.0000015, metrics:accuracy:0.5190
INFO:root:14:57:12 [Epoch 3 Batch 5600/13130] loss=1.0138, lr=0.0000046, metrics:accuracy:0.4779
INFO:root:14:57:24 [Epoch 3 Batch 7600/13130] loss=1.0892, lr=0.0000108, metrics:accuracy:0.3859
INFO:root:14:57:32 [Epoch 5 Batch 4800/13130] loss=0.9546, lr=0.0000014, metrics:accuracy:0.5197
INFO:root:14:57:38 [Epoch 4 Batch 2000/13130] loss=0.9538, lr=0.0000164, metrics:accuracy:0.5197
INFO:root:14:57:46 [Epoch 3 Batch 4400/13130] loss=1.0226, lr=0.0000118, metrics:accuracy:0.4849
INFO:root:14:57:51 [Epoch 3 Batch 6000/13130] loss=1.0221, lr=0.0000045, metrics:accuracy:0.4781
INFO:root:14:57:54 [Epoch 5 Batch 5200/13130] loss=0.9577, lr=0.0000013, metrics:accuracy:0.5208
INFO:root:14:58:05 [Epoch 3 Batch 8000/13130] loss=1.0887, lr=0.0000106, metrics:accuracy:0.3860
INFO:root:14:58:09 [Epoch 4 Batch 2400/13130] loss=0.9745, lr=0.0000161, metrics:accuracy:0.5188
INFO:root:14:58:16 [Epoch 5 Batch 5600/13130] loss=0.9729, lr=0.0000013, metrics:accuracy:0.5204
INFO:root:14:58:26 [Epoch 3 Batch 4800/13130] loss=1.0126, lr=0.0000117, metrics:accuracy:0.4856
INFO:root:14:58:31 [Epoch 3 Batch 6400/13130] loss=1.0334, lr=0.0000045, metrics:accuracy:0.4770
INFO:root:14:58:37 [Epoch 5 Batch 6000/13130] loss=0.9717, lr=0.0000012, metrics:accuracy:0.5207
INFO:root:14:58:40 [Epoch 4 Batch 2800/13130] loss=0.9631, lr=0.0000159, metrics:accuracy:0.5204
INFO:root:14:58:45 [Epoch 3 Batch 8400/13130] loss=1.0838, lr=0.0000105, metrics:accuracy:0.3865
INFO:root:14:58:59 [Epoch 5 Batch 6400/13130] loss=0.9580, lr=0.0000011, metrics:accuracy:0.5217
INFO:root:14:59:05 [Epoch 3 Batch 5200/13130] loss=1.0303, lr=0.0000116, metrics:accuracy:0.4842
INFO:root:14:59:10 [Epoch 4 Batch 3200/13130] loss=0.9693, lr=0.0000156, metrics:accuracy:0.5200
INFO:root:14:59:10 [Epoch 3 Batch 6800/13130] loss=1.0078, lr=0.0000044, metrics:accuracy:0.4778
INFO:root:14:59:20 [Epoch 5 Batch 6800/13130] loss=0.9575, lr=0.0000011, metrics:accuracy:0.5226
INFO:root:14:59:26 [Epoch 3 Batch 8800/13130] loss=1.0871, lr=0.0000104, metrics:accuracy:0.3864
INFO:root:14:59:40 [Epoch 4 Batch 3600/13130] loss=0.9684, lr=0.0000153, metrics:accuracy:0.5193
INFO:root:14:59:42 [Epoch 5 Batch 7200/13130] loss=0.9656, lr=0.0000010, metrics:accuracy:0.5223
INFO:root:14:59:45 [Epoch 3 Batch 5600/13130] loss=1.0062, lr=0.0000114, metrics:accuracy:0.4846
INFO:root:14:59:50 [Epoch 3 Batch 7200/13130] loss=1.0342, lr=0.0000044, metrics:accuracy:0.4776
INFO:root:15:00:03 [Epoch 5 Batch 7600/13130] loss=0.9671, lr=0.0000009, metrics:accuracy:0.5225
INFO:root:15:00:06 [Epoch 3 Batch 9200/13130] loss=1.0858, lr=0.0000102, metrics:accuracy:0.3859
INFO:root:15:00:12 [Epoch 4 Batch 4000/13130] loss=0.9633, lr=0.0000151, metrics:accuracy:0.5195
INFO:root:15:00:23 [Epoch 3 Batch 6000/13130] loss=1.0130, lr=0.0000113, metrics:accuracy:0.4848
INFO:root:15:00:24 [Epoch 5 Batch 8000/13130] loss=0.9682, lr=0.0000009, metrics:accuracy:0.5224
INFO:root:15:00:29 [Epoch 3 Batch 7600/13130] loss=1.0259, lr=0.0000043, metrics:accuracy:0.4772
INFO:root:15:00:42 [Epoch 4 Batch 4400/13130] loss=0.9632, lr=0.0000148, metrics:accuracy:0.5205
INFO:root:15:00:45 [Epoch 5 Batch 8400/13130] loss=0.9486, lr=0.0000008, metrics:accuracy:0.5237
INFO:root:15:00:47 AMP: increasing loss scale to 16384.000000
INFO:root:15:00:48 [Epoch 3 Batch 9600/13130] loss=1.0858, lr=0.0000101, metrics:accuracy:0.3857
INFO:root:15:01:02 [Epoch 3 Batch 6400/13130] loss=1.0204, lr=0.0000112, metrics:accuracy:0.4843
INFO:root:15:01:07 [Epoch 5 Batch 8800/13130] loss=0.9709, lr=0.0000007, metrics:accuracy:0.5232
INFO:root:15:01:08 [Epoch 3 Batch 8000/13130] loss=1.0217, lr=0.0000042, metrics:accuracy:0.4774
INFO:root:15:01:12 [Epoch 4 Batch 4800/13130] loss=0.9733, lr=0.0000145, metrics:accuracy:0.5204
INFO:root:15:01:28 [Epoch 3 Batch 10000/13130] loss=1.0862, lr=0.0000099, metrics:accuracy:0.3856
INFO:root:15:01:28 [Epoch 5 Batch 9200/13130] loss=0.9603, lr=0.0000007, metrics:accuracy:0.5233
INFO:root:15:01:42 [Epoch 3 Batch 6800/13130] loss=0.9943, lr=0.0000110, metrics:accuracy:0.4852
INFO:root:15:01:42 [Epoch 4 Batch 5200/13130] loss=0.9889, lr=0.0000142, metrics:accuracy:0.5195
INFO:root:15:01:47 [Epoch 3 Batch 8400/13130] loss=1.0109, lr=0.0000042, metrics:accuracy:0.4783
INFO:root:15:01:50 [Epoch 5 Batch 9600/13130] loss=0.9479, lr=0.0000006, metrics:accuracy:0.5241
INFO:root:15:02:10 [Epoch 3 Batch 10400/13130] loss=1.0840, lr=0.0000098, metrics:accuracy:0.3859
INFO:root:15:02:12 [Epoch 5 Batch 10000/13130] loss=0.9603, lr=0.0000005, metrics:accuracy:0.5240
INFO:root:15:02:13 [Epoch 4 Batch 5600/13130] loss=0.9734, lr=0.0000140, metrics:accuracy:0.5203
INFO:root:15:02:21 [Epoch 3 Batch 7200/13130] loss=1.0277, lr=0.0000109, metrics:accuracy:0.4847
INFO:root:15:02:26 [Epoch 3 Batch 8800/13130] loss=1.0121, lr=0.0000041, metrics:accuracy:0.4786
INFO:root:15:02:33 [Epoch 5 Batch 10400/13130] loss=0.9659, lr=0.0000005, metrics:accuracy:0.5241
INFO:root:15:02:44 [Epoch 4 Batch 6000/13130] loss=0.9750, lr=0.0000137, metrics:accuracy:0.5204
INFO:root:15:02:50 [Epoch 3 Batch 10800/13130] loss=1.0848, lr=0.0000097, metrics:accuracy:0.3865
INFO:root:15:02:55 [Epoch 5 Batch 10800/13130] loss=0.9685, lr=0.0000004, metrics:accuracy:0.5240
INFO:root:15:02:59 [Epoch 3 Batch 7600/13130] loss=1.0140, lr=0.0000108, metrics:accuracy:0.4845
INFO:root:15:03:05 [Epoch 3 Batch 9200/13130] loss=1.0315, lr=0.0000041, metrics:accuracy:0.4779
INFO:root:15:03:14 [Epoch 4 Batch 6400/13130] loss=0.9781, lr=0.0000134, metrics:accuracy:0.5204
INFO:root:15:03:16 [Epoch 5 Batch 11200/13130] loss=0.9637, lr=0.0000003, metrics:accuracy:0.5242
INFO:root:15:03:31 [Epoch 3 Batch 11200/13130] loss=1.0829, lr=0.0000095, metrics:accuracy:0.3864
INFO:root:15:03:38 [Epoch 5 Batch 11600/13130] loss=0.9714, lr=0.0000003, metrics:accuracy:0.5237
INFO:root:15:03:39 [Epoch 3 Batch 8000/13130] loss=1.0118, lr=0.0000106, metrics:accuracy:0.4847
INFO:root:15:03:44 [Epoch 4 Batch 6800/13130] loss=0.9696, lr=0.0000132, metrics:accuracy:0.5205
INFO:root:15:03:44 [Epoch 3 Batch 9600/13130] loss=1.0178, lr=0.0000040, metrics:accuracy:0.4781
INFO:root:15:03:59 [Epoch 5 Batch 12000/13130] loss=0.9641, lr=0.0000002, metrics:accuracy:0.5236
INFO:root:15:04:12 [Epoch 3 Batch 11600/13130] loss=1.0861, lr=0.0000094, metrics:accuracy:0.3862
INFO:root:15:04:14 [Epoch 4 Batch 7200/13130] loss=0.9591, lr=0.0000129, metrics:accuracy:0.5204
INFO:root:15:04:18 [Epoch 3 Batch 8400/13130] loss=0.9999, lr=0.0000105, metrics:accuracy:0.4852
INFO:root:15:04:21 [Epoch 5 Batch 12400/13130] loss=0.9545, lr=0.0000001, metrics:accuracy:0.5239
INFO:root:15:04:23 [Epoch 3 Batch 10000/13130] loss=1.0146, lr=0.0000040, metrics:accuracy:0.4782
INFO:root:15:04:43 [Epoch 5 Batch 12800/13130] loss=0.9689, lr=0.0000001, metrics:accuracy:0.5238
INFO:root:15:04:44 [Epoch 4 Batch 7600/13130] loss=0.9744, lr=0.0000126, metrics:accuracy:0.5198
INFO:root:15:04:52 [Epoch 3 Batch 12000/13130] loss=1.0846, lr=0.0000093, metrics:accuracy:0.3864
INFO:root:15:04:57 [Epoch 3 Batch 8800/13130] loss=1.0056, lr=0.0000104, metrics:accuracy:0.4852
INFO:root:15:05:00 Finish training step: 32812
INFO:root:15:05:00 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:05:03 [Epoch 3 Batch 10400/13130] loss=1.0078, lr=0.0000039, metrics:accuracy:0.4784
INFO:root:15:05:11 [Batch 400/3750] loss=0.9745, metrics:accuracy:0.5837
INFO:root:15:05:14 [Epoch 4 Batch 8000/13130] loss=0.9553, lr=0.0000124, metrics:accuracy:0.5199
INFO:root:15:05:22 [Batch 800/3750] loss=0.9937, metrics:accuracy:0.5809
INFO:root:15:05:32 [Epoch 3 Batch 12400/13130] loss=1.0812, lr=0.0000091, metrics:accuracy:0.3869
INFO:root:15:05:33 [Batch 1200/3750] loss=0.9855, metrics:accuracy:0.5772
INFO:root:15:05:36 [Epoch 3 Batch 9200/13130] loss=1.0217, lr=0.0000102, metrics:accuracy:0.4847
INFO:root:15:05:42 [Epoch 3 Batch 10800/13130] loss=1.0179, lr=0.0000039, metrics:accuracy:0.4785
INFO:root:15:05:43 [Batch 1600/3750] loss=1.0810, metrics:accuracy:0.5394
INFO:root:15:05:44 [Epoch 4 Batch 8400/13130] loss=0.9595, lr=0.0000121, metrics:accuracy:0.5199
INFO:root:15:05:54 [Batch 2000/3750] loss=1.0790, metrics:accuracy:0.5151
INFO:root:15:06:05 [Batch 2400/3750] loss=1.0644, metrics:accuracy:0.5026
INFO:root:15:06:13 [Epoch 3 Batch 12800/13130] loss=1.0765, lr=0.0000090, metrics:accuracy:0.3876
INFO:root:15:06:14 [Epoch 3 Batch 9600/13130] loss=1.0102, lr=0.0000101, metrics:accuracy:0.4850
INFO:root:15:06:15 [Epoch 4 Batch 8800/13130] loss=0.9624, lr=0.0000118, metrics:accuracy:0.5204
INFO:root:15:06:16 [Batch 2800/3750] loss=1.0539, metrics:accuracy:0.4917
INFO:root:15:06:20 [Epoch 3 Batch 11200/13130] loss=1.0252, lr=0.0000038, metrics:accuracy:0.4783
INFO:root:15:06:27 [Batch 3200/3750] loss=1.0336, metrics:accuracy:0.4854
INFO:root:15:06:38 [Batch 3600/3750] loss=1.0393, metrics:accuracy:0.4793
INFO:root:15:06:42 validation metrics:accuracy:0.4778
INFO:root:15:06:42 Time cost=102.40s, throughput=292.97 samples/s
INFO:root:15:06:44 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:15:06:44 Time cost=810.57s
INFO:root:15:06:44 Best model at epoch 4. Validation metrics:accuracy:0.4778
INFO:root:15:06:44 Now we are doing testing on test with gpu(0).
INFO:root:15:06:45 [Epoch 4 Batch 9200/13130] loss=0.9674, lr=0.0000115, metrics:accuracy:0.5207
INFO:root:15:06:46 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:06:54 [Epoch 3 Batch 10000/13130] loss=1.0056, lr=0.0000099, metrics:accuracy:0.4850
INFO:root:15:07:00 [Epoch 3 Batch 11600/13130] loss=1.0147, lr=0.0000038, metrics:accuracy:0.4786
INFO:root:15:07:04 [Batch 400/3750] loss=1.0359, metrics:accuracy:0.5803
INFO:root:15:07:13 [Epoch 4 Batch 9600/13130] loss=0.9772, lr=0.0000113, metrics:accuracy:0.5203
INFO:root:15:07:22 [Batch 800/3750] loss=1.0409, metrics:accuracy:0.5745
INFO:root:15:07:34 [Epoch 3 Batch 10400/13130] loss=0.9990, lr=0.0000098, metrics:accuracy:0.4855
INFO:root:15:07:35 Time cost=50.43s, throughput=297.47 samples/s
INFO:root:15:07:39 [Epoch 3 Batch 12000/13130] loss=1.0001, lr=0.0000037, metrics:accuracy:0.4792
INFO:root:15:07:40 [Batch 1200/3750] loss=1.0417, metrics:accuracy:0.5761
INFO:root:15:07:41 [Epoch 4 Batch 10000/13130] loss=0.9708, lr=0.0000110, metrics:accuracy:0.5203
INFO:root:15:07:58 [Batch 1600/3750] loss=1.0844, metrics:accuracy:0.5133
INFO:root:15:08:08 [Epoch 4 Batch 10400/13130] loss=0.9595, lr=0.0000107, metrics:accuracy:0.5209
INFO:root:15:08:13 [Epoch 3 Batch 10800/13130] loss=1.0091, lr=0.0000097, metrics:accuracy:0.4856
INFO:root:15:08:16 [Batch 2000/3750] loss=1.0832, metrics:accuracy:0.4703
INFO:root:15:08:18 [Epoch 3 Batch 12400/13130] loss=1.0033, lr=0.0000037, metrics:accuracy:0.4800
INFO:root:15:08:34 [Batch 2400/3750] loss=1.0735, metrics:accuracy:0.4442
INFO:root:15:08:36 [Epoch 4 Batch 10800/13130] loss=0.9689, lr=0.0000105, metrics:accuracy:0.5207
INFO:root:15:08:52 [Epoch 3 Batch 11200/13130] loss=1.0104, lr=0.0000095, metrics:accuracy:0.4857
INFO:root:15:08:53 [Batch 2800/3750] loss=1.1165, metrics:accuracy:0.4242
INFO:root:15:08:57 [Epoch 3 Batch 12800/13130] loss=0.9915, lr=0.0000036, metrics:accuracy:0.4811
INFO:root:15:09:04 [Epoch 4 Batch 11200/13130] loss=0.9565, lr=0.0000102, metrics:accuracy:0.5211
INFO:root:15:09:11 [Batch 3200/3750] loss=1.1154, metrics:accuracy:0.4123
INFO:root:15:09:29 [Batch 3600/3750] loss=1.1269, metrics:accuracy:0.4018
INFO:root:15:09:30 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:09:31 [Epoch 3 Batch 11600/13130] loss=1.0038, lr=0.0000094, metrics:accuracy:0.4859
INFO:root:15:09:32 [Epoch 4 Batch 11600/13130] loss=0.9603, lr=0.0000099, metrics:accuracy:0.5211
INFO:root:15:09:36 validation metrics:accuracy:0.3990
INFO:root:15:09:36 Time cost=170.21s, throughput=176.25 samples/s
INFO:root:15:09:38 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:15:09:38 Time cost=1472.98s
INFO:root:15:09:50 [Batch 400/3750] loss=1.0062, metrics:accuracy:0.5641
INFO:root:15:10:01 [Epoch 4 Batch 12000/13130] loss=0.9716, lr=0.0000096, metrics:accuracy:0.5213
INFO:root:15:10:06 [Epoch 3 Batch 12000/13130] loss=0.9970, lr=0.0000093, metrics:accuracy:0.4864
INFO:root:15:10:11 [Batch 800/3750] loss=1.0158, metrics:accuracy:0.5622
INFO:root:15:10:18 [Epoch 4 Batch 400/13130] loss=1.0847, lr=0.0000087, metrics:accuracy:0.4008
INFO:root:15:10:24 AMP: increasing loss scale to 32768.000000
INFO:root:15:10:32 [Epoch 4 Batch 12400/13130] loss=0.9519, lr=0.0000094, metrics:accuracy:0.5215
INFO:root:15:10:32 [Batch 1200/3750] loss=1.0080, metrics:accuracy:0.5629
INFO:root:15:10:41 AMP: decreasing loss scale to 16384.000000
INFO:root:15:10:41 [Epoch 3 Batch 12400/13130] loss=0.9918, lr=0.0000091, metrics:accuracy:0.4869
INFO:root:15:10:53 [Batch 1600/3750] loss=1.1008, metrics:accuracy:0.5109
INFO:root:15:10:58 [Epoch 4 Batch 800/13130] loss=1.0899, lr=0.0000086, metrics:accuracy:0.3872
INFO:root:15:11:02 [Epoch 4 Batch 12800/13130] loss=0.9688, lr=0.0000091, metrics:accuracy:0.5217
INFO:root:15:11:13 [Batch 2000/3750] loss=1.1020, metrics:accuracy:0.4753
INFO:root:15:11:16 [Epoch 3 Batch 12800/13130] loss=0.9792, lr=0.0000090, metrics:accuracy:0.4879
INFO:root:15:11:28 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:11:34 [Batch 2400/3750] loss=1.0913, metrics:accuracy:0.4542
INFO:root:15:11:37 [Epoch 4 Batch 1200/13130] loss=1.0767, lr=0.0000085, metrics:accuracy:0.3923
INFO:root:15:11:44 [Batch 400/3750] loss=1.0235, metrics:accuracy:0.5513
INFO:root:15:11:45 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:11:55 [Batch 2800/3750] loss=1.0002, metrics:accuracy:0.4567
INFO:root:15:12:01 [Batch 800/3750] loss=1.0475, metrics:accuracy:0.5469
INFO:root:15:12:06 [Batch 400/3750] loss=0.9812, metrics:accuracy:0.5866
INFO:root:15:12:11 [Epoch 4 Batch 1600/13130] loss=1.0890, lr=0.0000083, metrics:accuracy:0.3905
INFO:root:15:12:16 [Batch 3200/3750] loss=0.9512, metrics:accuracy:0.4663
INFO:root:15:12:17 [Batch 1200/3750] loss=1.0379, metrics:accuracy:0.5449
INFO:root:15:12:27 [Batch 800/3750] loss=0.9888, metrics:accuracy:0.5833
INFO:root:15:12:34 [Batch 1600/3750] loss=1.0569, metrics:accuracy:0.5177
INFO:root:15:12:37 [Batch 3600/3750] loss=0.9554, metrics:accuracy:0.4726
INFO:root:15:12:44 [Epoch 4 Batch 2000/13130] loss=1.0789, lr=0.0000082, metrics:accuracy:0.3938
INFO:root:15:12:44 validation metrics:accuracy:0.4749
INFO:root:15:12:44 Time cost=194.72s, throughput=154.07 samples/s
INFO:root:15:12:46 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:15:12:46 Time cost=1466.70s
INFO:root:15:12:46 [Batch 1200/3750] loss=0.9819, metrics:accuracy:0.5842
INFO:root:15:12:50 [Batch 2000/3750] loss=1.0480, metrics:accuracy:0.5001
INFO:root:15:13:07 [Batch 1600/3750] loss=1.1361, metrics:accuracy:0.5279
INFO:root:15:13:07 [Batch 2400/3750] loss=1.0434, metrics:accuracy:0.4903
INFO:root:15:13:18 [Epoch 4 Batch 2400/13130] loss=1.0847, lr=0.0000081, metrics:accuracy:0.3938
INFO:root:15:13:21 [Epoch 4 Batch 400/13130] loss=0.9948, lr=0.0000035, metrics:accuracy:0.5105
INFO:root:15:13:24 [Batch 2800/3750] loss=1.0301, metrics:accuracy:0.4848
INFO:root:15:13:28 [Batch 2000/3750] loss=1.1435, metrics:accuracy:0.4897
INFO:root:15:13:40 [Batch 3200/3750] loss=1.0164, metrics:accuracy:0.4806
INFO:root:15:13:48 [Batch 2400/3750] loss=1.1378, metrics:accuracy:0.4636
INFO:root:15:13:51 [Epoch 4 Batch 2800/13130] loss=1.0810, lr=0.0000079, metrics:accuracy:0.3946
INFO:root:15:13:55 [Epoch 4 Batch 800/13130] loss=1.0010, lr=0.0000034, metrics:accuracy:0.5032
INFO:root:15:13:57 [Batch 3600/3750] loss=1.0186, metrics:accuracy:0.4770
INFO:root:15:14:03 validation metrics:accuracy:0.4757
INFO:root:15:14:03 Time cost=155.55s, throughput=192.86 samples/s
INFO:root:15:14:05 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:15:14:05 Time cost=1139.81s
INFO:root:15:14:09 [Batch 2800/3750] loss=1.0162, metrics:accuracy:0.4635
INFO:root:15:14:27 [Epoch 4 Batch 3200/13130] loss=1.0815, lr=0.0000078, metrics:accuracy:0.3939
INFO:root:15:14:31 [Batch 3200/3750] loss=0.9621, metrics:accuracy:0.4678
INFO:root:15:14:31 [Epoch 4 Batch 1200/13130] loss=1.0007, lr=0.0000034, metrics:accuracy:0.5008
INFO:root:15:14:36 [Epoch 5 Batch 400/13130] loss=0.9209, lr=0.0000086, metrics:accuracy:0.5564
INFO:root:15:14:51 [Batch 3600/3750] loss=0.9616, metrics:accuracy:0.4717
INFO:root:15:14:59 validation metrics:accuracy:0.4729
INFO:root:15:14:59 Time cost=194.36s, throughput=154.35 samples/s
INFO:root:15:15:01 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:15:15:01 Time cost=1467.32s
INFO:root:15:15:06 [Epoch 4 Batch 1600/13130] loss=0.9925, lr=0.0000033, metrics:accuracy:0.4996
INFO:root:15:15:07 [Epoch 5 Batch 800/13130] loss=0.9407, lr=0.0000083, metrics:accuracy:0.5465
INFO:root:15:15:08 [Epoch 4 Batch 3600/13130] loss=1.0856, lr=0.0000077, metrics:accuracy:0.3932
INFO:root:15:15:37 [Epoch 5 Batch 1200/13130] loss=0.8991, lr=0.0000081, metrics:accuracy:0.5523
INFO:root:15:15:41 [Epoch 4 Batch 400/13130] loss=0.9760, lr=0.0000087, metrics:accuracy:0.5192
INFO:root:15:15:46 [Epoch 4 Batch 2000/13130] loss=0.9838, lr=0.0000033, metrics:accuracy:0.5004
INFO:root:15:15:49 [Epoch 4 Batch 4000/13130] loss=1.0856, lr=0.0000075, metrics:accuracy:0.3935
INFO:root:15:16:07 [Epoch 5 Batch 1600/13130] loss=0.9153, lr=0.0000078, metrics:accuracy:0.5545
INFO:root:15:16:20 [Epoch 4 Batch 800/13130] loss=0.9784, lr=0.0000086, metrics:accuracy:0.5145
INFO:root:15:16:25 [Epoch 4 Batch 2400/13130] loss=0.9979, lr=0.0000032, metrics:accuracy:0.5008
INFO:root:15:16:29 [Epoch 4 Batch 4400/13130] loss=1.0850, lr=0.0000074, metrics:accuracy:0.3925
INFO:root:15:16:37 [Epoch 5 Batch 2000/13130] loss=0.9214, lr=0.0000075, metrics:accuracy:0.5538
INFO:root:15:16:53 AMP: increasing loss scale to 32768.000000
INFO:root:15:17:00 [Epoch 4 Batch 1200/13130] loss=0.9748, lr=0.0000085, metrics:accuracy:0.5172
INFO:root:15:17:05 [Epoch 4 Batch 2800/13130] loss=0.9929, lr=0.0000032, metrics:accuracy:0.5006
INFO:root:15:17:08 [Epoch 5 Batch 2400/13130] loss=0.9267, lr=0.0000073, metrics:accuracy:0.5523
INFO:root:15:17:10 [Epoch 4 Batch 4800/13130] loss=1.0851, lr=0.0000073, metrics:accuracy:0.3925
INFO:root:15:17:38 [Epoch 5 Batch 2800/13130] loss=0.9233, lr=0.0000070, metrics:accuracy:0.5503
INFO:root:15:17:39 [Epoch 4 Batch 1600/13130] loss=0.9696, lr=0.0000083, metrics:accuracy:0.5174
INFO:root:15:17:45 [Epoch 4 Batch 3200/13130] loss=0.9948, lr=0.0000031, metrics:accuracy:0.4993
INFO:root:15:17:51 [Epoch 4 Batch 5200/13130] loss=1.0848, lr=0.0000071, metrics:accuracy:0.3926
INFO:root:15:17:55 AMP: decreasing loss scale to 16384.000000
INFO:root:15:18:09 [Epoch 5 Batch 3200/13130] loss=0.9235, lr=0.0000067, metrics:accuracy:0.5499
INFO:root:15:18:19 [Epoch 4 Batch 2000/13130] loss=0.9564, lr=0.0000082, metrics:accuracy:0.5188
INFO:root:15:18:24 [Epoch 4 Batch 3600/13130] loss=0.9903, lr=0.0000031, metrics:accuracy:0.5002
INFO:root:15:18:31 [Epoch 4 Batch 5600/13130] loss=1.0847, lr=0.0000070, metrics:accuracy:0.3926
INFO:root:15:18:39 [Epoch 5 Batch 3600/13130] loss=0.9290, lr=0.0000064, metrics:accuracy:0.5495
INFO:root:15:18:59 [Epoch 4 Batch 2400/13130] loss=0.9754, lr=0.0000081, metrics:accuracy:0.5186
INFO:root:15:19:04 [Epoch 4 Batch 4000/13130] loss=0.9884, lr=0.0000030, metrics:accuracy:0.5016
INFO:root:15:19:09 [Epoch 5 Batch 4000/13130] loss=0.9313, lr=0.0000062, metrics:accuracy:0.5494
INFO:root:15:19:12 [Epoch 4 Batch 6000/13130] loss=1.0833, lr=0.0000069, metrics:accuracy:0.3925
INFO:root:15:19:38 [Epoch 4 Batch 2800/13130] loss=0.9643, lr=0.0000079, metrics:accuracy:0.5189
INFO:root:15:19:40 [Epoch 5 Batch 4400/13130] loss=0.9080, lr=0.0000059, metrics:accuracy:0.5506
INFO:root:15:19:44 [Epoch 4 Batch 4400/13130] loss=0.9925, lr=0.0000030, metrics:accuracy:0.5018
INFO:root:15:19:53 [Epoch 4 Batch 6400/13130] loss=1.0873, lr=0.0000067, metrics:accuracy:0.3926
INFO:root:15:20:10 [Epoch 5 Batch 4800/13130] loss=0.9086, lr=0.0000056, metrics:accuracy:0.5519
INFO:root:15:20:17 [Epoch 4 Batch 3200/13130] loss=0.9747, lr=0.0000078, metrics:accuracy:0.5187
INFO:root:15:20:23 [Epoch 4 Batch 4800/13130] loss=1.0004, lr=0.0000029, metrics:accuracy:0.5010
INFO:root:15:20:32 [Epoch 4 Batch 6800/13130] loss=1.0773, lr=0.0000066, metrics:accuracy:0.3934
INFO:root:15:20:40 [Epoch 5 Batch 5200/13130] loss=0.9022, lr=0.0000054, metrics:accuracy:0.5532
INFO:root:15:20:56 [Epoch 4 Batch 3600/13130] loss=0.9677, lr=0.0000077, metrics:accuracy:0.5180
INFO:root:15:21:02 [Epoch 4 Batch 5200/13130] loss=1.0038, lr=0.0000028, metrics:accuracy:0.5001
INFO:root:15:21:11 [Epoch 5 Batch 5600/13130] loss=0.9326, lr=0.0000051, metrics:accuracy:0.5528
INFO:root:15:21:13 [Epoch 4 Batch 7200/13130] loss=1.0825, lr=0.0000064, metrics:accuracy:0.3935
INFO:root:15:21:37 AMP: decreasing loss scale to 8192.000000
INFO:root:15:21:37 [Epoch 4 Batch 4000/13130] loss=0.9665, lr=0.0000075, metrics:accuracy:0.5182
INFO:root:15:21:41 [Epoch 5 Batch 6000/13130] loss=0.9157, lr=0.0000048, metrics:accuracy:0.5535
INFO:root:15:21:42 [Epoch 4 Batch 5600/13130] loss=1.0013, lr=0.0000028, metrics:accuracy:0.5005
INFO:root:15:21:53 [Epoch 4 Batch 7600/13130] loss=1.0834, lr=0.0000063, metrics:accuracy:0.3938
INFO:root:15:22:11 [Epoch 5 Batch 6400/13130] loss=0.9088, lr=0.0000045, metrics:accuracy:0.5535
INFO:root:15:22:16 [Epoch 4 Batch 4400/13130] loss=0.9660, lr=0.0000074, metrics:accuracy:0.5192
INFO:root:15:22:21 [Epoch 4 Batch 6000/13130] loss=1.0075, lr=0.0000027, metrics:accuracy:0.5006
INFO:root:15:22:34 [Epoch 4 Batch 8000/13130] loss=1.0851, lr=0.0000062, metrics:accuracy:0.3937
INFO:root:15:22:42 [Epoch 5 Batch 6800/13130] loss=0.9073, lr=0.0000043, metrics:accuracy:0.5546
INFO:root:15:22:55 [Epoch 4 Batch 4800/13130] loss=0.9739, lr=0.0000073, metrics:accuracy:0.5185
INFO:root:15:23:00 [Epoch 4 Batch 6400/13130] loss=1.0030, lr=0.0000027, metrics:accuracy:0.5003
INFO:root:15:23:12 [Epoch 5 Batch 7200/13130] loss=0.9052, lr=0.0000040, metrics:accuracy:0.5548
INFO:root:15:23:14 [Epoch 4 Batch 8400/13130] loss=1.0829, lr=0.0000060, metrics:accuracy:0.3935
INFO:root:15:23:34 [Epoch 4 Batch 5200/13130] loss=0.9862, lr=0.0000071, metrics:accuracy:0.5179
INFO:root:15:23:39 [Epoch 4 Batch 6800/13130] loss=0.9955, lr=0.0000026, metrics:accuracy:0.5006
INFO:root:15:23:42 [Epoch 5 Batch 7600/13130] loss=0.9299, lr=0.0000037, metrics:accuracy:0.5548
INFO:root:15:23:54 [Epoch 4 Batch 8800/13130] loss=1.0835, lr=0.0000059, metrics:accuracy:0.3939
INFO:root:15:24:12 [Epoch 5 Batch 8000/13130] loss=0.9133, lr=0.0000035, metrics:accuracy:0.5550
INFO:root:15:24:13 [Epoch 4 Batch 5600/13130] loss=0.9759, lr=0.0000070, metrics:accuracy:0.5180
INFO:root:15:24:19 [Epoch 4 Batch 7200/13130] loss=0.9902, lr=0.0000026, metrics:accuracy:0.5003
INFO:root:15:24:35 [Epoch 4 Batch 9200/13130] loss=1.0803, lr=0.0000058, metrics:accuracy:0.3944
INFO:root:15:24:42 [Epoch 5 Batch 8400/13130] loss=0.9069, lr=0.0000032, metrics:accuracy:0.5558
INFO:root:15:24:52 [Epoch 4 Batch 6000/13130] loss=0.9837, lr=0.0000069, metrics:accuracy:0.5174
INFO:root:15:24:57 [Epoch 4 Batch 7600/13130] loss=1.0050, lr=0.0000025, metrics:accuracy:0.5002
INFO:root:15:25:12 [Epoch 5 Batch 8800/13130] loss=0.9457, lr=0.0000029, metrics:accuracy:0.5549
INFO:root:15:25:15 [Epoch 4 Batch 9600/13130] loss=1.0806, lr=0.0000056, metrics:accuracy:0.3947
INFO:root:15:25:31 [Epoch 4 Batch 6400/13130] loss=0.9815, lr=0.0000067, metrics:accuracy:0.5168
INFO:root:15:25:36 [Epoch 4 Batch 8000/13130] loss=0.9885, lr=0.0000025, metrics:accuracy:0.5003
INFO:root:15:25:43 [Epoch 5 Batch 9200/13130] loss=0.9051, lr=0.0000026, metrics:accuracy:0.5554
INFO:root:15:25:56 [Epoch 4 Batch 10000/13130] loss=1.0842, lr=0.0000055, metrics:accuracy:0.3944
INFO:root:15:26:10 [Epoch 4 Batch 6800/13130] loss=0.9646, lr=0.0000066, metrics:accuracy:0.5174
INFO:root:15:26:13 [Epoch 5 Batch 9600/13130] loss=0.8996, lr=0.0000024, metrics:accuracy:0.5562
INFO:root:15:26:15 [Epoch 4 Batch 8400/13130] loss=0.9939, lr=0.0000024, metrics:accuracy:0.5007
INFO:root:15:26:36 [Epoch 4 Batch 10400/13130] loss=1.0787, lr=0.0000054, metrics:accuracy:0.3947
INFO:root:15:26:44 [Epoch 5 Batch 10000/13130] loss=0.9081, lr=0.0000021, metrics:accuracy:0.5564
INFO:root:15:26:49 [Epoch 4 Batch 7200/13130] loss=0.9636, lr=0.0000064, metrics:accuracy:0.5176
INFO:root:15:26:54 [Epoch 4 Batch 8800/13130] loss=0.9817, lr=0.0000024, metrics:accuracy:0.5009
INFO:root:15:27:14 [Epoch 5 Batch 10400/13130] loss=0.9162, lr=0.0000018, metrics:accuracy:0.5565
INFO:root:15:27:17 [Epoch 4 Batch 10800/13130] loss=1.0802, lr=0.0000052, metrics:accuracy:0.3944
INFO:root:15:27:27 [Epoch 4 Batch 7600/13130] loss=0.9783, lr=0.0000063, metrics:accuracy:0.5174
INFO:root:15:27:32 [Epoch 4 Batch 9200/13130] loss=0.9982, lr=0.0000023, metrics:accuracy:0.5008
INFO:root:15:27:44 [Epoch 5 Batch 10800/13130] loss=0.9149, lr=0.0000016, metrics:accuracy:0.5563
INFO:root:15:27:57 [Epoch 4 Batch 11200/13130] loss=1.0830, lr=0.0000051, metrics:accuracy:0.3943
INFO:root:15:28:06 [Epoch 4 Batch 8000/13130] loss=0.9635, lr=0.0000062, metrics:accuracy:0.5175
INFO:root:15:28:11 [Epoch 4 Batch 9600/13130] loss=1.0025, lr=0.0000023, metrics:accuracy:0.5001
INFO:root:15:28:15 [Epoch 5 Batch 11200/13130] loss=0.8993, lr=0.0000013, metrics:accuracy:0.5568
INFO:root:15:28:21 AMP: increasing loss scale to 16384.000000
INFO:root:15:28:38 [Epoch 4 Batch 11600/13130] loss=1.0860, lr=0.0000050, metrics:accuracy:0.3943
INFO:root:15:28:45 [Epoch 4 Batch 8400/13130] loss=0.9645, lr=0.0000060, metrics:accuracy:0.5179
INFO:root:15:28:45 [Epoch 5 Batch 11600/13130] loss=0.9274, lr=0.0000010, metrics:accuracy:0.5566
INFO:root:15:28:50 [Epoch 4 Batch 10000/13130] loss=1.0001, lr=0.0000022, metrics:accuracy:0.4998
INFO:root:15:29:16 [Epoch 5 Batch 12000/13130] loss=0.9139, lr=0.0000007, metrics:accuracy:0.5565
INFO:root:15:29:18 [Epoch 4 Batch 12000/13130] loss=1.0808, lr=0.0000048, metrics:accuracy:0.3942
INFO:root:15:29:24 [Epoch 4 Batch 8800/13130] loss=0.9652, lr=0.0000059, metrics:accuracy:0.5183
INFO:root:15:29:28 [Epoch 4 Batch 10400/13130] loss=0.9844, lr=0.0000021, metrics:accuracy:0.5000
INFO:root:15:29:46 [Epoch 5 Batch 12400/13130] loss=0.9082, lr=0.0000005, metrics:accuracy:0.5567
INFO:root:15:29:59 [Epoch 4 Batch 12400/13130] loss=1.0808, lr=0.0000047, metrics:accuracy:0.3946
INFO:root:15:30:03 [Epoch 4 Batch 9200/13130] loss=0.9660, lr=0.0000058, metrics:accuracy:0.5186
INFO:root:15:30:08 [Epoch 4 Batch 10800/13130] loss=0.9918, lr=0.0000021, metrics:accuracy:0.5002
INFO:root:15:30:17 [Epoch 5 Batch 12800/13130] loss=0.9132, lr=0.0000002, metrics:accuracy:0.5569
INFO:root:15:30:40 [Epoch 4 Batch 12800/13130] loss=1.0870, lr=0.0000045, metrics:accuracy:0.3941
INFO:root:15:30:41 Finish training step: 32812
INFO:root:15:30:41 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:30:41 [Epoch 4 Batch 9600/13130] loss=0.9741, lr=0.0000056, metrics:accuracy:0.5183
INFO:root:15:30:47 [Epoch 4 Batch 11200/13130] loss=0.9900, lr=0.0000020, metrics:accuracy:0.5007
INFO:root:15:30:57 [Batch 400/3750] loss=1.0260, metrics:accuracy:0.5750
INFO:root:15:31:07 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:31:13 [Batch 800/3750] loss=1.0601, metrics:accuracy:0.5669
INFO:root:15:31:20 [Epoch 4 Batch 10000/13130] loss=0.9738, lr=0.0000055, metrics:accuracy:0.5181
INFO:root:15:31:22 [Batch 400/3750] loss=1.0400, metrics:accuracy:0.5641
INFO:root:15:31:26 [Epoch 4 Batch 11600/13130] loss=0.9939, lr=0.0000020, metrics:accuracy:0.5007
INFO:root:15:31:28 [Batch 1200/3750] loss=1.0543, metrics:accuracy:0.5668
INFO:root:15:31:38 [Batch 800/3750] loss=1.0460, metrics:accuracy:0.5575
INFO:root:15:31:43 [Batch 1600/3750] loss=1.1034, metrics:accuracy:0.5334
INFO:root:15:31:53 [Batch 1200/3750] loss=1.0467, metrics:accuracy:0.5563
INFO:root:15:31:58 [Batch 2000/3750] loss=1.0940, metrics:accuracy:0.5124
INFO:root:15:31:59 [Epoch 4 Batch 10400/13130] loss=0.9546, lr=0.0000054, metrics:accuracy:0.5186
INFO:root:15:32:04 [Epoch 4 Batch 12000/13130] loss=0.9977, lr=0.0000019, metrics:accuracy:0.5005
INFO:root:15:32:08 [Batch 1600/3750] loss=1.0868, metrics:accuracy:0.4995
INFO:root:15:32:14 [Batch 2400/3750] loss=1.0905, metrics:accuracy:0.4995
INFO:root:15:32:24 [Batch 2000/3750] loss=1.0844, metrics:accuracy:0.4611
INFO:root:15:32:29 [Batch 2800/3750] loss=1.0829, metrics:accuracy:0.4908
INFO:root:15:32:38 [Epoch 4 Batch 10800/13130] loss=0.9712, lr=0.0000052, metrics:accuracy:0.5183
INFO:root:15:32:39 [Batch 2400/3750] loss=1.0748, metrics:accuracy:0.4383
INFO:root:15:32:44 [Epoch 4 Batch 12400/13130] loss=0.9873, lr=0.0000019, metrics:accuracy:0.5009
INFO:root:15:32:45 [Batch 3200/3750] loss=1.0737, metrics:accuracy:0.4834
INFO:root:15:32:55 [Batch 2800/3750] loss=1.1072, metrics:accuracy:0.4231
INFO:root:15:33:00 [Batch 3600/3750] loss=1.0755, metrics:accuracy:0.4777
INFO:root:15:33:06 validation metrics:accuracy:0.4755
INFO:root:15:33:06 Time cost=145.33s, throughput=206.43 samples/s
INFO:root:15:33:07 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:15:33:07 Time cost=1142.68s
INFO:root:15:33:08 Best model at epoch 3. Validation metrics:accuracy:0.4757
INFO:root:15:33:08 Now we are doing testing on test with gpu(0).
INFO:root:15:33:08 [Batch 3200/3750] loss=1.1031, metrics:accuracy:0.4146
INFO:root:15:33:18 [Epoch 4 Batch 11200/13130] loss=0.9633, lr=0.0000051, metrics:accuracy:0.5189
INFO:root:15:33:23 [Epoch 4 Batch 12800/13130] loss=0.9965, lr=0.0000018, metrics:accuracy:0.5008
INFO:root:15:33:24 [Batch 3600/3750] loss=1.1144, metrics:accuracy:0.4074
INFO:root:15:33:30 validation metrics:accuracy:0.4053
INFO:root:15:33:30 Time cost=142.96s, throughput=209.86 samples/s
INFO:root:15:33:32 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:15:33:32 Time cost=1434.34s
INFO:root:15:33:53 AMP: decreasing loss scale to 8192.000000
INFO:root:15:33:55 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:33:57 [Epoch 4 Batch 11600/13130] loss=0.9677, lr=0.0000050, metrics:accuracy:0.5186
INFO:root:15:34:08 [Epoch 5 Batch 400/13130] loss=1.0815, lr=0.0000043, metrics:accuracy:0.4033
INFO:root:15:34:16 [Batch 400/3750] loss=1.0476, metrics:accuracy:0.5441
INFO:root:15:34:19 Time cost=71.47s, throughput=209.87 samples/s
INFO:root:15:34:31 [Epoch 4 Batch 12000/13130] loss=0.9761, lr=0.0000048, metrics:accuracy:0.5184
INFO:root:15:34:32 [Epoch 5 Batch 800/13130] loss=1.0876, lr=0.0000042, metrics:accuracy:0.3916
INFO:root:15:34:36 [Batch 800/3750] loss=1.0615, metrics:accuracy:0.5398
INFO:root:15:34:49 [Epoch 5 Batch 1200/13130] loss=1.0835, lr=0.0000040, metrics:accuracy:0.3924
INFO:root:15:34:57 [Batch 1200/3750] loss=1.0468, metrics:accuracy:0.5382
INFO:root:15:35:05 [Epoch 4 Batch 12400/13130] loss=0.9651, lr=0.0000047, metrics:accuracy:0.5184
INFO:root:15:35:06 [Epoch 5 Batch 1600/13130] loss=1.0764, lr=0.0000039, metrics:accuracy:0.3952
INFO:root:15:35:18 [Batch 1600/3750] loss=1.0617, metrics:accuracy:0.5091
INFO:root:15:35:23 [Epoch 5 Batch 2000/13130] loss=1.0748, lr=0.0000038, metrics:accuracy:0.3973
INFO:root:15:35:39 [Batch 2000/3750] loss=1.0440, metrics:accuracy:0.4911
INFO:root:15:35:40 [Epoch 4 Batch 12800/13130] loss=0.9712, lr=0.0000045, metrics:accuracy:0.5183
INFO:root:15:35:40 [Epoch 5 Batch 2400/13130] loss=1.0800, lr=0.0000036, metrics:accuracy:0.4005
INFO:root:15:35:57 [Epoch 5 Batch 2800/13130] loss=1.0847, lr=0.0000035, metrics:accuracy:0.3996
INFO:root:15:36:00 [Batch 2400/3750] loss=1.0327, metrics:accuracy:0.4821
INFO:root:15:36:08 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:36:15 [Epoch 5 Batch 3200/13130] loss=1.0833, lr=0.0000034, metrics:accuracy:0.3989
INFO:root:15:36:21 [Batch 2800/3750] loss=1.0036, metrics:accuracy:0.4797
INFO:root:15:36:29 [Batch 400/3750] loss=1.0386, metrics:accuracy:0.5428
INFO:root:15:36:32 [Epoch 5 Batch 3600/13130] loss=1.0854, lr=0.0000032, metrics:accuracy:0.3977
INFO:root:15:36:41 [Batch 3200/3750] loss=0.9728, metrics:accuracy:0.4799
INFO:root:15:36:49 [Epoch 5 Batch 4000/13130] loss=1.0829, lr=0.0000031, metrics:accuracy:0.3974
INFO:root:15:36:49 [Batch 800/3750] loss=1.0519, metrics:accuracy:0.5445
INFO:root:15:36:59 AMP: increasing loss scale to 16384.000000
INFO:root:15:37:01 [Batch 3600/3750] loss=0.9795, metrics:accuracy:0.4791
INFO:root:15:37:06 [Epoch 5 Batch 4400/13130] loss=1.0802, lr=0.0000029, metrics:accuracy:0.3980
INFO:root:15:37:09 validation metrics:accuracy:0.4792
INFO:root:15:37:09 Time cost=193.68s, throughput=154.90 samples/s
INFO:root:15:37:09 [Batch 1200/3750] loss=1.0421, metrics:accuracy:0.5430
INFO:root:15:37:11 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:15:37:11 Time cost=1464.60s
INFO:root:15:37:23 [Epoch 5 Batch 4800/13130] loss=1.0827, lr=0.0000028, metrics:accuracy:0.3985
INFO:root:15:37:29 [Batch 1600/3750] loss=1.0671, metrics:accuracy:0.5166
INFO:root:15:37:41 [Epoch 5 Batch 5200/13130] loss=1.0802, lr=0.0000027, metrics:accuracy:0.3992
INFO:root:15:37:46 [Epoch 5 Batch 400/13130] loss=0.9756, lr=0.0000017, metrics:accuracy:0.5119
INFO:root:15:37:50 [Batch 2000/3750] loss=1.0492, metrics:accuracy:0.5009
INFO:root:15:37:58 [Epoch 5 Batch 5600/13130] loss=1.0832, lr=0.0000025, metrics:accuracy:0.3986
INFO:root:15:38:11 [Batch 2400/3750] loss=1.0486, metrics:accuracy:0.4904
INFO:root:15:38:15 [Epoch 5 Batch 6000/13130] loss=1.0856, lr=0.0000024, metrics:accuracy:0.3983
INFO:root:15:38:21 [Epoch 5 Batch 800/13130] loss=0.9956, lr=0.0000017, metrics:accuracy:0.5046
INFO:root:15:38:31 [Epoch 5 Batch 6400/13130] loss=1.0819, lr=0.0000023, metrics:accuracy:0.3985
INFO:root:15:38:32 [Batch 2800/3750] loss=1.0223, metrics:accuracy:0.4851
INFO:root:15:38:48 [Epoch 5 Batch 6800/13130] loss=1.0801, lr=0.0000021, metrics:accuracy:0.3987
INFO:root:15:38:52 [Batch 3200/3750] loss=1.0021, metrics:accuracy:0.4825
INFO:root:15:38:56 [Epoch 5 Batch 1200/13130] loss=0.9617, lr=0.0000016, metrics:accuracy:0.5106
INFO:root:15:39:05 [Epoch 5 Batch 7200/13130] loss=1.0840, lr=0.0000020, metrics:accuracy:0.3987
INFO:root:15:39:13 [Batch 3600/3750] loss=1.0043, metrics:accuracy:0.4797
INFO:root:15:39:20 validation metrics:accuracy:0.4789
INFO:root:15:39:20 Time cost=191.53s, throughput=156.63 samples/s
INFO:root:15:39:22 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:15:39:22 Time cost=1460.73s
INFO:root:15:39:23 [Epoch 5 Batch 7600/13130] loss=1.0789, lr=0.0000019, metrics:accuracy:0.3988
INFO:root:15:39:23 AMP: decreasing loss scale to 8192.000000
INFO:root:15:39:31 [Epoch 5 Batch 1600/13130] loss=0.9707, lr=0.0000016, metrics:accuracy:0.5095
INFO:root:15:39:40 [Epoch 5 Batch 8000/13130] loss=1.0847, lr=0.0000017, metrics:accuracy:0.3993
INFO:root:15:39:57 [Epoch 5 Batch 8400/13130] loss=1.0774, lr=0.0000016, metrics:accuracy:0.3996
INFO:root:15:40:01 [Epoch 5 Batch 400/13130] loss=0.9320, lr=0.0000043, metrics:accuracy:0.5517
INFO:root:15:40:10 [Epoch 5 Batch 2000/13130] loss=0.9860, lr=0.0000015, metrics:accuracy:0.5075
INFO:root:15:40:14 [Epoch 5 Batch 8800/13130] loss=1.0844, lr=0.0000015, metrics:accuracy:0.3993
INFO:root:15:40:32 [Epoch 5 Batch 9200/13130] loss=1.0790, lr=0.0000013, metrics:accuracy:0.3997
INFO:root:15:40:41 [Epoch 5 Batch 800/13130] loss=0.9521, lr=0.0000042, metrics:accuracy:0.5401
INFO:root:15:40:48 [Epoch 5 Batch 9600/13130] loss=1.0808, lr=0.0000012, metrics:accuracy:0.3999
INFO:root:15:40:50 [Epoch 5 Batch 2400/13130] loss=0.9866, lr=0.0000015, metrics:accuracy:0.5070
INFO:root:15:41:05 [Epoch 5 Batch 10000/13130] loss=1.0799, lr=0.0000011, metrics:accuracy:0.4000
INFO:root:15:41:20 [Epoch 5 Batch 1200/13130] loss=0.9129, lr=0.0000040, metrics:accuracy:0.5461
INFO:root:15:41:22 [Epoch 5 Batch 10400/13130] loss=1.0828, lr=0.0000009, metrics:accuracy:0.4002
INFO:root:15:41:29 [Epoch 5 Batch 2800/13130] loss=0.9824, lr=0.0000014, metrics:accuracy:0.5074
INFO:root:15:41:39 [Epoch 5 Batch 10800/13130] loss=1.0821, lr=0.0000008, metrics:accuracy:0.4001
INFO:root:15:41:55 [Epoch 5 Batch 11200/13130] loss=1.0787, lr=0.0000006, metrics:accuracy:0.4003
INFO:root:15:41:59 [Epoch 5 Batch 1600/13130] loss=0.9273, lr=0.0000039, metrics:accuracy:0.5457
INFO:root:15:42:08 [Epoch 5 Batch 3200/13130] loss=0.9707, lr=0.0000013, metrics:accuracy:0.5091
INFO:root:15:42:13 [Epoch 5 Batch 11600/13130] loss=1.0790, lr=0.0000005, metrics:accuracy:0.4009
INFO:root:15:42:13 AMP: increasing loss scale to 16384.000000
INFO:root:15:42:30 [Epoch 5 Batch 12000/13130] loss=1.0803, lr=0.0000004, metrics:accuracy:0.4010
INFO:root:15:42:38 [Epoch 5 Batch 2000/13130] loss=0.9422, lr=0.0000038, metrics:accuracy:0.5432
INFO:root:15:42:46 [Epoch 5 Batch 12400/13130] loss=1.0774, lr=0.0000002, metrics:accuracy:0.4012
INFO:root:15:42:47 [Epoch 5 Batch 3600/13130] loss=0.9808, lr=0.0000013, metrics:accuracy:0.5095
INFO:root:15:43:03 [Epoch 5 Batch 12800/13130] loss=1.0857, lr=0.0000001, metrics:accuracy:0.4013
INFO:root:15:43:16 Finish training step: 32812
INFO:root:15:43:16 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:43:17 [Epoch 5 Batch 2400/13130] loss=0.9468, lr=0.0000036, metrics:accuracy:0.5435
INFO:root:15:43:24 [Batch 400/3750] loss=1.0274, metrics:accuracy:0.5959
INFO:root:15:43:26 [Epoch 5 Batch 4000/13130] loss=0.9816, lr=0.0000012, metrics:accuracy:0.5091
INFO:root:15:43:31 [Batch 800/3750] loss=1.0339, metrics:accuracy:0.5848
INFO:root:15:43:39 [Batch 1200/3750] loss=1.0348, metrics:accuracy:0.5842
INFO:root:15:43:46 [Batch 1600/3750] loss=1.0889, metrics:accuracy:0.5199
INFO:root:15:43:54 [Batch 2000/3750] loss=1.0888, metrics:accuracy:0.4749
INFO:root:15:43:56 [Epoch 5 Batch 2800/13130] loss=0.9326, lr=0.0000035, metrics:accuracy:0.5419
INFO:root:15:44:01 [Batch 2400/3750] loss=1.0794, metrics:accuracy:0.4482
INFO:root:15:44:05 [Epoch 5 Batch 4400/13130] loss=0.9662, lr=0.0000012, metrics:accuracy:0.5101
INFO:root:15:44:08 [Batch 2800/3750] loss=1.1122, metrics:accuracy:0.4299
INFO:root:15:44:15 [Batch 3200/3750] loss=1.1089, metrics:accuracy:0.4187
INFO:root:15:44:23 [Batch 3600/3750] loss=1.1201, metrics:accuracy:0.4095
INFO:root:15:44:26 validation metrics:accuracy:0.4070
INFO:root:15:44:26 Time cost=69.68s, throughput=430.55 samples/s
INFO:root:15:44:27 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:15:44:27 Time cost=655.15s
INFO:root:15:44:27 Best model at epoch 4. Validation metrics:accuracy:0.4070
INFO:root:15:44:27 Now we are doing testing on test with gpu(0).
INFO:root:15:44:35 [Epoch 5 Batch 3200/13130] loss=0.9248, lr=0.0000034, metrics:accuracy:0.5430
INFO:root:15:44:45 [Epoch 5 Batch 4800/13130] loss=0.9694, lr=0.0000011, metrics:accuracy:0.5108
INFO:root:15:45:01 Time cost=33.77s, throughput=444.19 samples/s
INFO:root:15:45:13 [Epoch 5 Batch 3600/13130] loss=0.9457, lr=0.0000032, metrics:accuracy:0.5415
INFO:root:15:45:23 [Epoch 5 Batch 5200/13130] loss=0.9723, lr=0.0000011, metrics:accuracy:0.5117
INFO:root:15:45:53 [Epoch 5 Batch 4000/13130] loss=0.9451, lr=0.0000031, metrics:accuracy:0.5409
INFO:root:15:46:02 [Epoch 5 Batch 5600/13130] loss=0.9836, lr=0.0000010, metrics:accuracy:0.5115
INFO:root:15:46:32 [Epoch 5 Batch 4400/13130] loss=0.9195, lr=0.0000029, metrics:accuracy:0.5428
INFO:root:15:46:41 [Epoch 5 Batch 6000/13130] loss=0.9786, lr=0.0000010, metrics:accuracy:0.5119
INFO:root:15:47:11 [Epoch 5 Batch 4800/13130] loss=0.9166, lr=0.0000028, metrics:accuracy:0.5431
INFO:root:15:47:21 [Epoch 5 Batch 6400/13130] loss=0.9711, lr=0.0000009, metrics:accuracy:0.5123
INFO:root:15:47:50 [Epoch 5 Batch 5200/13130] loss=0.9181, lr=0.0000027, metrics:accuracy:0.5437
INFO:root:15:48:00 [Epoch 5 Batch 6800/13130] loss=0.9666, lr=0.0000009, metrics:accuracy:0.5138
INFO:root:15:48:29 [Epoch 5 Batch 5600/13130] loss=0.9455, lr=0.0000025, metrics:accuracy:0.5426
INFO:root:15:48:39 [Epoch 5 Batch 7200/13130] loss=0.9769, lr=0.0000008, metrics:accuracy:0.5135
INFO:root:15:49:08 [Epoch 5 Batch 6000/13130] loss=0.9235, lr=0.0000024, metrics:accuracy:0.5434
INFO:root:15:49:17 [Epoch 5 Batch 7600/13130] loss=0.9796, lr=0.0000007, metrics:accuracy:0.5134
INFO:root:15:49:47 [Epoch 5 Batch 6400/13130] loss=0.9194, lr=0.0000023, metrics:accuracy:0.5450
INFO:root:15:49:56 [Epoch 5 Batch 8000/13130] loss=0.9787, lr=0.0000007, metrics:accuracy:0.5133
INFO:root:15:50:26 [Epoch 5 Batch 6800/13130] loss=0.9312, lr=0.0000021, metrics:accuracy:0.5454
INFO:root:15:50:35 [Epoch 5 Batch 8400/13130] loss=0.9584, lr=0.0000006, metrics:accuracy:0.5145
INFO:root:15:51:05 [Epoch 5 Batch 7200/13130] loss=0.9236, lr=0.0000020, metrics:accuracy:0.5452
INFO:root:15:51:14 [Epoch 5 Batch 8800/13130] loss=0.9837, lr=0.0000006, metrics:accuracy:0.5140
INFO:root:15:51:44 [Epoch 5 Batch 7600/13130] loss=0.9345, lr=0.0000019, metrics:accuracy:0.5446
INFO:root:15:51:54 [Epoch 5 Batch 9200/13130] loss=0.9716, lr=0.0000005, metrics:accuracy:0.5140
INFO:root:15:52:23 [Epoch 5 Batch 8000/13130] loss=0.9216, lr=0.0000017, metrics:accuracy:0.5444
INFO:root:15:52:32 [Epoch 5 Batch 9600/13130] loss=0.9645, lr=0.0000005, metrics:accuracy:0.5148
INFO:root:15:53:02 [Epoch 5 Batch 8400/13130] loss=0.9193, lr=0.0000016, metrics:accuracy:0.5455
INFO:root:15:53:12 [Epoch 5 Batch 10000/13130] loss=0.9744, lr=0.0000004, metrics:accuracy:0.5151
INFO:root:15:53:40 [Epoch 5 Batch 8800/13130] loss=0.9468, lr=0.0000015, metrics:accuracy:0.5450
INFO:root:15:53:50 [Epoch 5 Batch 10400/13130] loss=0.9774, lr=0.0000004, metrics:accuracy:0.5153
INFO:root:15:54:20 [Epoch 5 Batch 9200/13130] loss=0.9272, lr=0.0000013, metrics:accuracy:0.5449
INFO:root:15:54:30 [Epoch 5 Batch 10800/13130] loss=0.9804, lr=0.0000003, metrics:accuracy:0.5153
INFO:root:15:54:58 [Epoch 5 Batch 9600/13130] loss=0.9211, lr=0.0000012, metrics:accuracy:0.5453
INFO:root:15:55:09 [Epoch 5 Batch 11200/13130] loss=0.9769, lr=0.0000003, metrics:accuracy:0.5154
INFO:root:15:55:38 [Epoch 5 Batch 10000/13130] loss=0.9266, lr=0.0000011, metrics:accuracy:0.5457
INFO:root:15:55:48 [Epoch 5 Batch 11600/13130] loss=0.9854, lr=0.0000002, metrics:accuracy:0.5151
INFO:root:15:56:17 [Epoch 5 Batch 10400/13130] loss=0.9322, lr=0.0000009, metrics:accuracy:0.5457
INFO:root:15:56:28 [Epoch 5 Batch 12000/13130] loss=0.9778, lr=0.0000001, metrics:accuracy:0.5152
INFO:root:15:56:56 [Epoch 5 Batch 10800/13130] loss=0.9293, lr=0.0000008, metrics:accuracy:0.5455
INFO:root:15:57:07 [Epoch 5 Batch 12400/13130] loss=0.9660, lr=0.0000001, metrics:accuracy:0.5155
INFO:root:15:57:35 [Epoch 5 Batch 11200/13130] loss=0.9294, lr=0.0000006, metrics:accuracy:0.5456
INFO:root:15:57:46 [Epoch 5 Batch 12800/13130] loss=0.9781, lr=0.0000000, metrics:accuracy:0.5156
INFO:root:15:58:14 [Epoch 5 Batch 11600/13130] loss=0.9406, lr=0.0000005, metrics:accuracy:0.5453
INFO:root:15:58:16 Finish training step: 32812
INFO:root:15:58:16 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:58:37 [Batch 400/3750] loss=0.9920, metrics:accuracy:0.5856
INFO:root:15:58:49 [Epoch 5 Batch 12000/13130] loss=0.9294, lr=0.0000004, metrics:accuracy:0.5454
INFO:root:15:58:58 [Batch 800/3750] loss=1.0050, metrics:accuracy:0.5814
INFO:root:15:59:19 [Batch 1200/3750] loss=0.9894, metrics:accuracy:0.5776
INFO:root:15:59:24 [Epoch 5 Batch 12400/13130] loss=0.9149, lr=0.0000002, metrics:accuracy:0.5456
INFO:root:15:59:40 [Batch 1600/3750] loss=1.0562, metrics:accuracy:0.5424
INFO:root:15:59:59 [Epoch 5 Batch 12800/13130] loss=0.9219, lr=0.0000001, metrics:accuracy:0.5459
INFO:root:16:00:01 [Batch 2000/3750] loss=1.0473, metrics:accuracy:0.5194
INFO:root:16:00:22 [Batch 2400/3750] loss=1.0340, metrics:accuracy:0.5073
INFO:root:16:00:26 Finish training step: 32812
INFO:root:16:00:26 Now we are doing evaluation on dev with gpu(0).
INFO:root:16:00:43 [Batch 2800/3750] loss=1.0391, metrics:accuracy:0.4959
INFO:root:16:00:47 [Batch 400/3750] loss=0.9923, metrics:accuracy:0.5887
INFO:root:16:01:04 [Batch 3200/3750] loss=1.0191, metrics:accuracy:0.4893
INFO:root:16:01:07 [Batch 800/3750] loss=1.0058, metrics:accuracy:0.5892
INFO:root:16:01:25 [Batch 3600/3750] loss=1.0242, metrics:accuracy:0.4833
INFO:root:16:01:28 [Batch 1200/3750] loss=0.9982, metrics:accuracy:0.5870
INFO:root:16:01:33 validation metrics:accuracy:0.4815
INFO:root:16:01:33 Time cost=196.57s, throughput=152.62 samples/s
INFO:root:16:01:35 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:16:01:35 Time cost=1464.26s
INFO:root:16:01:35 Best model at epoch 4. Validation metrics:accuracy:0.4815
INFO:root:16:01:35 Now we are doing testing on test with gpu(0).
INFO:root:16:01:47 [Batch 1600/3750] loss=1.1126, metrics:accuracy:0.5452
INFO:root:16:02:07 [Batch 2000/3750] loss=1.1045, metrics:accuracy:0.5206
INFO:root:16:02:27 [Batch 2400/3750] loss=1.1015, metrics:accuracy:0.5030
INFO:root:16:02:48 [Batch 2800/3750] loss=1.0800, metrics:accuracy:0.4920
INFO:root:16:03:04 Time cost=89.18s, throughput=168.20 samples/s
INFO:root:16:03:06 [Batch 3200/3750] loss=1.0656, metrics:accuracy:0.4851
INFO:root:16:03:18 [Batch 3600/3750] loss=1.0695, metrics:accuracy:0.4794
INFO:root:16:03:22 validation metrics:accuracy:0.4773
INFO:root:16:03:22 Time cost=176.49s, throughput=169.98 samples/s
INFO:root:16:03:24 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:16:03:24 Time cost=1442.06s
INFO:root:16:03:24 Best model at epoch 3. Validation metrics:accuracy:0.4789
INFO:root:16:03:24 Now we are doing testing on test with gpu(0).
INFO:root:16:04:16 Time cost=51.71s, throughput=290.09 samples/s
INFO:root:20:39:06 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:39:06 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:39:12 processing dataset...
INFO:root:20:39:34 Now we are doing BERT classification training on gpu(0)!
INFO:root:20:39:34 training steps=32812
INFO:root:20:40:30 [Epoch 1 Batch 400/13130] loss=1.1233, lr=0.0000012, metrics:accuracy:0.3364
INFO:root:20:41:27 [Epoch 1 Batch 800/13130] loss=1.1150, lr=0.0000024, metrics:accuracy:0.3355
INFO:root:20:42:25 [Epoch 1 Batch 1200/13130] loss=1.1161, lr=0.0000037, metrics:accuracy:0.3318
INFO:root:20:43:23 [Epoch 1 Batch 1600/13130] loss=1.1130, lr=0.0000049, metrics:accuracy:0.3344
INFO:root:20:44:21 [Epoch 1 Batch 2000/13130] loss=1.1181, lr=0.0000061, metrics:accuracy:0.3354
INFO:root:20:51:47 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:51:47 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:51:51 processing dataset...
INFO:root:20:52:14 Now we are doing BERT classification training on gpu(0)!
INFO:root:20:52:14 training steps=32812
INFO:root:20:52:35 [Epoch 1 Batch 400/13130] loss=1.1057, lr=0.0000012, metrics:accuracy:0.3869
INFO:root:20:52:57 [Epoch 1 Batch 800/13130] loss=1.0355, lr=0.0000024, metrics:accuracy:0.4232
INFO:root:20:53:15 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=8e-06, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:53:15 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:53:18 [Epoch 1 Batch 1200/13130] loss=0.9947, lr=0.0000037, metrics:accuracy:0.4504
INFO:root:20:53:20 processing dataset...
INFO:root:20:53:40 [Epoch 1 Batch 1600/13130] loss=0.9882, lr=0.0000049, metrics:accuracy:0.4670
INFO:root:20:53:44 Now we are doing BERT classification training on gpu(0)!
INFO:root:20:53:44 training steps=32812
INFO:root:20:53:57 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=1e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:53:57 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:54:01 processing dataset...
INFO:root:20:54:14 [Epoch 1 Batch 2000/13130] loss=0.9692, lr=0.0000061, metrics:accuracy:0.4779
INFO:root:20:54:23 [Epoch 1 Batch 400/13130] loss=1.1260, lr=0.0000005, metrics:accuracy:0.3681
INFO:root:20:54:24 Now we are doing BERT classification training on gpu(0)!
INFO:root:20:54:24 training steps=32812
INFO:root:20:54:45 [Epoch 1 Batch 400/13130] loss=1.1215, lr=0.0000006, metrics:accuracy:0.3753
INFO:root:20:54:53 [Epoch 1 Batch 2400/13130] loss=0.9699, lr=0.0000073, metrics:accuracy:0.4854
INFO:root:20:55:02 [Epoch 1 Batch 800/13130] loss=1.0737, lr=0.0000010, metrics:accuracy:0.3918
INFO:root:20:55:07 [Epoch 1 Batch 800/13130] loss=1.0665, lr=0.0000012, metrics:accuracy:0.4029
INFO:root:20:55:17 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=4e-06, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:55:17 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:55:28 [Epoch 1 Batch 1200/13130] loss=1.0187, lr=0.0000018, metrics:accuracy:0.4266
INFO:root:20:55:32 [Epoch 1 Batch 2800/13130] loss=0.9615, lr=0.0000085, metrics:accuracy:0.4916
INFO:root:20:55:41 [Epoch 1 Batch 1200/13130] loss=1.0263, lr=0.0000015, metrics:accuracy:0.4161
INFO:root:20:55:50 [Epoch 1 Batch 1600/13130] loss=1.0142, lr=0.0000024, metrics:accuracy:0.4428
INFO:root:20:56:00 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=4e-06, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:56:00 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:56:11 [Epoch 1 Batch 3200/13130] loss=0.9577, lr=0.0000097, metrics:accuracy:0.4980
INFO:root:20:56:11 [Epoch 1 Batch 2000/13130] loss=0.9913, lr=0.0000030, metrics:accuracy:0.4567
INFO:root:20:56:14 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=4e-06, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:56:14 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:56:19 processing dataset...
INFO:root:20:56:20 [Epoch 1 Batch 1600/13130] loss=1.0184, lr=0.0000019, metrics:accuracy:0.4339
INFO:root:20:56:33 [Epoch 1 Batch 2400/13130] loss=0.9835, lr=0.0000037, metrics:accuracy:0.4679
INFO:root:20:56:42 Now we are doing BERT classification training on gpu(0)!
INFO:root:20:56:42 training steps=32812
INFO:root:20:56:49 [Epoch 1 Batch 3600/13130] loss=0.9422, lr=0.0000110, metrics:accuracy:0.5040
INFO:root:20:56:59 [Epoch 1 Batch 2000/13130] loss=0.9974, lr=0.0000024, metrics:accuracy:0.4489
INFO:root:20:57:01 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float16', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:57:01 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:57:01 Using AMP
INFO:root:20:57:03 [Epoch 1 Batch 2800/13130] loss=0.9760, lr=0.0000043, metrics:accuracy:0.4752
INFO:root:20:57:21 [Epoch 1 Batch 400/13130] loss=1.1414, lr=0.0000002, metrics:accuracy:0.3575
INFO:root:20:57:29 [Epoch 1 Batch 4000/13130] loss=0.9404, lr=0.0000122, metrics:accuracy:0.5103
INFO:root:20:57:39 [Epoch 1 Batch 2400/13130] loss=0.9875, lr=0.0000029, metrics:accuracy:0.4600
INFO:root:20:57:41 [Epoch 1 Batch 3200/13130] loss=0.9635, lr=0.0000049, metrics:accuracy:0.4824
INFO:root:20:57:49 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float16', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:20:57:49 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:20:57:49 Using AMP
INFO:root:20:57:53 processing dataset...
INFO:root:20:57:59 [Epoch 1 Batch 800/13130] loss=1.0902, lr=0.0000005, metrics:accuracy:0.3730
INFO:root:20:58:08 [Epoch 1 Batch 4400/13130] loss=0.9395, lr=0.0000134, metrics:accuracy:0.5134
INFO:root:20:58:17 Now we are doing BERT classification training on gpu(0)!
INFO:root:20:58:17 training steps=32812
INFO:root:20:58:18 [Epoch 1 Batch 2800/13130] loss=0.9800, lr=0.0000034, metrics:accuracy:0.4689
INFO:root:20:58:20 [Epoch 1 Batch 3600/13130] loss=0.9478, lr=0.0000055, metrics:accuracy:0.4898
WARNING:py.warnings:20:58:20 finetune_classifier.py:642: UserWarning: nan or inf is detected. Clipping results will be undefined.
  nlp.utils.clip_grad_global_norm(params, 1)

INFO:root:20:58:20 AMP: decreasing loss scale to 32768.000000
INFO:root:20:58:21 AMP: decreasing loss scale to 16384.000000
INFO:root:20:58:22 AMP: decreasing loss scale to 8192.000000
INFO:root:20:58:31 AMP: decreasing loss scale to 4096.000000
INFO:root:20:58:38 [Epoch 1 Batch 1200/13130] loss=1.0609, lr=0.0000007, metrics:accuracy:0.3920
INFO:root:20:58:38 [Epoch 1 Batch 400/13130] loss=1.2014, lr=0.0000012, metrics:accuracy:0.3387
INFO:root:20:58:46 [Epoch 1 Batch 4800/13130] loss=0.9490, lr=0.0000146, metrics:accuracy:0.5160
INFO:root:20:58:55 [Epoch 1 Batch 800/13130] loss=1.1457, lr=0.0000024, metrics:accuracy:0.3458
INFO:root:20:58:56 [Epoch 1 Batch 3200/13130] loss=0.9644, lr=0.0000039, metrics:accuracy:0.4760
INFO:root:20:58:59 [Epoch 1 Batch 4000/13130] loss=0.9444, lr=0.0000061, metrics:accuracy:0.4965
INFO:root:20:59:13 [Epoch 1 Batch 1200/13130] loss=1.1100, lr=0.0000037, metrics:accuracy:0.3535
INFO:root:20:59:17 [Epoch 1 Batch 1600/13130] loss=1.0437, lr=0.0000010, metrics:accuracy:0.4099
INFO:root:20:59:25 [Epoch 1 Batch 5200/13130] loss=0.9449, lr=0.0000158, metrics:accuracy:0.5183
INFO:root:20:59:30 [Epoch 1 Batch 1600/13130] loss=1.1027, lr=0.0000049, metrics:accuracy:0.3568
INFO:root:20:59:34 [Epoch 1 Batch 3600/13130] loss=0.9498, lr=0.0000044, metrics:accuracy:0.4839
INFO:root:20:59:37 [Epoch 1 Batch 4400/13130] loss=0.9335, lr=0.0000067, metrics:accuracy:0.5016
INFO:root:20:59:48 [Epoch 1 Batch 2000/13130] loss=1.0983, lr=0.0000061, metrics:accuracy:0.3585
INFO:root:20:59:56 [Epoch 1 Batch 2000/13130] loss=1.0234, lr=0.0000012, metrics:accuracy:0.4235
INFO:root:21:00:04 [Epoch 1 Batch 5600/13130] loss=0.9284, lr=0.0000171, metrics:accuracy:0.5203
INFO:root:21:00:06 [Epoch 1 Batch 2400/13130] loss=1.1003, lr=0.0000073, metrics:accuracy:0.3606
INFO:root:21:00:11 AMP: decreasing loss scale to 2048.000000
INFO:root:21:00:14 [Epoch 1 Batch 4000/13130] loss=0.9488, lr=0.0000049, metrics:accuracy:0.4905
INFO:root:21:00:16 [Epoch 1 Batch 4800/13130] loss=0.9502, lr=0.0000073, metrics:accuracy:0.5047
INFO:root:21:00:23 AMP: decreasing loss scale to 1024.000000
INFO:root:21:00:24 [Epoch 1 Batch 2800/13130] loss=1.0896, lr=0.0000085, metrics:accuracy:0.3653
INFO:root:21:00:35 [Epoch 1 Batch 2400/13130] loss=1.0101, lr=0.0000015, metrics:accuracy:0.4341
INFO:root:21:00:42 [Epoch 1 Batch 3200/13130] loss=1.0593, lr=0.0000097, metrics:accuracy:0.3731
INFO:root:21:00:43 [Epoch 1 Batch 6000/13130] loss=0.9396, lr=0.0000183, metrics:accuracy:0.5220
INFO:root:21:00:53 [Epoch 1 Batch 4400/13130] loss=0.9362, lr=0.0000054, metrics:accuracy:0.4959
INFO:root:21:00:55 [Epoch 1 Batch 5200/13130] loss=0.9454, lr=0.0000079, metrics:accuracy:0.5079
INFO:root:21:00:59 [Epoch 1 Batch 3600/13130] loss=1.0481, lr=0.0000110, metrics:accuracy:0.3821
INFO:root:21:01:12 [Epoch 1 Batch 2800/13130] loss=1.0044, lr=0.0000017, metrics:accuracy:0.4433
INFO:root:21:01:17 [Epoch 1 Batch 4000/13130] loss=1.0236, lr=0.0000122, metrics:accuracy:0.3920
INFO:root:21:01:22 [Epoch 1 Batch 6400/13130] loss=0.9220, lr=0.0000195, metrics:accuracy:0.5244
INFO:root:21:01:32 [Epoch 1 Batch 4800/13130] loss=0.9506, lr=0.0000058, metrics:accuracy:0.4995
INFO:root:21:01:33 [Epoch 1 Batch 5600/13130] loss=0.9281, lr=0.0000085, metrics:accuracy:0.5111
INFO:root:21:01:35 [Epoch 1 Batch 4400/13130] loss=1.0083, lr=0.0000134, metrics:accuracy:0.4015
INFO:root:21:01:51 [Epoch 1 Batch 3200/13130] loss=0.9813, lr=0.0000019, metrics:accuracy:0.4527
INFO:root:21:01:53 [Epoch 1 Batch 4800/13130] loss=1.0111, lr=0.0000146, metrics:accuracy:0.4088
INFO:root:21:02:01 [Epoch 1 Batch 6800/13130] loss=0.9442, lr=0.0000199, metrics:accuracy:0.5258
INFO:root:21:02:10 [Epoch 1 Batch 5200/13130] loss=0.9480, lr=0.0000063, metrics:accuracy:0.5029
INFO:root:21:02:11 [Epoch 1 Batch 5200/13130] loss=0.9933, lr=0.0000158, metrics:accuracy:0.4167
INFO:root:21:02:12 [Epoch 1 Batch 6000/13130] loss=0.9443, lr=0.0000091, metrics:accuracy:0.5136
INFO:root:21:02:29 [Epoch 1 Batch 5600/13130] loss=0.9817, lr=0.0000171, metrics:accuracy:0.4235
INFO:root:21:02:29 [Epoch 1 Batch 3600/13130] loss=0.9724, lr=0.0000022, metrics:accuracy:0.4607
INFO:root:21:02:39 [Epoch 1 Batch 7200/13130] loss=0.9252, lr=0.0000198, metrics:accuracy:0.5274
INFO:root:21:02:47 [Epoch 1 Batch 6000/13130] loss=0.9810, lr=0.0000183, metrics:accuracy:0.4292
INFO:root:21:02:49 [Epoch 1 Batch 5600/13130] loss=0.9320, lr=0.0000068, metrics:accuracy:0.5063
INFO:root:21:02:50 [Epoch 1 Batch 6400/13130] loss=0.9164, lr=0.0000098, metrics:accuracy:0.5169
INFO:root:21:03:05 [Epoch 1 Batch 6400/13130] loss=0.9690, lr=0.0000195, metrics:accuracy:0.4352
INFO:root:21:03:08 [Epoch 1 Batch 4000/13130] loss=0.9608, lr=0.0000024, metrics:accuracy:0.4692
INFO:root:21:03:18 [Epoch 1 Batch 7600/13130] loss=0.9313, lr=0.0000196, metrics:accuracy:0.5289
INFO:root:21:03:22 AMP: increasing loss scale to 2048.000000
INFO:root:21:03:23 [Epoch 1 Batch 6800/13130] loss=0.9649, lr=0.0000199, metrics:accuracy:0.4408
INFO:root:21:03:28 [Epoch 1 Batch 6000/13130] loss=0.9404, lr=0.0000073, metrics:accuracy:0.5095
INFO:root:21:03:29 [Epoch 1 Batch 6800/13130] loss=0.9332, lr=0.0000100, metrics:accuracy:0.5192
INFO:root:21:03:40 [Epoch 1 Batch 7200/13130] loss=0.9441, lr=0.0000198, metrics:accuracy:0.4468
INFO:root:21:03:47 [Epoch 1 Batch 4400/13130] loss=0.9476, lr=0.0000027, metrics:accuracy:0.4757
INFO:root:21:03:57 [Epoch 1 Batch 8000/13130] loss=0.9413, lr=0.0000195, metrics:accuracy:0.5292
INFO:root:21:03:58 [Epoch 1 Batch 7600/13130] loss=0.9533, lr=0.0000196, metrics:accuracy:0.4517
INFO:root:21:04:07 [Epoch 1 Batch 7200/13130] loss=0.9180, lr=0.0000099, metrics:accuracy:0.5212
INFO:root:21:04:08 [Epoch 1 Batch 6400/13130] loss=0.9180, lr=0.0000078, metrics:accuracy:0.5126
INFO:root:21:04:16 [Epoch 1 Batch 8000/13130] loss=0.9647, lr=0.0000195, metrics:accuracy:0.4554
INFO:root:21:04:25 [Epoch 1 Batch 4800/13130] loss=0.9623, lr=0.0000029, metrics:accuracy:0.4801
INFO:root:21:04:34 [Epoch 1 Batch 8400/13130] loss=0.9550, lr=0.0000194, metrics:accuracy:0.4589
INFO:root:21:04:37 [Epoch 1 Batch 8400/13130] loss=0.9276, lr=0.0000194, metrics:accuracy:0.5307
INFO:root:21:04:45 [Epoch 1 Batch 7600/13130] loss=0.9230, lr=0.0000098, metrics:accuracy:0.5238
INFO:root:21:04:46 AMP: decreasing loss scale to 1024.000000
INFO:root:21:04:47 [Epoch 1 Batch 6800/13130] loss=0.9280, lr=0.0000080, metrics:accuracy:0.5155
INFO:root:21:04:51 [Epoch 1 Batch 8800/13130] loss=0.9718, lr=0.0000192, metrics:accuracy:0.4620
INFO:root:21:05:03 [Epoch 1 Batch 5200/13130] loss=0.9596, lr=0.0000032, metrics:accuracy:0.4847
INFO:root:21:05:09 [Epoch 1 Batch 9200/13130] loss=0.9422, lr=0.0000191, metrics:accuracy:0.4658
INFO:root:21:05:15 [Epoch 1 Batch 8800/13130] loss=0.9391, lr=0.0000192, metrics:accuracy:0.5315
INFO:root:21:05:23 [Epoch 1 Batch 8000/13130] loss=0.9293, lr=0.0000098, metrics:accuracy:0.5253
INFO:root:21:05:25 [Epoch 1 Batch 7200/13130] loss=0.9190, lr=0.0000079, metrics:accuracy:0.5179
INFO:root:21:05:27 [Epoch 1 Batch 9600/13130] loss=0.9600, lr=0.0000190, metrics:accuracy:0.4686
INFO:root:21:05:42 [Epoch 1 Batch 5600/13130] loss=0.9407, lr=0.0000034, metrics:accuracy:0.4884
INFO:root:21:05:45 [Epoch 1 Batch 10000/13130] loss=0.9357, lr=0.0000188, metrics:accuracy:0.4717
INFO:root:21:05:54 [Epoch 1 Batch 9200/13130] loss=0.9284, lr=0.0000191, metrics:accuracy:0.5327
INFO:root:21:06:02 [Epoch 1 Batch 10400/13130] loss=0.9310, lr=0.0000187, metrics:accuracy:0.4753
INFO:root:21:06:03 [Epoch 1 Batch 8400/13130] loss=0.9233, lr=0.0000097, metrics:accuracy:0.5269
INFO:root:21:06:04 [Epoch 1 Batch 7600/13130] loss=0.9219, lr=0.0000079, metrics:accuracy:0.5204
INFO:root:21:06:20 [Epoch 1 Batch 10800/13130] loss=0.9212, lr=0.0000186, metrics:accuracy:0.4784
INFO:root:21:06:21 [Epoch 1 Batch 6000/13130] loss=0.9459, lr=0.0000037, metrics:accuracy:0.4915
INFO:root:21:06:32 [Epoch 1 Batch 9600/13130] loss=0.9348, lr=0.0000190, metrics:accuracy:0.5336
INFO:root:21:06:37 [Epoch 1 Batch 11200/13130] loss=0.9460, lr=0.0000184, metrics:accuracy:0.4810
INFO:root:21:06:40 [Epoch 1 Batch 8800/13130] loss=0.9317, lr=0.0000096, metrics:accuracy:0.5277
INFO:root:21:06:43 [Epoch 1 Batch 8000/13130] loss=0.9272, lr=0.0000078, metrics:accuracy:0.5219
INFO:root:21:06:54 [Epoch 1 Batch 11600/13130] loss=0.9296, lr=0.0000183, metrics:accuracy:0.4834
INFO:root:21:06:59 [Epoch 1 Batch 6400/13130] loss=0.9302, lr=0.0000039, metrics:accuracy:0.4949
INFO:root:21:07:11 [Epoch 1 Batch 12000/13130] loss=0.9434, lr=0.0000182, metrics:accuracy:0.4853
INFO:root:21:07:12 [Epoch 1 Batch 10000/13130] loss=0.9138, lr=0.0000188, metrics:accuracy:0.5348
INFO:root:21:07:19 [Epoch 1 Batch 9200/13130] loss=0.9243, lr=0.0000096, metrics:accuracy:0.5291
INFO:root:21:07:22 [Epoch 1 Batch 8400/13130] loss=0.9219, lr=0.0000078, metrics:accuracy:0.5237
INFO:root:21:07:28 [Epoch 1 Batch 12400/13130] loss=0.9422, lr=0.0000180, metrics:accuracy:0.4871
INFO:root:21:07:37 [Epoch 1 Batch 6800/13130] loss=0.9327, lr=0.0000040, metrics:accuracy:0.4985
INFO:root:21:07:40 AMP: increasing loss scale to 2048.000000
INFO:root:21:07:45 [Epoch 1 Batch 12800/13130] loss=0.9163, lr=0.0000179, metrics:accuracy:0.4895
INFO:root:21:07:51 [Epoch 1 Batch 10400/13130] loss=0.9074, lr=0.0000187, metrics:accuracy:0.5367
INFO:root:21:07:57 [Epoch 1 Batch 9600/13130] loss=0.9233, lr=0.0000095, metrics:accuracy:0.5304
INFO:root:21:08:00 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:08:01 [Epoch 1 Batch 8800/13130] loss=0.9304, lr=0.0000077, metrics:accuracy:0.5249
INFO:root:21:08:08 [Batch 400/3750] loss=0.9477, metrics:accuracy:0.6538
INFO:root:21:08:15 [Epoch 1 Batch 7200/13130] loss=0.9266, lr=0.0000040, metrics:accuracy:0.5016
INFO:root:21:08:15 [Batch 800/3750] loss=0.9485, metrics:accuracy:0.6559
INFO:root:21:08:23 [Batch 1200/3750] loss=0.9470, metrics:accuracy:0.6529
INFO:root:21:08:30 [Epoch 1 Batch 10800/13130] loss=0.8949, lr=0.0000186, metrics:accuracy:0.5387
INFO:root:21:08:30 [Batch 1600/3750] loss=0.9401, metrics:accuracy:0.6184
INFO:root:21:08:35 [Epoch 1 Batch 10000/13130] loss=0.9086, lr=0.0000094, metrics:accuracy:0.5320
INFO:root:21:08:38 [Batch 2000/3750] loss=0.9168, metrics:accuracy:0.5978
INFO:root:21:08:39 [Epoch 1 Batch 9200/13130] loss=0.9134, lr=0.0000076, metrics:accuracy:0.5266
INFO:root:21:08:46 [Batch 2400/3750] loss=0.9197, metrics:accuracy:0.5829
INFO:root:21:08:53 [Batch 2800/3750] loss=0.9090, metrics:accuracy:0.5734
INFO:root:21:08:54 [Epoch 1 Batch 7600/13130] loss=0.9260, lr=0.0000039, metrics:accuracy:0.5048
INFO:root:21:09:01 [Batch 3200/3750] loss=0.8834, metrics:accuracy:0.5673
INFO:root:21:09:08 [Epoch 1 Batch 11200/13130] loss=0.9155, lr=0.0000184, metrics:accuracy:0.5401
INFO:root:21:09:08 [Batch 3600/3750] loss=0.8830, metrics:accuracy:0.5633
INFO:root:21:09:11 validation metrics:accuracy:0.5623
INFO:root:21:09:11 Time cost=70.68s, throughput=424.46 samples/s
INFO:root:21:09:13 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:21:09:13 Time cost=656.36s
INFO:root:21:09:14 [Epoch 1 Batch 10400/13130] loss=0.8964, lr=0.0000094, metrics:accuracy:0.5342
INFO:root:21:09:17 [Epoch 1 Batch 9600/13130] loss=0.9231, lr=0.0000076, metrics:accuracy:0.5281
INFO:root:21:09:31 [Epoch 2 Batch 400/13130] loss=0.9265, lr=0.0000176, metrics:accuracy:0.5572
INFO:root:21:09:32 [Epoch 1 Batch 8000/13130] loss=0.9327, lr=0.0000039, metrics:accuracy:0.5073
INFO:root:21:09:48 [Epoch 1 Batch 11600/13130] loss=0.9011, lr=0.0000183, metrics:accuracy:0.5414
INFO:root:21:09:49 [Epoch 2 Batch 800/13130] loss=0.9460, lr=0.0000175, metrics:accuracy:0.5495
INFO:root:21:09:52 [Epoch 1 Batch 10800/13130] loss=0.8830, lr=0.0000093, metrics:accuracy:0.5365
INFO:root:21:09:57 [Epoch 1 Batch 10000/13130] loss=0.9045, lr=0.0000075, metrics:accuracy:0.5296
INFO:root:21:10:07 [Epoch 2 Batch 1200/13130] loss=0.9359, lr=0.0000174, metrics:accuracy:0.5511
INFO:root:21:10:10 [Epoch 1 Batch 8400/13130] loss=0.9268, lr=0.0000039, metrics:accuracy:0.5096
INFO:root:21:10:23 AMP: decreasing loss scale to 1024.000000
INFO:root:21:10:25 [Epoch 2 Batch 1600/13130] loss=0.9253, lr=0.0000172, metrics:accuracy:0.5555
INFO:root:21:10:26 [Epoch 1 Batch 12000/13130] loss=0.9145, lr=0.0000182, metrics:accuracy:0.5421
INFO:root:21:10:30 [Epoch 1 Batch 11200/13130] loss=0.9128, lr=0.0000092, metrics:accuracy:0.5377
INFO:root:21:10:36 [Epoch 1 Batch 10400/13130] loss=0.8984, lr=0.0000075, metrics:accuracy:0.5317
INFO:root:21:10:43 [Epoch 2 Batch 2000/13130] loss=0.9311, lr=0.0000171, metrics:accuracy:0.5544
INFO:root:21:10:48 [Epoch 1 Batch 8800/13130] loss=0.9385, lr=0.0000038, metrics:accuracy:0.5114
INFO:root:21:11:01 [Epoch 2 Batch 2400/13130] loss=0.9281, lr=0.0000170, metrics:accuracy:0.5546
INFO:root:21:11:05 [Epoch 1 Batch 12400/13130] loss=0.9138, lr=0.0000180, metrics:accuracy:0.5428
INFO:root:21:11:09 [Epoch 1 Batch 11600/13130] loss=0.9026, lr=0.0000091, metrics:accuracy:0.5392
INFO:root:21:11:14 [Epoch 1 Batch 10800/13130] loss=0.8838, lr=0.0000074, metrics:accuracy:0.5342
INFO:root:21:11:18 [Epoch 2 Batch 2800/13130] loss=0.9333, lr=0.0000168, metrics:accuracy:0.5545
INFO:root:21:11:27 [Epoch 1 Batch 9200/13130] loss=0.9164, lr=0.0000038, metrics:accuracy:0.5135
INFO:root:21:11:36 [Epoch 2 Batch 3200/13130] loss=0.9185, lr=0.0000167, metrics:accuracy:0.5559
INFO:root:21:11:44 [Epoch 1 Batch 12800/13130] loss=0.8892, lr=0.0000179, metrics:accuracy:0.5441
INFO:root:21:11:47 [Epoch 1 Batch 12000/13130] loss=0.9084, lr=0.0000091, metrics:accuracy:0.5400
INFO:root:21:11:53 [Epoch 1 Batch 11200/13130] loss=0.9139, lr=0.0000074, metrics:accuracy:0.5357
INFO:root:21:11:53 [Epoch 2 Batch 3600/13130] loss=0.9118, lr=0.0000166, metrics:accuracy:0.5565
INFO:root:21:12:05 [Epoch 1 Batch 9600/13130] loss=0.9247, lr=0.0000038, metrics:accuracy:0.5152
INFO:root:21:12:11 [Epoch 2 Batch 4000/13130] loss=0.9193, lr=0.0000164, metrics:accuracy:0.5556
INFO:root:21:12:16 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:12:25 [Epoch 1 Batch 12400/13130] loss=0.9086, lr=0.0000090, metrics:accuracy:0.5407
INFO:root:21:12:28 [Epoch 2 Batch 4400/13130] loss=0.9146, lr=0.0000163, metrics:accuracy:0.5570
INFO:root:21:12:30 [Epoch 1 Batch 11600/13130] loss=0.8992, lr=0.0000073, metrics:accuracy:0.5373
INFO:root:21:12:37 [Batch 400/3750] loss=0.9334, metrics:accuracy:0.6663
INFO:root:21:12:43 [Epoch 1 Batch 10000/13130] loss=0.9067, lr=0.0000038, metrics:accuracy:0.5176
INFO:root:21:12:46 [Epoch 2 Batch 4800/13130] loss=0.9443, lr=0.0000162, metrics:accuracy:0.5565
INFO:root:21:12:58 [Batch 800/3750] loss=0.9456, metrics:accuracy:0.6625
INFO:root:21:13:04 [Epoch 2 Batch 5200/13130] loss=0.9265, lr=0.0000160, metrics:accuracy:0.5568
INFO:root:21:13:04 [Epoch 1 Batch 12800/13130] loss=0.8874, lr=0.0000089, metrics:accuracy:0.5422
INFO:root:21:13:05 [Epoch 1 Batch 12000/13130] loss=0.9058, lr=0.0000073, metrics:accuracy:0.5384
INFO:root:21:13:19 [Batch 1200/3750] loss=0.9419, metrics:accuracy:0.6613
INFO:root:21:13:20 AMP: increasing loss scale to 2048.000000
INFO:root:21:13:22 [Epoch 2 Batch 5600/13130] loss=0.9064, lr=0.0000159, metrics:accuracy:0.5572
INFO:root:21:13:22 [Epoch 1 Batch 10400/13130] loss=0.9046, lr=0.0000037, metrics:accuracy:0.5199
INFO:root:21:13:36 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:13:39 [Batch 1600/3750] loss=0.8558, metrics:accuracy:0.6437
INFO:root:21:13:39 [Epoch 1 Batch 12400/13130] loss=0.9095, lr=0.0000072, metrics:accuracy:0.5396
INFO:root:21:13:40 [Epoch 2 Batch 6000/13130] loss=0.9213, lr=0.0000157, metrics:accuracy:0.5582
INFO:root:21:13:56 [Batch 400/3750] loss=0.9369, metrics:accuracy:0.6716
INFO:root:21:13:57 [Epoch 1 Batch 10800/13130] loss=0.8905, lr=0.0000037, metrics:accuracy:0.5225
INFO:root:21:13:58 [Epoch 2 Batch 6400/13130] loss=0.9184, lr=0.0000156, metrics:accuracy:0.5584
INFO:root:21:14:00 [Batch 2000/3750] loss=0.8229, metrics:accuracy:0.6342
INFO:root:21:14:06 AMP: decreasing loss scale to 1024.000000
INFO:root:21:14:14 [Epoch 1 Batch 12800/13130] loss=0.8829, lr=0.0000072, metrics:accuracy:0.5412
INFO:root:21:14:16 [Epoch 2 Batch 6800/13130] loss=0.9362, lr=0.0000155, metrics:accuracy:0.5581
INFO:root:21:14:16 [Batch 800/3750] loss=0.9449, metrics:accuracy:0.6689
INFO:root:21:14:21 [Batch 2400/3750] loss=0.8321, metrics:accuracy:0.6268
INFO:root:21:14:30 [Epoch 1 Batch 11200/13130] loss=0.9208, lr=0.0000037, metrics:accuracy:0.5242
INFO:root:21:14:34 [Epoch 2 Batch 7200/13130] loss=0.9142, lr=0.0000153, metrics:accuracy:0.5584
INFO:root:21:14:37 [Batch 1200/3750] loss=0.9404, metrics:accuracy:0.6670
INFO:root:21:14:42 [Batch 2800/3750] loss=0.8848, metrics:accuracy:0.6118
INFO:root:21:14:43 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:14:44 AMP: decreasing loss scale to 512.000000
INFO:root:21:14:51 [Epoch 2 Batch 7600/13130] loss=0.9175, lr=0.0000152, metrics:accuracy:0.5586
INFO:root:21:14:58 [Batch 1600/3750] loss=0.8786, metrics:accuracy:0.6393
INFO:root:21:15:02 [Batch 3200/3750] loss=0.8932, metrics:accuracy:0.5982
INFO:root:21:15:03 [Batch 400/3750] loss=0.9558, metrics:accuracy:0.6609
INFO:root:21:15:05 [Epoch 1 Batch 11600/13130] loss=0.9050, lr=0.0000037, metrics:accuracy:0.5258
INFO:root:21:15:09 [Epoch 2 Batch 8000/13130] loss=0.9186, lr=0.0000151, metrics:accuracy:0.5582
INFO:root:21:15:18 [Batch 2000/3750] loss=0.8500, metrics:accuracy:0.6218
INFO:root:21:15:23 [Batch 3600/3750] loss=0.8862, metrics:accuracy:0.5887
INFO:root:21:15:24 [Batch 800/3750] loss=0.9654, metrics:accuracy:0.6592
INFO:root:21:15:27 [Epoch 2 Batch 8400/13130] loss=0.9174, lr=0.0000149, metrics:accuracy:0.5583
INFO:root:21:15:31 validation metrics:accuracy:0.5861
INFO:root:21:15:31 Time cost=195.46s, throughput=153.49 samples/s
INFO:root:21:15:33 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:21:15:33 Time cost=1398.69s
INFO:root:21:15:39 [Epoch 1 Batch 12000/13130] loss=0.9117, lr=0.0000036, metrics:accuracy:0.5272
INFO:root:21:15:39 [Batch 2400/3750] loss=0.8579, metrics:accuracy:0.6090
INFO:root:21:15:43 [Batch 1200/3750] loss=0.9620, metrics:accuracy:0.6580
INFO:root:21:15:45 [Epoch 2 Batch 8800/13130] loss=0.9268, lr=0.0000148, metrics:accuracy:0.5583
INFO:root:21:15:59 [Batch 2800/3750] loss=0.8655, metrics:accuracy:0.5992
INFO:root:21:16:03 [Batch 1600/3750] loss=0.8830, metrics:accuracy:0.6352
INFO:root:21:16:04 [Epoch 2 Batch 9200/13130] loss=0.9003, lr=0.0000147, metrics:accuracy:0.5592
INFO:root:21:16:09 [Epoch 2 Batch 400/13130] loss=0.8461, lr=0.0000176, metrics:accuracy:0.6203
INFO:root:21:16:13 [Epoch 1 Batch 12400/13130] loss=0.9152, lr=0.0000036, metrics:accuracy:0.5283
INFO:root:21:16:20 [Batch 3200/3750] loss=0.8583, metrics:accuracy:0.5915
INFO:root:21:16:22 [Epoch 2 Batch 9600/13130] loss=0.9075, lr=0.0000145, metrics:accuracy:0.5594
INFO:root:21:16:24 [Batch 2000/3750] loss=0.8546, metrics:accuracy:0.6222
INFO:root:21:16:40 [Epoch 2 Batch 10000/13130] loss=0.9038, lr=0.0000144, metrics:accuracy:0.5600
INFO:root:21:16:40 [Batch 3600/3750] loss=0.8527, metrics:accuracy:0.5862
INFO:root:21:16:43 [Epoch 2 Batch 800/13130] loss=0.8620, lr=0.0000175, metrics:accuracy:0.6145
INFO:root:21:16:45 [Batch 2400/3750] loss=0.8643, metrics:accuracy:0.6103
INFO:root:21:16:48 [Epoch 1 Batch 12800/13130] loss=0.8867, lr=0.0000036, metrics:accuracy:0.5301
INFO:root:21:16:48 validation metrics:accuracy:0.5856
INFO:root:21:16:48 Time cost=192.56s, throughput=155.79 samples/s
INFO:root:21:16:50 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:21:16:50 Time cost=1345.41s
INFO:root:21:16:57 [Epoch 2 Batch 10400/13130] loss=0.9115, lr=0.0000143, metrics:accuracy:0.5604
INFO:root:21:17:06 [Batch 2800/3750] loss=0.8523, metrics:accuracy:0.6011
INFO:root:21:17:15 [Epoch 2 Batch 10800/13130] loss=0.9242, lr=0.0000141, metrics:accuracy:0.5603
INFO:root:21:17:17 [Epoch 2 Batch 1200/13130] loss=0.8593, lr=0.0000174, metrics:accuracy:0.6119
INFO:root:21:17:18 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:17:26 [Batch 3200/3750] loss=0.8408, metrics:accuracy:0.5938
INFO:root:21:17:28 [Epoch 2 Batch 400/13130] loss=0.8499, lr=0.0000088, metrics:accuracy:0.6084
INFO:root:21:17:33 [Epoch 2 Batch 11200/13130] loss=0.9243, lr=0.0000140, metrics:accuracy:0.5600
INFO:root:21:17:38 [Batch 400/3750] loss=0.9899, metrics:accuracy:0.6381
INFO:root:21:17:43 AMP: increasing loss scale to 1024.000000
INFO:root:21:17:47 [Batch 3600/3750] loss=0.8352, metrics:accuracy:0.5890
INFO:root:21:17:51 [Epoch 2 Batch 11600/13130] loss=0.9161, lr=0.0000138, metrics:accuracy:0.5603
INFO:root:21:17:52 [Epoch 2 Batch 1600/13130] loss=0.8368, lr=0.0000172, metrics:accuracy:0.6142
INFO:root:21:17:54 validation metrics:accuracy:0.5884
INFO:root:21:17:54 Time cost=191.52s, throughput=156.64 samples/s
INFO:root:21:17:56 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:21:17:56 Time cost=1451.92s
INFO:root:21:17:59 [Batch 800/3750] loss=0.9951, metrics:accuracy:0.6395
INFO:root:21:18:02 [Epoch 2 Batch 800/13130] loss=0.8761, lr=0.0000088, metrics:accuracy:0.6033
INFO:root:21:18:10 [Epoch 2 Batch 12000/13130] loss=0.9155, lr=0.0000137, metrics:accuracy:0.5605
INFO:root:21:18:19 [Batch 1200/3750] loss=0.9921, metrics:accuracy:0.6379
INFO:root:21:18:29 [Epoch 2 Batch 12400/13130] loss=0.9144, lr=0.0000136, metrics:accuracy:0.5608
INFO:root:21:18:30 [Epoch 2 Batch 2000/13130] loss=0.8341, lr=0.0000171, metrics:accuracy:0.6172
INFO:root:21:18:36 [Epoch 2 Batch 1200/13130] loss=0.8687, lr=0.0000087, metrics:accuracy:0.6051
INFO:root:21:18:36 [Epoch 2 Batch 400/13130] loss=0.8596, lr=0.0000071, metrics:accuracy:0.5972
INFO:root:21:18:39 [Batch 1600/3750] loss=0.9009, metrics:accuracy:0.6129
INFO:root:21:18:48 [Epoch 2 Batch 12800/13130] loss=0.9030, lr=0.0000134, metrics:accuracy:0.5610
INFO:root:21:19:00 [Batch 2000/3750] loss=0.8724, metrics:accuracy:0.5989
INFO:root:21:19:03 AMP: decreasing loss scale to 512.000000
INFO:root:21:19:03 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:19:09 [Epoch 2 Batch 2400/13130] loss=0.8509, lr=0.0000170, metrics:accuracy:0.6154
INFO:root:21:19:10 [Epoch 2 Batch 1600/13130] loss=0.8447, lr=0.0000086, metrics:accuracy:0.6078
INFO:root:21:19:11 [Batch 400/3750] loss=0.7537, metrics:accuracy:0.7722
INFO:root:21:19:15 [Epoch 2 Batch 800/13130] loss=0.8777, lr=0.0000070, metrics:accuracy:0.5978
INFO:root:21:19:19 [Batch 800/3750] loss=0.7585, metrics:accuracy:0.7697
INFO:root:21:19:20 [Batch 2400/3750] loss=0.8800, metrics:accuracy:0.5886
INFO:root:21:19:26 [Batch 1200/3750] loss=0.7568, metrics:accuracy:0.7693
INFO:root:21:19:34 [Batch 1600/3750] loss=0.9468, metrics:accuracy:0.6989
INFO:root:21:19:40 [Batch 2800/3750] loss=0.8405, metrics:accuracy:0.5844
INFO:root:21:19:42 [Batch 2000/3750] loss=0.9607, metrics:accuracy:0.6516
INFO:root:21:19:44 [Epoch 2 Batch 2000/13130] loss=0.8465, lr=0.0000085, metrics:accuracy:0.6080
INFO:root:21:19:48 [Epoch 2 Batch 2800/13130] loss=0.8691, lr=0.0000168, metrics:accuracy:0.6140
INFO:root:21:19:50 [Batch 2400/3750] loss=0.9661, metrics:accuracy:0.6199
INFO:root:21:19:53 [Epoch 2 Batch 1200/13130] loss=0.8752, lr=0.0000069, metrics:accuracy:0.5988
INFO:root:21:19:58 [Batch 2800/3750] loss=0.9801, metrics:accuracy:0.5998
INFO:root:21:20:01 [Batch 3200/3750] loss=0.8181, metrics:accuracy:0.5816
INFO:root:21:20:05 [Batch 3200/3750] loss=0.9683, metrics:accuracy:0.5878
INFO:root:21:20:13 [Batch 3600/3750] loss=0.9711, metrics:accuracy:0.5780
INFO:root:21:20:16 validation metrics:accuracy:0.5754
INFO:root:21:20:16 Time cost=73.08s, throughput=410.49 samples/s
INFO:root:21:20:18 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:21:20:18 Time cost=664.50s
INFO:root:21:20:18 [Epoch 2 Batch 2400/13130] loss=0.8531, lr=0.0000085, metrics:accuracy:0.6069
INFO:root:21:20:22 [Batch 3600/3750] loss=0.8131, metrics:accuracy:0.5807
INFO:root:21:20:27 [Epoch 2 Batch 3200/13130] loss=0.8378, lr=0.0000167, metrics:accuracy:0.6143
INFO:root:21:20:29 validation metrics:accuracy:0.5810
INFO:root:21:20:29 Time cost=191.41s, throughput=156.73 samples/s
INFO:root:21:20:31 params saved in: ./output_dir/model_bert_weibo3_0.params
INFO:root:21:20:31 Time cost=1428.84s
INFO:root:21:20:31 [Epoch 2 Batch 1600/13130] loss=0.8540, lr=0.0000069, metrics:accuracy:0.6009
INFO:root:21:20:36 [Epoch 3 Batch 400/13130] loss=0.9037, lr=0.0000132, metrics:accuracy:0.5706
INFO:root:21:20:54 [Epoch 2 Batch 2800/13130] loss=0.8737, lr=0.0000084, metrics:accuracy:0.6067
INFO:root:21:20:54 [Epoch 3 Batch 800/13130] loss=0.9117, lr=0.0000131, metrics:accuracy:0.5678
INFO:root:21:21:06 [Epoch 2 Batch 3600/13130] loss=0.8197, lr=0.0000166, metrics:accuracy:0.6149
INFO:root:21:21:10 [Epoch 2 Batch 2000/13130] loss=0.8524, lr=0.0000068, metrics:accuracy:0.6025
INFO:root:21:21:10 [Epoch 2 Batch 400/13130] loss=0.8811, lr=0.0000035, metrics:accuracy:0.5856
INFO:root:21:21:12 [Epoch 3 Batch 1200/13130] loss=0.9020, lr=0.0000129, metrics:accuracy:0.5687
INFO:root:21:21:30 [Epoch 3 Batch 1600/13130] loss=0.9072, lr=0.0000128, metrics:accuracy:0.5703
INFO:root:21:21:33 [Epoch 2 Batch 3200/13130] loss=0.8515, lr=0.0000083, metrics:accuracy:0.6070
INFO:root:21:21:45 [Epoch 2 Batch 4000/13130] loss=0.8418, lr=0.0000164, metrics:accuracy:0.6153
INFO:root:21:21:47 [Epoch 3 Batch 2000/13130] loss=0.8904, lr=0.0000127, metrics:accuracy:0.5731
INFO:root:21:21:49 [Epoch 2 Batch 800/13130] loss=0.9025, lr=0.0000035, metrics:accuracy:0.5795
INFO:root:21:21:49 [Epoch 2 Batch 2400/13130] loss=0.8639, lr=0.0000068, metrics:accuracy:0.6031
INFO:root:21:22:05 [Epoch 3 Batch 2400/13130] loss=0.8982, lr=0.0000125, metrics:accuracy:0.5726
INFO:root:21:22:10 [Epoch 2 Batch 3600/13130] loss=0.8320, lr=0.0000083, metrics:accuracy:0.6074
INFO:root:21:22:23 [Epoch 3 Batch 2800/13130] loss=0.8855, lr=0.0000124, metrics:accuracy:0.5744
INFO:root:21:22:23 [Epoch 2 Batch 4400/13130] loss=0.8301, lr=0.0000163, metrics:accuracy:0.6161
INFO:root:21:22:26 [Epoch 2 Batch 1200/13130] loss=0.8932, lr=0.0000035, metrics:accuracy:0.5822
INFO:root:21:22:28 [Epoch 2 Batch 2800/13130] loss=0.8786, lr=0.0000067, metrics:accuracy:0.6032
INFO:root:21:22:40 [Epoch 3 Batch 3200/13130] loss=0.8924, lr=0.0000122, metrics:accuracy:0.5751
INFO:root:21:22:49 [Epoch 2 Batch 4000/13130] loss=0.8458, lr=0.0000082, metrics:accuracy:0.6073
INFO:root:21:22:58 [Epoch 3 Batch 3600/13130] loss=0.9050, lr=0.0000121, metrics:accuracy:0.5750
INFO:root:21:23:03 [Epoch 2 Batch 4800/13130] loss=0.8530, lr=0.0000162, metrics:accuracy:0.6156
INFO:root:21:23:05 [Epoch 2 Batch 1600/13130] loss=0.8785, lr=0.0000034, metrics:accuracy:0.5865
INFO:root:21:23:08 [Epoch 2 Batch 3200/13130] loss=0.8542, lr=0.0000067, metrics:accuracy:0.6030
INFO:root:21:23:15 AMP: increasing loss scale to 1024.000000
INFO:root:21:23:15 [Epoch 3 Batch 4000/13130] loss=0.9047, lr=0.0000120, metrics:accuracy:0.5739
INFO:root:21:23:26 [Epoch 2 Batch 4400/13130] loss=0.8345, lr=0.0000081, metrics:accuracy:0.6086
INFO:root:21:23:34 [Epoch 3 Batch 4400/13130] loss=0.8925, lr=0.0000118, metrics:accuracy:0.5750
INFO:root:21:23:42 [Epoch 2 Batch 5200/13130] loss=0.8342, lr=0.0000160, metrics:accuracy:0.6163
INFO:root:21:23:44 [Epoch 2 Batch 2000/13130] loss=0.8742, lr=0.0000034, metrics:accuracy:0.5891
INFO:root:21:23:47 [Epoch 2 Batch 3600/13130] loss=0.8382, lr=0.0000066, metrics:accuracy:0.6035
INFO:root:21:23:51 [Epoch 3 Batch 4800/13130] loss=0.8979, lr=0.0000117, metrics:accuracy:0.5748
INFO:root:21:24:06 [Epoch 2 Batch 4800/13130] loss=0.8616, lr=0.0000081, metrics:accuracy:0.6082
INFO:root:21:24:09 [Epoch 3 Batch 5200/13130] loss=0.8950, lr=0.0000116, metrics:accuracy:0.5743
INFO:root:21:24:21 [Epoch 2 Batch 5600/13130] loss=0.8210, lr=0.0000159, metrics:accuracy:0.6171
INFO:root:21:24:22 [Epoch 2 Batch 2400/13130] loss=0.8812, lr=0.0000034, metrics:accuracy:0.5890
INFO:root:21:24:26 [Epoch 2 Batch 4000/13130] loss=0.8504, lr=0.0000066, metrics:accuracy:0.6029
INFO:root:21:24:27 [Epoch 3 Batch 5600/13130] loss=0.8930, lr=0.0000114, metrics:accuracy:0.5752
INFO:root:21:24:44 [Epoch 2 Batch 5200/13130] loss=0.8424, lr=0.0000080, metrics:accuracy:0.6086
INFO:root:21:24:45 [Epoch 3 Batch 6000/13130] loss=0.8871, lr=0.0000113, metrics:accuracy:0.5758
INFO:root:21:25:00 [Epoch 2 Batch 6000/13130] loss=0.8402, lr=0.0000157, metrics:accuracy:0.6171
INFO:root:21:25:01 [Epoch 2 Batch 2800/13130] loss=0.8927, lr=0.0000034, metrics:accuracy:0.5890
INFO:root:21:25:03 [Epoch 3 Batch 6400/13130] loss=0.8926, lr=0.0000112, metrics:accuracy:0.5766
INFO:root:21:25:05 [Epoch 2 Batch 4400/13130] loss=0.8432, lr=0.0000065, metrics:accuracy:0.6043
INFO:root:21:25:21 [Epoch 3 Batch 6800/13130] loss=0.9013, lr=0.0000110, metrics:accuracy:0.5763
INFO:root:21:25:22 [Epoch 2 Batch 5600/13130] loss=0.8246, lr=0.0000079, metrics:accuracy:0.6096
INFO:root:21:25:39 [Epoch 3 Batch 7200/13130] loss=0.8904, lr=0.0000109, metrics:accuracy:0.5768
INFO:root:21:25:39 [Epoch 2 Batch 3200/13130] loss=0.8785, lr=0.0000033, metrics:accuracy:0.5887
INFO:root:21:25:40 [Epoch 2 Batch 6400/13130] loss=0.8388, lr=0.0000156, metrics:accuracy:0.6174
INFO:root:21:25:45 [Epoch 2 Batch 4800/13130] loss=0.8668, lr=0.0000065, metrics:accuracy:0.6046
INFO:root:21:25:56 [Epoch 3 Batch 7600/13130] loss=0.8894, lr=0.0000108, metrics:accuracy:0.5770
INFO:root:21:26:01 [Epoch 2 Batch 6000/13130] loss=0.8528, lr=0.0000079, metrics:accuracy:0.6094
INFO:root:21:26:14 AMP: increasing loss scale to 2048.000000
INFO:root:21:26:14 [Epoch 3 Batch 8000/13130] loss=0.8782, lr=0.0000106, metrics:accuracy:0.5777
INFO:root:21:26:18 [Epoch 2 Batch 3600/13130] loss=0.8632, lr=0.0000033, metrics:accuracy:0.5891
INFO:root:21:26:19 [Epoch 2 Batch 6800/13130] loss=0.8558, lr=0.0000155, metrics:accuracy:0.6170
INFO:root:21:26:24 [Epoch 2 Batch 5200/13130] loss=0.8486, lr=0.0000064, metrics:accuracy:0.6052
INFO:root:21:26:32 [Epoch 3 Batch 8400/13130] loss=0.8982, lr=0.0000105, metrics:accuracy:0.5773
INFO:root:21:26:40 [Epoch 2 Batch 6400/13130] loss=0.8418, lr=0.0000078, metrics:accuracy:0.6099
INFO:root:21:26:50 [Epoch 3 Batch 8800/13130] loss=0.8906, lr=0.0000104, metrics:accuracy:0.5774
INFO:root:21:26:57 [Epoch 2 Batch 4000/13130] loss=0.8693, lr=0.0000033, metrics:accuracy:0.5887
INFO:root:21:26:58 [Epoch 2 Batch 7200/13130] loss=0.8309, lr=0.0000153, metrics:accuracy:0.6171
INFO:root:21:27:03 [Epoch 2 Batch 5600/13130] loss=0.8340, lr=0.0000064, metrics:accuracy:0.6058
INFO:root:21:27:08 [Epoch 3 Batch 9200/13130] loss=0.9021, lr=0.0000102, metrics:accuracy:0.5772
INFO:root:21:27:19 [Epoch 2 Batch 6800/13130] loss=0.8639, lr=0.0000077, metrics:accuracy:0.6091
INFO:root:21:27:26 [Epoch 3 Batch 9600/13130] loss=0.8960, lr=0.0000101, metrics:accuracy:0.5773
INFO:root:21:27:35 [Epoch 2 Batch 4400/13130] loss=0.8655, lr=0.0000033, metrics:accuracy:0.5901
INFO:root:21:27:37 [Epoch 2 Batch 7600/13130] loss=0.8347, lr=0.0000152, metrics:accuracy:0.6174
INFO:root:21:27:42 [Epoch 2 Batch 6000/13130] loss=0.8598, lr=0.0000063, metrics:accuracy:0.6058
INFO:root:21:27:44 [Epoch 3 Batch 10000/13130] loss=0.9039, lr=0.0000099, metrics:accuracy:0.5771
INFO:root:21:27:57 [Epoch 2 Batch 7200/13130] loss=0.8372, lr=0.0000077, metrics:accuracy:0.6097
INFO:root:21:28:02 [Epoch 3 Batch 10400/13130] loss=0.8990, lr=0.0000098, metrics:accuracy:0.5770
INFO:root:21:28:13 [Epoch 2 Batch 4800/13130] loss=0.8943, lr=0.0000032, metrics:accuracy:0.5897
INFO:root:21:28:16 [Epoch 2 Batch 8000/13130] loss=0.8325, lr=0.0000151, metrics:accuracy:0.6175
INFO:root:21:28:20 [Epoch 3 Batch 10800/13130] loss=0.9066, lr=0.0000097, metrics:accuracy:0.5766
INFO:root:21:28:21 [Epoch 2 Batch 6400/13130] loss=0.8465, lr=0.0000062, metrics:accuracy:0.6056
INFO:root:21:28:35 [Epoch 2 Batch 7600/13130] loss=0.8389, lr=0.0000076, metrics:accuracy:0.6102
INFO:root:21:28:39 [Epoch 3 Batch 11200/13130] loss=0.8839, lr=0.0000095, metrics:accuracy:0.5770
INFO:root:21:28:51 [Epoch 2 Batch 5200/13130] loss=0.8721, lr=0.0000032, metrics:accuracy:0.5902
INFO:root:21:28:55 [Epoch 2 Batch 8400/13130] loss=0.8330, lr=0.0000149, metrics:accuracy:0.6172
INFO:root:21:28:57 [Epoch 3 Batch 11600/13130] loss=0.8886, lr=0.0000094, metrics:accuracy:0.5770
INFO:root:21:29:00 [Epoch 2 Batch 6800/13130] loss=0.8716, lr=0.0000062, metrics:accuracy:0.6049
INFO:root:21:29:13 [Epoch 2 Batch 8000/13130] loss=0.8422, lr=0.0000075, metrics:accuracy:0.6102
INFO:root:21:29:15 AMP: increasing loss scale to 4096.000000
INFO:root:21:29:15 [Epoch 3 Batch 12000/13130] loss=0.8880, lr=0.0000093, metrics:accuracy:0.5772
INFO:root:21:29:29 [Epoch 2 Batch 5600/13130] loss=0.8587, lr=0.0000032, metrics:accuracy:0.5904
INFO:root:21:29:33 [Epoch 3 Batch 12400/13130] loss=0.8921, lr=0.0000091, metrics:accuracy:0.5772
INFO:root:21:29:34 [Epoch 2 Batch 8800/13130] loss=0.8409, lr=0.0000148, metrics:accuracy:0.6178
INFO:root:21:29:39 [Epoch 2 Batch 7200/13130] loss=0.8434, lr=0.0000061, metrics:accuracy:0.6059
INFO:root:21:29:51 [Epoch 3 Batch 12800/13130] loss=0.8930, lr=0.0000090, metrics:accuracy:0.5775
INFO:root:21:29:52 [Epoch 2 Batch 8400/13130] loss=0.8394, lr=0.0000075, metrics:accuracy:0.6104
INFO:root:21:30:06 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:30:08 [Epoch 2 Batch 6000/13130] loss=0.8835, lr=0.0000031, metrics:accuracy:0.5906
INFO:root:21:30:12 [Epoch 2 Batch 9200/13130] loss=0.8168, lr=0.0000147, metrics:accuracy:0.6185
INFO:root:21:30:14 [Batch 400/3750] loss=0.7880, metrics:accuracy:0.7369
INFO:root:21:30:18 [Epoch 2 Batch 7600/13130] loss=0.8498, lr=0.0000061, metrics:accuracy:0.6063
INFO:root:21:30:21 [Batch 800/3750] loss=0.7949, metrics:accuracy:0.7364
INFO:root:21:30:29 [Batch 1200/3750] loss=0.7941, metrics:accuracy:0.7343
INFO:root:21:30:31 [Epoch 2 Batch 8800/13130] loss=0.8494, lr=0.0000074, metrics:accuracy:0.6109
INFO:root:21:30:36 [Batch 1600/3750] loss=0.9493, metrics:accuracy:0.6752
INFO:root:21:30:44 [Batch 2000/3750] loss=0.9573, metrics:accuracy:0.6352
INFO:root:21:30:47 [Epoch 2 Batch 6400/13130] loss=0.8693, lr=0.0000031, metrics:accuracy:0.5905
INFO:root:21:30:51 [Epoch 2 Batch 9600/13130] loss=0.8305, lr=0.0000145, metrics:accuracy:0.6185
INFO:root:21:30:51 [Batch 2400/3750] loss=0.9628, metrics:accuracy:0.6099
INFO:root:21:30:56 [Epoch 2 Batch 8000/13130] loss=0.8483, lr=0.0000060, metrics:accuracy:0.6061
INFO:root:21:30:59 [Batch 2800/3750] loss=0.9341, metrics:accuracy:0.5973
INFO:root:21:31:07 [Batch 3200/3750] loss=0.9098, metrics:accuracy:0.5900
INFO:root:21:31:10 [Epoch 2 Batch 9200/13130] loss=0.8301, lr=0.0000073, metrics:accuracy:0.6115
INFO:root:21:31:14 [Batch 3600/3750] loss=0.9113, metrics:accuracy:0.5838
INFO:root:21:31:17 validation metrics:accuracy:0.5826
INFO:root:21:31:17 Time cost=71.12s, throughput=421.83 samples/s
INFO:root:21:31:19 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:21:31:19 Time cost=661.24s
INFO:root:21:31:26 [Epoch 2 Batch 6800/13130] loss=0.8926, lr=0.0000031, metrics:accuracy:0.5900
INFO:root:21:31:30 [Epoch 2 Batch 10000/13130] loss=0.8258, lr=0.0000144, metrics:accuracy:0.6190
INFO:root:21:31:35 [Epoch 2 Batch 8400/13130] loss=0.8436, lr=0.0000060, metrics:accuracy:0.6064
INFO:root:21:31:37 [Epoch 4 Batch 400/13130] loss=0.8916, lr=0.0000087, metrics:accuracy:0.5831
INFO:root:21:31:48 [Epoch 2 Batch 9600/13130] loss=0.8344, lr=0.0000073, metrics:accuracy:0.6113
INFO:root:21:31:56 [Epoch 4 Batch 800/13130] loss=0.8905, lr=0.0000086, metrics:accuracy:0.5803
INFO:root:21:32:04 [Epoch 2 Batch 7200/13130] loss=0.8654, lr=0.0000031, metrics:accuracy:0.5908
INFO:root:21:32:09 [Epoch 2 Batch 10400/13130] loss=0.8367, lr=0.0000143, metrics:accuracy:0.6193
INFO:root:21:32:14 [Epoch 4 Batch 1200/13130] loss=0.8712, lr=0.0000085, metrics:accuracy:0.5830
INFO:root:21:32:14 [Epoch 2 Batch 8800/13130] loss=0.8594, lr=0.0000059, metrics:accuracy:0.6068
INFO:root:21:32:27 [Epoch 2 Batch 10000/13130] loss=0.8311, lr=0.0000072, metrics:accuracy:0.6117
INFO:root:21:32:32 [Epoch 4 Batch 1600/13130] loss=0.8836, lr=0.0000083, metrics:accuracy:0.5813
INFO:root:21:32:42 [Epoch 2 Batch 7600/13130] loss=0.8724, lr=0.0000030, metrics:accuracy:0.5910
INFO:root:21:32:48 [Epoch 2 Batch 10800/13130] loss=0.8432, lr=0.0000141, metrics:accuracy:0.6192
INFO:root:21:32:50 [Epoch 4 Batch 2000/13130] loss=0.8836, lr=0.0000082, metrics:accuracy:0.5832
INFO:root:21:32:52 [Epoch 2 Batch 9200/13130] loss=0.8348, lr=0.0000059, metrics:accuracy:0.6074
INFO:root:21:33:04 [Epoch 2 Batch 10400/13130] loss=0.8425, lr=0.0000071, metrics:accuracy:0.6121
INFO:root:21:33:07 [Epoch 4 Batch 2400/13130] loss=0.8761, lr=0.0000081, metrics:accuracy:0.5840
INFO:root:21:33:20 [Epoch 2 Batch 8000/13130] loss=0.8703, lr=0.0000030, metrics:accuracy:0.5908
INFO:root:21:33:25 [Epoch 4 Batch 2800/13130] loss=0.8813, lr=0.0000079, metrics:accuracy:0.5854
INFO:root:21:33:26 [Epoch 2 Batch 11200/13130] loss=0.8429, lr=0.0000140, metrics:accuracy:0.6194
INFO:root:21:33:28 AMP: increasing loss scale to 8192.000000
INFO:root:21:33:31 [Epoch 2 Batch 9600/13130] loss=0.8446, lr=0.0000058, metrics:accuracy:0.6074
INFO:root:21:33:42 [Epoch 2 Batch 10800/13130] loss=0.8500, lr=0.0000071, metrics:accuracy:0.6123
INFO:root:21:33:43 [Epoch 4 Batch 3200/13130] loss=0.8834, lr=0.0000078, metrics:accuracy:0.5845
INFO:root:21:33:58 [Epoch 2 Batch 8400/13130] loss=0.8649, lr=0.0000030, metrics:accuracy:0.5908
INFO:root:21:34:01 [Epoch 4 Batch 3600/13130] loss=0.8756, lr=0.0000077, metrics:accuracy:0.5851
INFO:root:21:34:06 [Epoch 2 Batch 11600/13130] loss=0.8312, lr=0.0000138, metrics:accuracy:0.6198
INFO:root:21:34:11 [Epoch 2 Batch 10000/13130] loss=0.8355, lr=0.0000058, metrics:accuracy:0.6081
INFO:root:21:34:19 [Epoch 4 Batch 4000/13130] loss=0.8886, lr=0.0000075, metrics:accuracy:0.5847
INFO:root:21:34:20 [Epoch 2 Batch 11200/13130] loss=0.8543, lr=0.0000070, metrics:accuracy:0.6122
INFO:root:21:34:36 [Epoch 2 Batch 8800/13130] loss=0.8777, lr=0.0000030, metrics:accuracy:0.5911
INFO:root:21:34:37 [Epoch 4 Batch 4400/13130] loss=0.8669, lr=0.0000074, metrics:accuracy:0.5865
INFO:root:21:34:45 [Epoch 2 Batch 12000/13130] loss=0.8389, lr=0.0000137, metrics:accuracy:0.6198
INFO:root:21:34:50 [Epoch 2 Batch 10400/13130] loss=0.8476, lr=0.0000057, metrics:accuracy:0.6084
INFO:root:21:34:55 [Epoch 4 Batch 4800/13130] loss=0.8892, lr=0.0000073, metrics:accuracy:0.5863
INFO:root:21:34:59 [Epoch 2 Batch 11600/13130] loss=0.8359, lr=0.0000069, metrics:accuracy:0.6126
INFO:root:21:35:13 [Epoch 4 Batch 5200/13130] loss=0.8857, lr=0.0000071, metrics:accuracy:0.5867
INFO:root:21:35:15 [Epoch 2 Batch 9200/13130] loss=0.8566, lr=0.0000029, metrics:accuracy:0.5918
INFO:root:21:35:23 [Epoch 2 Batch 12400/13130] loss=0.8370, lr=0.0000136, metrics:accuracy:0.6198
INFO:root:21:35:28 [Epoch 2 Batch 10800/13130] loss=0.8583, lr=0.0000056, metrics:accuracy:0.6085
INFO:root:21:35:31 [Epoch 4 Batch 5600/13130] loss=0.8712, lr=0.0000070, metrics:accuracy:0.5876
INFO:root:21:35:38 [Epoch 2 Batch 12000/13130] loss=0.8423, lr=0.0000069, metrics:accuracy:0.6128
INFO:root:21:35:49 [Epoch 4 Batch 6000/13130] loss=0.8876, lr=0.0000069, metrics:accuracy:0.5875
INFO:root:21:35:53 [Epoch 2 Batch 9600/13130] loss=0.8579, lr=0.0000029, metrics:accuracy:0.5922
INFO:root:21:36:03 [Epoch 2 Batch 12800/13130] loss=0.8256, lr=0.0000134, metrics:accuracy:0.6201
INFO:root:21:36:06 [Epoch 2 Batch 11200/13130] loss=0.8591, lr=0.0000056, metrics:accuracy:0.6085
INFO:root:21:36:08 [Epoch 4 Batch 6400/13130] loss=0.9106, lr=0.0000067, metrics:accuracy:0.5863
INFO:root:21:36:16 [Epoch 2 Batch 12400/13130] loss=0.8407, lr=0.0000068, metrics:accuracy:0.6130
INFO:root:21:36:19 AMP: decreasing loss scale to 4096.000000
INFO:root:21:36:26 [Epoch 4 Batch 6800/13130] loss=0.8830, lr=0.0000066, metrics:accuracy:0.5860
INFO:root:21:36:31 [Epoch 2 Batch 10000/13130] loss=0.8535, lr=0.0000029, metrics:accuracy:0.5932
INFO:root:21:36:35 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:36:44 [Epoch 4 Batch 7200/13130] loss=0.8818, lr=0.0000064, metrics:accuracy:0.5862
INFO:root:21:36:45 [Epoch 2 Batch 11600/13130] loss=0.8433, lr=0.0000055, metrics:accuracy:0.6090
INFO:root:21:36:55 [Epoch 2 Batch 12800/13130] loss=0.8326, lr=0.0000067, metrics:accuracy:0.6134
INFO:root:21:36:56 [Batch 400/3750] loss=0.7786, metrics:accuracy:0.7441
INFO:root:21:37:02 [Epoch 4 Batch 7600/13130] loss=0.8801, lr=0.0000063, metrics:accuracy:0.5860
INFO:root:21:37:10 [Epoch 2 Batch 10400/13130] loss=0.8677, lr=0.0000029, metrics:accuracy:0.5938
INFO:root:21:37:17 [Batch 800/3750] loss=0.7911, metrics:accuracy:0.7445
INFO:root:21:37:20 [Epoch 2 Batch 12000/13130] loss=0.8522, lr=0.0000055, metrics:accuracy:0.6089
INFO:root:21:37:20 [Epoch 4 Batch 8000/13130] loss=0.8722, lr=0.0000062, metrics:accuracy:0.5860
INFO:root:21:37:27 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:37:37 [Batch 1200/3750] loss=0.7886, metrics:accuracy:0.7432
INFO:root:21:37:37 [Epoch 4 Batch 8400/13130] loss=0.8861, lr=0.0000060, metrics:accuracy:0.5856
INFO:root:21:37:46 [Epoch 2 Batch 10800/13130] loss=0.8799, lr=0.0000028, metrics:accuracy:0.5935
INFO:root:21:37:47 [Batch 400/3750] loss=0.7937, metrics:accuracy:0.7475
INFO:root:21:37:54 [Epoch 2 Batch 12400/13130] loss=0.8520, lr=0.0000054, metrics:accuracy:0.6087
INFO:root:21:37:55 [Epoch 4 Batch 8800/13130] loss=0.8913, lr=0.0000059, metrics:accuracy:0.5853
INFO:root:21:37:57 [Batch 1600/3750] loss=0.9117, metrics:accuracy:0.6857
INFO:root:21:38:07 [Batch 800/3750] loss=0.8089, metrics:accuracy:0.7480
INFO:root:21:38:13 [Epoch 4 Batch 9200/13130] loss=0.8724, lr=0.0000058, metrics:accuracy:0.5855
INFO:root:21:38:18 [Batch 2000/3750] loss=0.9124, metrics:accuracy:0.6472
INFO:root:21:38:20 [Epoch 2 Batch 11200/13130] loss=0.8752, lr=0.0000028, metrics:accuracy:0.5937
INFO:root:21:38:27 [Batch 1200/3750] loss=0.8061, metrics:accuracy:0.7426
INFO:root:21:38:29 [Epoch 2 Batch 12800/13130] loss=0.8344, lr=0.0000054, metrics:accuracy:0.6090
INFO:root:21:38:31 [Epoch 4 Batch 9600/13130] loss=0.8865, lr=0.0000056, metrics:accuracy:0.5852
INFO:root:21:38:39 [Batch 2400/3750] loss=0.9175, metrics:accuracy:0.6220
INFO:root:21:38:48 [Batch 1600/3750] loss=0.9042, metrics:accuracy:0.6833
INFO:root:21:38:50 [Epoch 4 Batch 10000/13130] loss=0.8693, lr=0.0000055, metrics:accuracy:0.5856
INFO:root:21:38:54 [Epoch 2 Batch 11600/13130] loss=0.8693, lr=0.0000028, metrics:accuracy:0.5940
INFO:root:21:38:57 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:39:00 [Batch 2800/3750] loss=0.9180, metrics:accuracy:0.6104
INFO:root:21:39:07 [Epoch 4 Batch 10400/13130] loss=0.8925, lr=0.0000054, metrics:accuracy:0.5854
INFO:root:21:39:09 [Batch 2000/3750] loss=0.8987, metrics:accuracy:0.6427
INFO:root:21:39:18 [Batch 400/3750] loss=0.7753, metrics:accuracy:0.7572
INFO:root:21:39:19 AMP: increasing loss scale to 8192.000000
INFO:root:21:39:20 [Batch 3200/3750] loss=0.9024, metrics:accuracy:0.6041
INFO:root:21:39:25 [Epoch 4 Batch 10800/13130] loss=0.8835, lr=0.0000052, metrics:accuracy:0.5856
INFO:root:21:39:29 [Epoch 2 Batch 12000/13130] loss=0.8725, lr=0.0000027, metrics:accuracy:0.5942
INFO:root:21:39:29 [Batch 2400/3750] loss=0.9136, metrics:accuracy:0.6159
INFO:root:21:39:38 [Batch 800/3750] loss=0.7864, metrics:accuracy:0.7580
INFO:root:21:39:41 [Batch 3600/3750] loss=0.8988, metrics:accuracy:0.5989
INFO:root:21:39:43 [Epoch 4 Batch 11200/13130] loss=0.8685, lr=0.0000051, metrics:accuracy:0.5856
INFO:root:21:39:48 validation metrics:accuracy:0.5976
INFO:root:21:39:48 Time cost=193.68s, throughput=154.89 samples/s
INFO:root:21:39:49 [Batch 2800/3750] loss=0.9057, metrics:accuracy:0.6062
INFO:root:21:39:50 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:21:39:50 Time cost=1457.00s
INFO:root:21:39:57 [Batch 1200/3750] loss=0.7825, metrics:accuracy:0.7552
INFO:root:21:40:01 [Epoch 4 Batch 11600/13130] loss=0.8712, lr=0.0000050, metrics:accuracy:0.5861
INFO:root:21:40:02 AMP: decreasing loss scale to 4096.000000
INFO:root:21:40:03 [Epoch 2 Batch 12400/13130] loss=0.8741, lr=0.0000027, metrics:accuracy:0.5940
INFO:root:21:40:09 [Batch 3200/3750] loss=0.8938, metrics:accuracy:0.6023
INFO:root:21:40:18 [Epoch 4 Batch 12000/13130] loss=0.8774, lr=0.0000048, metrics:accuracy:0.5862
INFO:root:21:40:18 [Batch 1600/3750] loss=0.9081, metrics:accuracy:0.6916
INFO:root:21:40:24 [Epoch 3 Batch 400/13130] loss=0.6931, lr=0.0000132, metrics:accuracy:0.7106
INFO:root:21:40:30 [Batch 3600/3750] loss=0.8898, metrics:accuracy:0.5993
INFO:root:21:40:35 [Epoch 4 Batch 12400/13130] loss=0.8789, lr=0.0000047, metrics:accuracy:0.5863
INFO:root:21:40:37 [Epoch 2 Batch 12800/13130] loss=0.8568, lr=0.0000027, metrics:accuracy:0.5943
INFO:root:21:40:37 validation metrics:accuracy:0.5993
INFO:root:21:40:37 Time cost=190.60s, throughput=157.40 samples/s
INFO:root:21:40:39 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:21:40:39 Time cost=1429.03s
INFO:root:21:40:39 [Batch 2000/3750] loss=0.9076, metrics:accuracy:0.6472
INFO:root:21:40:54 [Epoch 4 Batch 12800/13130] loss=0.8931, lr=0.0000045, metrics:accuracy:0.5861
INFO:root:21:40:59 [Epoch 3 Batch 800/13130] loss=0.7062, lr=0.0000131, metrics:accuracy:0.7020
INFO:root:21:41:00 [Batch 2400/3750] loss=0.9199, metrics:accuracy:0.6191
INFO:root:21:41:08 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:41:08 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:41:16 [Epoch 3 Batch 400/13130] loss=0.7494, lr=0.0000066, metrics:accuracy:0.6825
INFO:root:21:41:16 [Batch 400/3750] loss=0.7877, metrics:accuracy:0.7384
INFO:root:21:41:21 [Batch 2800/3750] loss=0.9211, metrics:accuracy:0.6074
INFO:root:21:41:24 [Batch 800/3750] loss=0.7957, metrics:accuracy:0.7373
INFO:root:21:41:28 [Batch 400/3750] loss=0.7890, metrics:accuracy:0.7453
INFO:root:21:41:31 [Batch 1200/3750] loss=0.7935, metrics:accuracy:0.7354
INFO:root:21:41:35 [Epoch 3 Batch 1200/13130] loss=0.7055, lr=0.0000129, metrics:accuracy:0.7014
INFO:root:21:41:39 [Batch 1600/3750] loss=0.9157, metrics:accuracy:0.6835
INFO:root:21:41:42 [Batch 3200/3750] loss=0.9098, metrics:accuracy:0.6014
INFO:root:21:41:46 [Batch 2000/3750] loss=0.9193, metrics:accuracy:0.6496
INFO:root:21:41:49 [Batch 800/3750] loss=0.7987, metrics:accuracy:0.7456
INFO:root:21:41:51 [Epoch 3 Batch 800/13130] loss=0.7574, lr=0.0000065, metrics:accuracy:0.6716
INFO:root:21:41:54 [Batch 2400/3750] loss=0.9252, metrics:accuracy:0.6268
INFO:root:21:42:01 [Batch 2800/3750] loss=0.9566, metrics:accuracy:0.6088
INFO:root:21:42:02 [Batch 3600/3750] loss=0.9121, metrics:accuracy:0.5958
INFO:root:21:42:08 [Batch 3200/3750] loss=0.9518, metrics:accuracy:0.5963
INFO:root:21:42:09 [Batch 1200/3750] loss=0.7948, metrics:accuracy:0.7436
INFO:root:21:42:10 [Epoch 3 Batch 1600/13130] loss=0.7076, lr=0.0000128, metrics:accuracy:0.7017
INFO:root:21:42:10 validation metrics:accuracy:0.5951
INFO:root:21:42:10 Time cost=193.09s, throughput=155.37 samples/s
INFO:root:21:42:12 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:21:42:12 Time cost=1456.14s
INFO:root:21:42:16 [Batch 3600/3750] loss=0.9530, metrics:accuracy:0.5858
INFO:root:21:42:19 validation metrics:accuracy:0.5832
INFO:root:21:42:19 Time cost=70.43s, throughput=425.94 samples/s
INFO:root:21:42:21 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:21:42:21 Time cost=661.64s
INFO:root:21:42:25 [Epoch 3 Batch 1200/13130] loss=0.7449, lr=0.0000065, metrics:accuracy:0.6733
INFO:root:21:42:30 [Batch 1600/3750] loss=0.8840, metrics:accuracy:0.6898
INFO:root:21:42:38 [Epoch 5 Batch 400/13130] loss=0.8775, lr=0.0000043, metrics:accuracy:0.5913
INFO:root:21:42:47 [Epoch 3 Batch 2000/13130] loss=0.6878, lr=0.0000127, metrics:accuracy:0.7034
INFO:root:21:42:50 [Batch 2000/3750] loss=0.8829, metrics:accuracy:0.6519
INFO:root:21:42:51 [Epoch 3 Batch 400/13130] loss=0.7684, lr=0.0000053, metrics:accuracy:0.6753
INFO:root:21:42:55 [Epoch 5 Batch 800/13130] loss=0.8928, lr=0.0000042, metrics:accuracy:0.5877
INFO:root:21:43:00 [Epoch 3 Batch 1600/13130] loss=0.7626, lr=0.0000064, metrics:accuracy:0.6703
INFO:root:21:43:11 [Batch 2400/3750] loss=0.8931, metrics:accuracy:0.6272
INFO:root:21:43:13 [Epoch 5 Batch 1200/13130] loss=0.8888, lr=0.0000040, metrics:accuracy:0.5880
INFO:root:21:43:26 [Epoch 3 Batch 2400/13130] loss=0.6747, lr=0.0000125, metrics:accuracy:0.7052
INFO:root:21:43:30 [Epoch 3 Batch 800/13130] loss=0.7746, lr=0.0000052, metrics:accuracy:0.6631
INFO:root:21:43:31 [Epoch 5 Batch 1600/13130] loss=0.8980, lr=0.0000039, metrics:accuracy:0.5856
INFO:root:21:43:32 [Batch 2800/3750] loss=0.9368, metrics:accuracy:0.6117
INFO:root:21:43:33 [Epoch 3 Batch 2000/13130] loss=0.7456, lr=0.0000063, metrics:accuracy:0.6706
INFO:root:21:43:49 [Epoch 5 Batch 2000/13130] loss=0.8821, lr=0.0000038, metrics:accuracy:0.5847
INFO:root:21:43:52 [Batch 3200/3750] loss=0.9411, metrics:accuracy:0.6019
INFO:root:21:44:05 [Epoch 3 Batch 2800/13130] loss=0.6710, lr=0.0000124, metrics:accuracy:0.7069
INFO:root:21:44:06 [Epoch 5 Batch 2400/13130] loss=0.8745, lr=0.0000036, metrics:accuracy:0.5856
INFO:root:21:44:07 [Epoch 3 Batch 2400/13130] loss=0.7348, lr=0.0000063, metrics:accuracy:0.6718
INFO:root:21:44:09 [Epoch 3 Batch 1200/13130] loss=0.7692, lr=0.0000052, metrics:accuracy:0.6628
INFO:root:21:44:11 AMP: increasing loss scale to 8192.000000
INFO:root:21:44:13 [Batch 3600/3750] loss=0.9403, metrics:accuracy:0.5938
INFO:root:21:44:20 validation metrics:accuracy:0.5920
INFO:root:21:44:20 Time cost=192.71s, throughput=155.68 samples/s
INFO:root:21:44:22 params saved in: ./output_dir/model_bert_weibo3_1.params
INFO:root:21:44:22 Time cost=1430.82s
INFO:root:21:44:25 [Epoch 5 Batch 2800/13130] loss=0.8837, lr=0.0000035, metrics:accuracy:0.5851
INFO:root:21:44:42 [Epoch 3 Batch 2800/13130] loss=0.7165, lr=0.0000062, metrics:accuracy:0.6736
INFO:root:21:44:43 [Epoch 5 Batch 3200/13130] loss=0.8695, lr=0.0000034, metrics:accuracy:0.5868
INFO:root:21:44:43 [Epoch 3 Batch 3200/13130] loss=0.6804, lr=0.0000122, metrics:accuracy:0.7063
INFO:root:21:44:48 [Epoch 3 Batch 1600/13130] loss=0.7815, lr=0.0000051, metrics:accuracy:0.6595
INFO:root:21:44:49 AMP: decreasing loss scale to 4096.000000
INFO:root:21:45:00 [Epoch 3 Batch 400/13130] loss=0.8224, lr=0.0000026, metrics:accuracy:0.6397
INFO:root:21:45:01 [Epoch 5 Batch 3600/13130] loss=0.8714, lr=0.0000032, metrics:accuracy:0.5875
INFO:root:21:45:19 [Epoch 5 Batch 4000/13130] loss=0.8738, lr=0.0000031, metrics:accuracy:0.5883
INFO:root:21:45:20 [Epoch 3 Batch 3200/13130] loss=0.7271, lr=0.0000061, metrics:accuracy:0.6756
INFO:root:21:45:22 [Epoch 3 Batch 3600/13130] loss=0.7086, lr=0.0000121, metrics:accuracy:0.7051
INFO:root:21:45:26 [Epoch 3 Batch 2000/13130] loss=0.7611, lr=0.0000051, metrics:accuracy:0.6612
INFO:root:21:45:37 [Epoch 5 Batch 4400/13130] loss=0.8855, lr=0.0000029, metrics:accuracy:0.5882
INFO:root:21:45:39 [Epoch 3 Batch 800/13130] loss=0.8341, lr=0.0000026, metrics:accuracy:0.6245
INFO:root:21:45:55 [Epoch 5 Batch 4800/13130] loss=0.8642, lr=0.0000028, metrics:accuracy:0.5893
INFO:root:21:45:59 [Epoch 3 Batch 3600/13130] loss=0.7601, lr=0.0000061, metrics:accuracy:0.6740
INFO:root:21:46:01 [Epoch 3 Batch 4000/13130] loss=0.6963, lr=0.0000120, metrics:accuracy:0.7049
INFO:root:21:46:04 [Epoch 3 Batch 2400/13130] loss=0.7550, lr=0.0000050, metrics:accuracy:0.6617
INFO:root:21:46:13 [Epoch 5 Batch 5200/13130] loss=0.8765, lr=0.0000027, metrics:accuracy:0.5895
INFO:root:21:46:13 AMP: decreasing loss scale to 2048.000000
INFO:root:21:46:17 [Epoch 3 Batch 1200/13130] loss=0.8245, lr=0.0000026, metrics:accuracy:0.6233
INFO:root:21:46:30 [Epoch 5 Batch 5600/13130] loss=0.8656, lr=0.0000025, metrics:accuracy:0.5907
INFO:root:21:46:37 [Epoch 3 Batch 4000/13130] loss=0.7397, lr=0.0000060, metrics:accuracy:0.6745
INFO:root:21:46:39 [Epoch 3 Batch 4400/13130] loss=0.6801, lr=0.0000118, metrics:accuracy:0.7053
INFO:root:21:46:43 [Epoch 3 Batch 2800/13130] loss=0.7421, lr=0.0000050, metrics:accuracy:0.6631
INFO:root:21:46:48 [Epoch 5 Batch 6000/13130] loss=0.8797, lr=0.0000024, metrics:accuracy:0.5907
INFO:root:21:46:55 [Epoch 3 Batch 1600/13130] loss=0.8381, lr=0.0000026, metrics:accuracy:0.6225
INFO:root:21:47:06 [Epoch 5 Batch 6400/13130] loss=0.8758, lr=0.0000023, metrics:accuracy:0.5910
INFO:root:21:47:15 [Epoch 3 Batch 4400/13130] loss=0.7333, lr=0.0000059, metrics:accuracy:0.6754
INFO:root:21:47:18 [Epoch 3 Batch 4800/13130] loss=0.6779, lr=0.0000117, metrics:accuracy:0.7053
INFO:root:21:47:22 [Epoch 3 Batch 3200/13130] loss=0.7470, lr=0.0000049, metrics:accuracy:0.6652
INFO:root:21:47:23 [Epoch 5 Batch 6800/13130] loss=0.8941, lr=0.0000021, metrics:accuracy:0.5901
INFO:root:21:47:33 [Epoch 3 Batch 2000/13130] loss=0.8178, lr=0.0000025, metrics:accuracy:0.6241
INFO:root:21:47:41 [Epoch 5 Batch 7200/13130] loss=0.8781, lr=0.0000020, metrics:accuracy:0.5898
INFO:root:21:47:54 [Epoch 3 Batch 4800/13130] loss=0.7320, lr=0.0000059, metrics:accuracy:0.6750
INFO:root:21:47:57 [Epoch 3 Batch 5200/13130] loss=0.6958, lr=0.0000116, metrics:accuracy:0.7054
INFO:root:21:47:58 [Epoch 5 Batch 7600/13130] loss=0.8770, lr=0.0000019, metrics:accuracy:0.5897
INFO:root:21:48:01 [Epoch 3 Batch 3600/13130] loss=0.7819, lr=0.0000048, metrics:accuracy:0.6643
INFO:root:21:48:11 [Epoch 3 Batch 2400/13130] loss=0.8139, lr=0.0000025, metrics:accuracy:0.6249
INFO:root:21:48:15 [Epoch 5 Batch 8000/13130] loss=0.8550, lr=0.0000017, metrics:accuracy:0.5903
INFO:root:21:48:32 [Epoch 3 Batch 5200/13130] loss=0.7414, lr=0.0000058, metrics:accuracy:0.6752
INFO:root:21:48:32 [Epoch 5 Batch 8400/13130] loss=0.8820, lr=0.0000016, metrics:accuracy:0.5897
INFO:root:21:48:36 [Epoch 3 Batch 5600/13130] loss=0.6950, lr=0.0000114, metrics:accuracy:0.7055
INFO:root:21:48:40 [Epoch 3 Batch 4000/13130] loss=0.7651, lr=0.0000048, metrics:accuracy:0.6644
INFO:root:21:48:50 [Epoch 3 Batch 2800/13130] loss=0.8019, lr=0.0000025, metrics:accuracy:0.6263
INFO:root:21:48:50 [Epoch 5 Batch 8800/13130] loss=0.8749, lr=0.0000015, metrics:accuracy:0.5896
INFO:root:21:49:07 [Epoch 5 Batch 9200/13130] loss=0.8875, lr=0.0000013, metrics:accuracy:0.5892
INFO:root:21:49:07 AMP: increasing loss scale to 4096.000000
INFO:root:21:49:11 [Epoch 3 Batch 5600/13130] loss=0.7404, lr=0.0000057, metrics:accuracy:0.6752
INFO:root:21:49:15 [Epoch 3 Batch 6000/13130] loss=0.6795, lr=0.0000113, metrics:accuracy:0.7058
INFO:root:21:49:19 [Epoch 3 Batch 4400/13130] loss=0.7531, lr=0.0000047, metrics:accuracy:0.6649
INFO:root:21:49:25 [Epoch 5 Batch 9600/13130] loss=0.9003, lr=0.0000012, metrics:accuracy:0.5885
INFO:root:21:49:28 [Epoch 3 Batch 3200/13130] loss=0.8094, lr=0.0000024, metrics:accuracy:0.6284
INFO:root:21:49:43 [Epoch 5 Batch 10000/13130] loss=0.8685, lr=0.0000011, metrics:accuracy:0.5885
INFO:root:21:49:49 [Epoch 3 Batch 6000/13130] loss=0.7234, lr=0.0000056, metrics:accuracy:0.6758
INFO:root:21:49:55 [Epoch 3 Batch 6400/13130] loss=0.6928, lr=0.0000112, metrics:accuracy:0.7060
INFO:root:21:49:58 [Epoch 3 Batch 4800/13130] loss=0.7561, lr=0.0000047, metrics:accuracy:0.6648
INFO:root:21:50:00 [Epoch 5 Batch 10400/13130] loss=0.8826, lr=0.0000009, metrics:accuracy:0.5885
INFO:root:21:50:06 [Epoch 3 Batch 3600/13130] loss=0.8323, lr=0.0000024, metrics:accuracy:0.6281
INFO:root:21:50:18 [Epoch 5 Batch 10800/13130] loss=0.8887, lr=0.0000008, metrics:accuracy:0.5881
INFO:root:21:50:27 [Epoch 3 Batch 6400/13130] loss=0.7470, lr=0.0000056, metrics:accuracy:0.6760
INFO:root:21:50:34 [Epoch 3 Batch 6800/13130] loss=0.7007, lr=0.0000110, metrics:accuracy:0.7057
INFO:root:21:50:36 [Epoch 5 Batch 11200/13130] loss=0.8901, lr=0.0000006, metrics:accuracy:0.5878
INFO:root:21:50:37 [Epoch 3 Batch 5200/13130] loss=0.7652, lr=0.0000046, metrics:accuracy:0.6648
INFO:root:21:50:44 [Epoch 3 Batch 4000/13130] loss=0.8229, lr=0.0000024, metrics:accuracy:0.6280
INFO:root:21:50:53 [Epoch 5 Batch 11600/13130] loss=0.8659, lr=0.0000005, metrics:accuracy:0.5882
INFO:root:21:51:05 [Epoch 3 Batch 6800/13130] loss=0.7410, lr=0.0000055, metrics:accuracy:0.6757
INFO:root:21:51:10 [Epoch 5 Batch 12000/13130] loss=0.8909, lr=0.0000004, metrics:accuracy:0.5878
INFO:root:21:51:13 [Epoch 3 Batch 7200/13130] loss=0.6775, lr=0.0000109, metrics:accuracy:0.7059
INFO:root:21:51:17 [Epoch 3 Batch 5600/13130] loss=0.7659, lr=0.0000046, metrics:accuracy:0.6644
INFO:root:21:51:23 [Epoch 3 Batch 4400/13130] loss=0.8134, lr=0.0000024, metrics:accuracy:0.6286
INFO:root:21:51:28 [Epoch 5 Batch 12400/13130] loss=0.8924, lr=0.0000002, metrics:accuracy:0.5876
INFO:root:21:51:44 [Epoch 3 Batch 7200/13130] loss=0.7263, lr=0.0000054, metrics:accuracy:0.6760
INFO:root:21:51:45 [Epoch 5 Batch 12800/13130] loss=0.8725, lr=0.0000001, metrics:accuracy:0.5876
INFO:root:21:51:51 [Epoch 3 Batch 7600/13130] loss=0.6936, lr=0.0000108, metrics:accuracy:0.7056
INFO:root:21:51:55 [Epoch 3 Batch 6000/13130] loss=0.7510, lr=0.0000045, metrics:accuracy:0.6649
INFO:root:21:51:59 Finish training step: 32812
INFO:root:21:51:59 Now we are doing evaluation on dev with gpu(0).
INFO:root:21:52:02 [Epoch 3 Batch 4800/13130] loss=0.8180, lr=0.0000023, metrics:accuracy:0.6287
INFO:root:21:52:07 [Batch 400/3750] loss=0.7835, metrics:accuracy:0.7422
INFO:root:21:52:14 [Batch 800/3750] loss=0.7906, metrics:accuracy:0.7411
INFO:root:21:52:21 [Batch 1200/3750] loss=0.7886, metrics:accuracy:0.7383
INFO:root:21:52:23 [Epoch 3 Batch 7600/13130] loss=0.7394, lr=0.0000054, metrics:accuracy:0.6755
INFO:root:21:52:29 [Batch 1600/3750] loss=0.9472, metrics:accuracy:0.6796
INFO:root:21:52:31 [Epoch 3 Batch 8000/13130] loss=0.6857, lr=0.0000106, metrics:accuracy:0.7054
INFO:root:21:52:35 [Epoch 3 Batch 6400/13130] loss=0.7739, lr=0.0000045, metrics:accuracy:0.6648
INFO:root:21:52:36 [Batch 2000/3750] loss=0.9549, metrics:accuracy:0.6405
INFO:root:21:52:40 [Epoch 3 Batch 5200/13130] loss=0.8239, lr=0.0000023, metrics:accuracy:0.6286
INFO:root:21:52:44 [Batch 2400/3750] loss=0.9612, metrics:accuracy:0.6152
INFO:root:21:52:51 [Batch 2800/3750] loss=0.9415, metrics:accuracy:0.6005
INFO:root:21:52:58 [Batch 3200/3750] loss=0.9196, metrics:accuracy:0.5918
INFO:root:21:53:02 [Epoch 3 Batch 8000/13130] loss=0.7336, lr=0.0000053, metrics:accuracy:0.6752
INFO:root:21:53:05 [Batch 3600/3750] loss=0.9204, metrics:accuracy:0.5848
INFO:root:21:53:08 validation metrics:accuracy:0.5833
INFO:root:21:53:08 Time cost=68.89s, throughput=435.45 samples/s
INFO:root:21:53:09 [Epoch 3 Batch 8400/13130] loss=0.7038, lr=0.0000105, metrics:accuracy:0.7053
INFO:root:21:53:10 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:21:53:10 Time cost=649.21s
INFO:root:21:53:10 Best model at epoch 4. Validation metrics:accuracy:0.5833
INFO:root:21:53:10 Now we are doing testing on test with gpu(0).
INFO:root:21:53:12 [Epoch 3 Batch 6800/13130] loss=0.7629, lr=0.0000044, metrics:accuracy:0.6645
INFO:root:21:53:19 [Epoch 3 Batch 5600/13130] loss=0.8264, lr=0.0000023, metrics:accuracy:0.6279
INFO:root:21:53:40 [Epoch 3 Batch 8400/13130] loss=0.7502, lr=0.0000052, metrics:accuracy:0.6751
INFO:root:21:53:44 Time cost=33.82s, throughput=443.56 samples/s
INFO:root:21:53:49 [Epoch 3 Batch 8800/13130] loss=0.6822, lr=0.0000104, metrics:accuracy:0.7055
INFO:root:21:53:52 [Epoch 3 Batch 7200/13130] loss=0.7488, lr=0.0000044, metrics:accuracy:0.6644
INFO:root:21:53:57 [Epoch 3 Batch 6000/13130] loss=0.8081, lr=0.0000023, metrics:accuracy:0.6279
INFO:root:21:54:18 [Epoch 3 Batch 8800/13130] loss=0.7388, lr=0.0000052, metrics:accuracy:0.6753
INFO:root:21:54:27 [Epoch 3 Batch 9200/13130] loss=0.6912, lr=0.0000102, metrics:accuracy:0.7055
INFO:root:21:54:30 [Epoch 3 Batch 7600/13130] loss=0.7618, lr=0.0000043, metrics:accuracy:0.6640
INFO:root:21:54:35 [Epoch 3 Batch 6400/13130] loss=0.8275, lr=0.0000022, metrics:accuracy:0.6280
INFO:root:21:54:56 [Epoch 3 Batch 9200/13130] loss=0.7456, lr=0.0000051, metrics:accuracy:0.6750
INFO:root:21:55:06 [Epoch 3 Batch 9600/13130] loss=0.6985, lr=0.0000101, metrics:accuracy:0.7055
INFO:root:21:55:10 [Epoch 3 Batch 8000/13130] loss=0.7542, lr=0.0000042, metrics:accuracy:0.6642
INFO:root:21:55:13 [Epoch 3 Batch 6800/13130] loss=0.8227, lr=0.0000022, metrics:accuracy:0.6278
INFO:root:21:55:35 [Epoch 3 Batch 9600/13130] loss=0.7501, lr=0.0000050, metrics:accuracy:0.6750
INFO:root:21:55:45 [Epoch 3 Batch 10000/13130] loss=0.7004, lr=0.0000099, metrics:accuracy:0.7055
INFO:root:21:55:48 [Epoch 3 Batch 8400/13130] loss=0.7753, lr=0.0000042, metrics:accuracy:0.6638
INFO:root:21:55:52 [Epoch 3 Batch 7200/13130] loss=0.8149, lr=0.0000022, metrics:accuracy:0.6276
INFO:root:21:56:13 [Epoch 3 Batch 10000/13130] loss=0.7473, lr=0.0000050, metrics:accuracy:0.6753
INFO:root:21:56:24 [Epoch 3 Batch 10400/13130] loss=0.7023, lr=0.0000098, metrics:accuracy:0.7054
INFO:root:21:56:27 [Epoch 3 Batch 8800/13130] loss=0.7587, lr=0.0000041, metrics:accuracy:0.6639
INFO:root:21:56:31 [Epoch 3 Batch 7600/13130] loss=0.8216, lr=0.0000022, metrics:accuracy:0.6272
INFO:root:21:56:51 [Epoch 3 Batch 10400/13130] loss=0.7519, lr=0.0000049, metrics:accuracy:0.6750
INFO:root:21:57:03 [Epoch 3 Batch 10800/13130] loss=0.7095, lr=0.0000097, metrics:accuracy:0.7050
INFO:root:21:57:06 [Epoch 3 Batch 9200/13130] loss=0.7679, lr=0.0000041, metrics:accuracy:0.6637
INFO:root:21:57:09 [Epoch 3 Batch 8000/13130] loss=0.8025, lr=0.0000021, metrics:accuracy:0.6277
INFO:root:21:57:30 [Epoch 3 Batch 10800/13130] loss=0.7547, lr=0.0000048, metrics:accuracy:0.6749
INFO:root:21:57:42 [Epoch 3 Batch 11200/13130] loss=0.6805, lr=0.0000095, metrics:accuracy:0.7055
INFO:root:21:57:45 [Epoch 3 Batch 9600/13130] loss=0.7636, lr=0.0000040, metrics:accuracy:0.6638
INFO:root:21:57:47 [Epoch 3 Batch 8400/13130] loss=0.8291, lr=0.0000021, metrics:accuracy:0.6275
INFO:root:21:58:08 [Epoch 3 Batch 11200/13130] loss=0.7318, lr=0.0000048, metrics:accuracy:0.6752
INFO:root:21:58:22 [Epoch 3 Batch 11600/13130] loss=0.6864, lr=0.0000094, metrics:accuracy:0.7055
INFO:root:21:58:24 [Epoch 3 Batch 10000/13130] loss=0.7641, lr=0.0000040, metrics:accuracy:0.6638
INFO:root:21:58:26 [Epoch 3 Batch 8800/13130] loss=0.8234, lr=0.0000021, metrics:accuracy:0.6270
INFO:root:21:58:47 [Epoch 3 Batch 11600/13130] loss=0.7462, lr=0.0000047, metrics:accuracy:0.6751
INFO:root:21:59:01 [Epoch 3 Batch 12000/13130] loss=0.6699, lr=0.0000093, metrics:accuracy:0.7055
INFO:root:21:59:03 [Epoch 3 Batch 10400/13130] loss=0.7789, lr=0.0000039, metrics:accuracy:0.6637
INFO:root:21:59:04 [Epoch 3 Batch 9200/13130] loss=0.8280, lr=0.0000020, metrics:accuracy:0.6265
INFO:root:21:59:26 [Epoch 3 Batch 12000/13130] loss=0.7255, lr=0.0000046, metrics:accuracy:0.6752
INFO:root:21:59:40 [Epoch 3 Batch 12400/13130] loss=0.6959, lr=0.0000091, metrics:accuracy:0.7054
INFO:root:21:59:42 [Epoch 3 Batch 10800/13130] loss=0.7773, lr=0.0000039, metrics:accuracy:0.6636
INFO:root:21:59:43 [Epoch 3 Batch 9600/13130] loss=0.8274, lr=0.0000020, metrics:accuracy:0.6266
INFO:root:22:00:04 [Epoch 3 Batch 12400/13130] loss=0.7548, lr=0.0000046, metrics:accuracy:0.6747
INFO:root:22:00:18 [Epoch 3 Batch 12800/13130] loss=0.6710, lr=0.0000090, metrics:accuracy:0.7059
INFO:root:22:00:21 [Epoch 3 Batch 11200/13130] loss=0.7533, lr=0.0000038, metrics:accuracy:0.6638
INFO:root:22:00:21 [Epoch 3 Batch 10000/13130] loss=0.8309, lr=0.0000020, metrics:accuracy:0.6265
INFO:root:22:00:42 [Epoch 3 Batch 12800/13130] loss=0.7325, lr=0.0000045, metrics:accuracy:0.6751
INFO:root:22:00:50 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:00:59 [Epoch 3 Batch 11600/13130] loss=0.7723, lr=0.0000038, metrics:accuracy:0.6636
INFO:root:22:01:00 [Epoch 3 Batch 10400/13130] loss=0.8368, lr=0.0000020, metrics:accuracy:0.6263
INFO:root:22:01:11 [Batch 400/3750] loss=0.9340, metrics:accuracy:0.6794
INFO:root:22:01:14 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:01:32 [Batch 800/3750] loss=0.9383, metrics:accuracy:0.6773
INFO:root:22:01:34 [Epoch 3 Batch 12000/13130] loss=0.7416, lr=0.0000037, metrics:accuracy:0.6638
INFO:root:22:01:35 [Batch 400/3750] loss=0.8741, metrics:accuracy:0.6887
INFO:root:22:01:36 [Epoch 3 Batch 10800/13130] loss=0.8385, lr=0.0000019, metrics:accuracy:0.6261
INFO:root:22:01:53 [Batch 1200/3750] loss=0.9658, metrics:accuracy:0.6706
INFO:root:22:01:55 [Batch 800/3750] loss=0.8770, metrics:accuracy:0.6884
INFO:root:22:02:08 [Epoch 3 Batch 12400/13130] loss=0.7701, lr=0.0000037, metrics:accuracy:0.6636
INFO:root:22:02:10 [Epoch 3 Batch 11200/13130] loss=0.8166, lr=0.0000019, metrics:accuracy:0.6261
INFO:root:22:02:13 [Batch 1600/3750] loss=1.0118, metrics:accuracy:0.6348
INFO:root:22:02:15 [Batch 1200/3750] loss=0.8921, metrics:accuracy:0.6829
INFO:root:22:02:34 [Batch 2000/3750] loss=0.9919, metrics:accuracy:0.6113
INFO:root:22:02:36 [Batch 1600/3750] loss=0.9610, metrics:accuracy:0.6434
INFO:root:22:02:43 [Epoch 3 Batch 12800/13130] loss=0.7575, lr=0.0000036, metrics:accuracy:0.6636
INFO:root:22:02:45 [Epoch 3 Batch 11600/13130] loss=0.8258, lr=0.0000019, metrics:accuracy:0.6262
INFO:root:22:02:55 [Batch 2400/3750] loss=1.0022, metrics:accuracy:0.5960
INFO:root:22:02:57 [Batch 2000/3750] loss=0.9457, metrics:accuracy:0.6178
INFO:root:22:03:11 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:03:16 [Batch 2800/3750] loss=0.9529, metrics:accuracy:0.5916
INFO:root:22:03:18 [Batch 2400/3750] loss=0.9598, metrics:accuracy:0.5999
INFO:root:22:03:20 [Epoch 3 Batch 12000/13130] loss=0.8033, lr=0.0000019, metrics:accuracy:0.6267
INFO:root:22:03:31 [Batch 400/3750] loss=0.8800, metrics:accuracy:0.6903
INFO:root:22:03:36 [Batch 3200/3750] loss=0.9109, metrics:accuracy:0.5938
INFO:root:22:03:38 [Batch 2800/3750] loss=0.9158, metrics:accuracy:0.5951
INFO:root:22:03:52 [Batch 800/3750] loss=0.8832, metrics:accuracy:0.6933
INFO:root:22:03:54 [Epoch 3 Batch 12400/13130] loss=0.8269, lr=0.0000018, metrics:accuracy:0.6265
INFO:root:22:03:56 [Batch 3600/3750] loss=0.9203, metrics:accuracy:0.5943
INFO:root:22:03:58 [Batch 3200/3750] loss=0.8834, metrics:accuracy:0.5959
INFO:root:22:04:04 validation metrics:accuracy:0.5949
INFO:root:22:04:04 Time cost=193.56s, throughput=154.99 samples/s
INFO:root:22:04:06 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:22:04:06 Time cost=1455.60s
INFO:root:22:04:11 [Batch 1200/3750] loss=0.8932, metrics:accuracy:0.6878
INFO:root:22:04:18 [Batch 3600/3750] loss=0.8794, metrics:accuracy:0.5963
INFO:root:22:04:26 validation metrics:accuracy:0.5974
INFO:root:22:04:26 Time cost=191.89s, throughput=156.34 samples/s
INFO:root:22:04:27 [Epoch 3 Batch 12800/13130] loss=0.8176, lr=0.0000018, metrics:accuracy:0.6267
INFO:root:22:04:28 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:22:04:28 Time cost=1428.97s
INFO:root:22:04:32 [Batch 1600/3750] loss=0.9477, metrics:accuracy:0.6457
INFO:root:22:04:40 [Epoch 4 Batch 400/13130] loss=0.5507, lr=0.0000087, metrics:accuracy:0.7803
INFO:root:22:04:53 [Batch 2000/3750] loss=0.9335, metrics:accuracy:0.6193
INFO:root:22:04:58 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:05:05 [Epoch 4 Batch 400/13130] loss=0.6612, lr=0.0000044, metrics:accuracy:0.7212
INFO:root:22:05:13 [Batch 2400/3750] loss=0.9490, metrics:accuracy:0.6011
INFO:root:22:05:14 [Epoch 4 Batch 800/13130] loss=0.5367, lr=0.0000086, metrics:accuracy:0.7794
INFO:root:22:05:19 [Batch 400/3750] loss=0.8402, metrics:accuracy:0.7094
INFO:root:22:05:34 [Batch 2800/3750] loss=0.9109, metrics:accuracy:0.5958
INFO:root:22:05:39 [Batch 800/3750] loss=0.8449, metrics:accuracy:0.7080
INFO:root:22:05:40 [Epoch 4 Batch 800/13130] loss=0.6517, lr=0.0000043, metrics:accuracy:0.7173
INFO:root:22:05:49 [Epoch 4 Batch 1200/13130] loss=0.5274, lr=0.0000085, metrics:accuracy:0.7807
INFO:root:22:05:55 [Batch 3200/3750] loss=0.8825, metrics:accuracy:0.5950
INFO:root:22:05:59 [Batch 1200/3750] loss=0.8474, metrics:accuracy:0.7054
INFO:root:22:06:14 [Epoch 4 Batch 1200/13130] loss=0.6324, lr=0.0000042, metrics:accuracy:0.7227
INFO:root:22:06:16 [Batch 3600/3750] loss=0.8836, metrics:accuracy:0.5952
INFO:root:22:06:20 [Batch 1600/3750] loss=0.9430, metrics:accuracy:0.6538
INFO:root:22:06:23 validation metrics:accuracy:0.5962
INFO:root:22:06:23 Time cost=192.00s, throughput=156.25 samples/s
INFO:root:22:06:24 [Epoch 4 Batch 1600/13130] loss=0.5304, lr=0.0000083, metrics:accuracy:0.7813
INFO:root:22:06:25 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:22:06:25 Time cost=1452.94s
INFO:root:22:06:40 [Batch 2000/3750] loss=0.9392, metrics:accuracy:0.6209
INFO:root:22:06:49 [Epoch 4 Batch 1600/13130] loss=0.6332, lr=0.0000042, metrics:accuracy:0.7233
INFO:root:22:07:01 [Batch 2400/3750] loss=0.9523, metrics:accuracy:0.5978
INFO:root:22:07:01 [Epoch 4 Batch 2000/13130] loss=0.5161, lr=0.0000082, metrics:accuracy:0.7823
INFO:root:22:07:03 [Epoch 4 Batch 400/13130] loss=0.6978, lr=0.0000035, metrics:accuracy:0.7031
INFO:root:22:07:21 [Batch 2800/3750] loss=0.8860, metrics:accuracy:0.5930
INFO:root:22:07:23 [Epoch 4 Batch 2000/13130] loss=0.6348, lr=0.0000041, metrics:accuracy:0.7240
INFO:root:22:07:41 [Epoch 4 Batch 2400/13130] loss=0.5269, lr=0.0000081, metrics:accuracy:0.7830
INFO:root:22:07:41 [Batch 3200/3750] loss=0.8531, metrics:accuracy:0.5924
INFO:root:22:07:42 [Epoch 4 Batch 800/13130] loss=0.6896, lr=0.0000034, metrics:accuracy:0.6987
INFO:root:22:07:59 [Epoch 4 Batch 2400/13130] loss=0.6415, lr=0.0000040, metrics:accuracy:0.7240
INFO:root:22:08:02 [Batch 3600/3750] loss=0.8491, metrics:accuracy:0.5922
INFO:root:22:08:10 validation metrics:accuracy:0.5930
INFO:root:22:08:10 Time cost=191.57s, throughput=156.60 samples/s
INFO:root:22:08:11 params saved in: ./output_dir/model_bert_weibo3_2.params
INFO:root:22:08:11 Time cost=1429.14s
INFO:root:22:08:20 [Epoch 4 Batch 2800/13130] loss=0.5242, lr=0.0000079, metrics:accuracy:0.7849
INFO:root:22:08:21 [Epoch 4 Batch 1200/13130] loss=0.6672, lr=0.0000034, metrics:accuracy:0.7030
INFO:root:22:08:34 [Epoch 4 Batch 2800/13130] loss=0.6291, lr=0.0000040, metrics:accuracy:0.7254
INFO:root:22:08:49 [Epoch 4 Batch 400/13130] loss=0.7955, lr=0.0000017, metrics:accuracy:0.6434
INFO:root:22:08:58 [Epoch 4 Batch 3200/13130] loss=0.5214, lr=0.0000078, metrics:accuracy:0.7860
INFO:root:22:09:00 [Epoch 4 Batch 1600/13130] loss=0.6773, lr=0.0000033, metrics:accuracy:0.7030
INFO:root:22:09:13 [Epoch 4 Batch 3200/13130] loss=0.6280, lr=0.0000039, metrics:accuracy:0.7271
INFO:root:22:09:27 [Epoch 4 Batch 800/13130] loss=0.7862, lr=0.0000017, metrics:accuracy:0.6431
INFO:root:22:09:37 [Epoch 4 Batch 3600/13130] loss=0.5425, lr=0.0000077, metrics:accuracy:0.7845
INFO:root:22:09:38 [Epoch 4 Batch 2000/13130] loss=0.6766, lr=0.0000033, metrics:accuracy:0.7044
INFO:root:22:09:51 [Epoch 4 Batch 3600/13130] loss=0.6399, lr=0.0000038, metrics:accuracy:0.7280
INFO:root:22:10:05 [Epoch 4 Batch 1200/13130] loss=0.7651, lr=0.0000017, metrics:accuracy:0.6489
INFO:root:22:10:17 [Epoch 4 Batch 4000/13130] loss=0.5381, lr=0.0000075, metrics:accuracy:0.7842
INFO:root:22:10:18 [Epoch 4 Batch 2400/13130] loss=0.6821, lr=0.0000032, metrics:accuracy:0.7043
INFO:root:22:10:30 [Epoch 4 Batch 4000/13130] loss=0.6455, lr=0.0000038, metrics:accuracy:0.7275
INFO:root:22:10:44 [Epoch 4 Batch 1600/13130] loss=0.7731, lr=0.0000017, metrics:accuracy:0.6500
INFO:root:22:10:55 [Epoch 4 Batch 4400/13130] loss=0.5265, lr=0.0000074, metrics:accuracy:0.7845
INFO:root:22:10:56 [Epoch 4 Batch 2800/13130] loss=0.6704, lr=0.0000032, metrics:accuracy:0.7058
INFO:root:22:11:08 [Epoch 4 Batch 4400/13130] loss=0.6300, lr=0.0000037, metrics:accuracy:0.7283
INFO:root:22:11:22 [Epoch 4 Batch 2000/13130] loss=0.7791, lr=0.0000016, metrics:accuracy:0.6506
INFO:root:22:11:33 [Epoch 4 Batch 4800/13130] loss=0.5475, lr=0.0000073, metrics:accuracy:0.7840
INFO:root:22:11:35 [Epoch 4 Batch 3200/13130] loss=0.6750, lr=0.0000031, metrics:accuracy:0.7064
INFO:root:22:11:46 [Epoch 4 Batch 4800/13130] loss=0.6479, lr=0.0000036, metrics:accuracy:0.7281
INFO:root:22:12:01 [Epoch 4 Batch 2400/13130] loss=0.7836, lr=0.0000016, metrics:accuracy:0.6501
INFO:root:22:12:12 [Epoch 4 Batch 5200/13130] loss=0.5303, lr=0.0000071, metrics:accuracy:0.7839
INFO:root:22:12:13 [Epoch 4 Batch 3600/13130] loss=0.6810, lr=0.0000031, metrics:accuracy:0.7068
INFO:root:22:12:25 [Epoch 4 Batch 5200/13130] loss=0.6390, lr=0.0000036, metrics:accuracy:0.7288
INFO:root:22:12:39 [Epoch 4 Batch 2800/13130] loss=0.7756, lr=0.0000016, metrics:accuracy:0.6509
INFO:root:22:12:51 [Epoch 4 Batch 5600/13130] loss=0.5124, lr=0.0000070, metrics:accuracy:0.7848
INFO:root:22:12:52 [Epoch 4 Batch 4000/13130] loss=0.6862, lr=0.0000030, metrics:accuracy:0.7069
INFO:root:22:13:03 [Epoch 4 Batch 5600/13130] loss=0.6236, lr=0.0000035, metrics:accuracy:0.7297
INFO:root:22:13:17 [Epoch 4 Batch 3200/13130] loss=0.7793, lr=0.0000016, metrics:accuracy:0.6506
INFO:root:22:13:30 [Epoch 4 Batch 6000/13130] loss=0.5298, lr=0.0000069, metrics:accuracy:0.7850
INFO:root:22:13:31 [Epoch 4 Batch 4400/13130] loss=0.6695, lr=0.0000030, metrics:accuracy:0.7082
INFO:root:22:13:42 [Epoch 4 Batch 6000/13130] loss=0.6290, lr=0.0000034, metrics:accuracy:0.7304
INFO:root:22:13:56 [Epoch 4 Batch 3600/13130] loss=0.7725, lr=0.0000015, metrics:accuracy:0.6513
INFO:root:22:14:09 [Epoch 4 Batch 6400/13130] loss=0.5514, lr=0.0000067, metrics:accuracy:0.7846
INFO:root:22:14:10 [Epoch 4 Batch 4800/13130] loss=0.6817, lr=0.0000029, metrics:accuracy:0.7081
INFO:root:22:14:20 [Epoch 4 Batch 6400/13130] loss=0.6664, lr=0.0000034, metrics:accuracy:0.7295
INFO:root:22:14:35 [Epoch 4 Batch 4000/13130] loss=0.7879, lr=0.0000015, metrics:accuracy:0.6510
INFO:root:22:14:47 [Epoch 4 Batch 6800/13130] loss=0.5084, lr=0.0000066, metrics:accuracy:0.7849
INFO:root:22:14:49 [Epoch 4 Batch 5200/13130] loss=0.6829, lr=0.0000028, metrics:accuracy:0.7087
INFO:root:22:14:58 [Epoch 4 Batch 6800/13130] loss=0.6361, lr=0.0000033, metrics:accuracy:0.7297
INFO:root:22:15:13 [Epoch 4 Batch 4400/13130] loss=0.7649, lr=0.0000015, metrics:accuracy:0.6525
INFO:root:22:15:27 [Epoch 4 Batch 7200/13130] loss=0.5331, lr=0.0000064, metrics:accuracy:0.7851
INFO:root:22:15:28 [Epoch 4 Batch 5600/13130] loss=0.6692, lr=0.0000028, metrics:accuracy:0.7090
INFO:root:22:15:37 [Epoch 4 Batch 7200/13130] loss=0.6340, lr=0.0000032, metrics:accuracy:0.7305
INFO:root:22:15:51 [Epoch 4 Batch 4800/13130] loss=0.7815, lr=0.0000015, metrics:accuracy:0.6521
INFO:root:22:16:06 [Epoch 4 Batch 7600/13130] loss=0.5511, lr=0.0000063, metrics:accuracy:0.7850
INFO:root:22:16:07 [Epoch 4 Batch 6000/13130] loss=0.6707, lr=0.0000027, metrics:accuracy:0.7092
INFO:root:22:16:15 [Epoch 4 Batch 7600/13130] loss=0.6487, lr=0.0000032, metrics:accuracy:0.7308
INFO:root:22:16:29 [Epoch 4 Batch 5200/13130] loss=0.7824, lr=0.0000014, metrics:accuracy:0.6525
INFO:root:22:16:44 [Epoch 4 Batch 8000/13130] loss=0.5212, lr=0.0000062, metrics:accuracy:0.7849
INFO:root:22:16:46 [Epoch 4 Batch 6400/13130] loss=0.7120, lr=0.0000027, metrics:accuracy:0.7085
INFO:root:22:16:53 [Epoch 4 Batch 8000/13130] loss=0.6335, lr=0.0000031, metrics:accuracy:0.7310
INFO:root:22:17:08 [Epoch 4 Batch 5600/13130] loss=0.7736, lr=0.0000014, metrics:accuracy:0.6528
INFO:root:22:17:24 [Epoch 4 Batch 8400/13130] loss=0.5319, lr=0.0000060, metrics:accuracy:0.7849
INFO:root:22:17:25 [Epoch 4 Batch 6800/13130] loss=0.6752, lr=0.0000026, metrics:accuracy:0.7085
INFO:root:22:17:33 [Epoch 4 Batch 8400/13130] loss=0.6609, lr=0.0000030, metrics:accuracy:0.7303
INFO:root:22:17:47 [Epoch 4 Batch 6000/13130] loss=0.7825, lr=0.0000014, metrics:accuracy:0.6527
INFO:root:22:18:03 [Epoch 4 Batch 8800/13130] loss=0.5344, lr=0.0000059, metrics:accuracy:0.7846
INFO:root:22:18:04 [Epoch 4 Batch 7200/13130] loss=0.6781, lr=0.0000026, metrics:accuracy:0.7087
INFO:root:22:18:11 [Epoch 4 Batch 8800/13130] loss=0.6352, lr=0.0000030, metrics:accuracy:0.7306
INFO:root:22:18:25 [Epoch 4 Batch 6400/13130] loss=0.8060, lr=0.0000013, metrics:accuracy:0.6520
INFO:root:22:18:42 [Epoch 4 Batch 9200/13130] loss=0.5271, lr=0.0000058, metrics:accuracy:0.7848
INFO:root:22:18:43 [Epoch 4 Batch 7600/13130] loss=0.6861, lr=0.0000025, metrics:accuracy:0.7088
INFO:root:22:18:50 [Epoch 4 Batch 9200/13130] loss=0.6478, lr=0.0000029, metrics:accuracy:0.7303
INFO:root:22:19:04 [Epoch 4 Batch 6800/13130] loss=0.7831, lr=0.0000013, metrics:accuracy:0.6516
INFO:root:22:19:20 [Epoch 4 Batch 9600/13130] loss=0.5256, lr=0.0000056, metrics:accuracy:0.7848
INFO:root:22:19:21 [Epoch 4 Batch 8000/13130] loss=0.6731, lr=0.0000025, metrics:accuracy:0.7094
INFO:root:22:19:28 [Epoch 4 Batch 9600/13130] loss=0.6453, lr=0.0000028, metrics:accuracy:0.7303
INFO:root:22:19:42 [Epoch 4 Batch 7200/13130] loss=0.7737, lr=0.0000013, metrics:accuracy:0.6521
INFO:root:22:19:59 [Epoch 4 Batch 10000/13130] loss=0.5184, lr=0.0000055, metrics:accuracy:0.7853
INFO:root:22:20:01 [Epoch 4 Batch 8400/13130] loss=0.6840, lr=0.0000024, metrics:accuracy:0.7096
INFO:root:22:20:07 [Epoch 4 Batch 10000/13130] loss=0.6279, lr=0.0000027, metrics:accuracy:0.7306
INFO:root:22:20:21 [Epoch 4 Batch 7600/13130] loss=0.7789, lr=0.0000013, metrics:accuracy:0.6518
INFO:root:22:20:38 [Epoch 4 Batch 10400/13130] loss=0.5298, lr=0.0000054, metrics:accuracy:0.7852
INFO:root:22:20:39 [Epoch 4 Batch 8800/13130] loss=0.6741, lr=0.0000024, metrics:accuracy:0.7098
INFO:root:22:20:45 [Epoch 4 Batch 10400/13130] loss=0.6408, lr=0.0000027, metrics:accuracy:0.7305
INFO:root:22:20:59 [Epoch 4 Batch 8000/13130] loss=0.7732, lr=0.0000012, metrics:accuracy:0.6521
INFO:root:22:21:17 [Epoch 4 Batch 10800/13130] loss=0.5302, lr=0.0000052, metrics:accuracy:0.7854
INFO:root:22:21:18 [Epoch 4 Batch 9200/13130] loss=0.6850, lr=0.0000023, metrics:accuracy:0.7097
INFO:root:22:21:23 [Epoch 4 Batch 10800/13130] loss=0.6351, lr=0.0000026, metrics:accuracy:0.7306
INFO:root:22:21:37 [Epoch 4 Batch 8400/13130] loss=0.7876, lr=0.0000012, metrics:accuracy:0.6522
INFO:root:22:21:55 [Epoch 4 Batch 11200/13130] loss=0.4947, lr=0.0000051, metrics:accuracy:0.7858
INFO:root:22:21:56 [Epoch 4 Batch 9600/13130] loss=0.6836, lr=0.0000023, metrics:accuracy:0.7098
INFO:root:22:22:01 [Epoch 4 Batch 11200/13130] loss=0.6022, lr=0.0000025, metrics:accuracy:0.7313
INFO:root:22:22:15 [Epoch 4 Batch 8800/13130] loss=0.7797, lr=0.0000012, metrics:accuracy:0.6522
INFO:root:22:22:34 [Epoch 4 Batch 11600/13130] loss=0.5153, lr=0.0000050, metrics:accuracy:0.7860
INFO:root:22:22:35 [Epoch 4 Batch 10000/13130] loss=0.6768, lr=0.0000022, metrics:accuracy:0.7100
INFO:root:22:22:40 [Epoch 4 Batch 11600/13130] loss=0.6247, lr=0.0000025, metrics:accuracy:0.7316
INFO:root:22:22:53 [Epoch 4 Batch 9200/13130] loss=0.7783, lr=0.0000012, metrics:accuracy:0.6523
INFO:root:22:23:13 [Epoch 4 Batch 12000/13130] loss=0.5212, lr=0.0000048, metrics:accuracy:0.7862
INFO:root:22:23:14 [Epoch 4 Batch 10400/13130] loss=0.6762, lr=0.0000021, metrics:accuracy:0.7101
INFO:root:22:23:18 [Epoch 4 Batch 12000/13130] loss=0.6348, lr=0.0000024, metrics:accuracy:0.7318
INFO:root:22:23:32 [Epoch 4 Batch 9600/13130] loss=0.7842, lr=0.0000011, metrics:accuracy:0.6523
INFO:root:22:23:52 [Epoch 4 Batch 12400/13130] loss=0.5334, lr=0.0000047, metrics:accuracy:0.7861
INFO:root:22:23:53 [Epoch 4 Batch 10800/13130] loss=0.6747, lr=0.0000021, metrics:accuracy:0.7100
INFO:root:22:23:57 [Epoch 4 Batch 12400/13130] loss=0.6276, lr=0.0000023, metrics:accuracy:0.7323
INFO:root:22:24:11 [Epoch 4 Batch 10000/13130] loss=0.7674, lr=0.0000011, metrics:accuracy:0.6526
INFO:root:22:24:30 [Epoch 4 Batch 12800/13130] loss=0.5255, lr=0.0000045, metrics:accuracy:0.7861
INFO:root:22:24:31 [Epoch 4 Batch 11200/13130] loss=0.6513, lr=0.0000020, metrics:accuracy:0.7104
INFO:root:22:24:35 [Epoch 4 Batch 12800/13130] loss=0.6478, lr=0.0000023, metrics:accuracy:0.7320
INFO:root:22:24:49 [Epoch 4 Batch 10400/13130] loss=0.7849, lr=0.0000011, metrics:accuracy:0.6527
INFO:root:22:25:02 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:25:07 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:25:09 [Epoch 4 Batch 11600/13130] loss=0.6691, lr=0.0000020, metrics:accuracy:0.7106
INFO:root:22:25:23 [Batch 400/3750] loss=1.0438, metrics:accuracy:0.6787
INFO:root:22:25:25 [Epoch 4 Batch 10800/13130] loss=0.7781, lr=0.0000010, metrics:accuracy:0.6528
INFO:root:22:25:28 [Batch 400/3750] loss=0.8526, metrics:accuracy:0.7216
INFO:root:22:25:43 [Epoch 4 Batch 12000/13130] loss=0.6739, lr=0.0000019, metrics:accuracy:0.7109
INFO:root:22:25:44 [Batch 800/3750] loss=1.0504, metrics:accuracy:0.6791
INFO:root:22:25:48 [Batch 800/3750] loss=0.8765, metrics:accuracy:0.7178
INFO:root:22:25:59 [Epoch 4 Batch 11200/13130] loss=0.7531, lr=0.0000010, metrics:accuracy:0.6530
INFO:root:22:26:05 [Batch 1200/3750] loss=1.0922, metrics:accuracy:0.6716
INFO:root:22:26:08 [Batch 1200/3750] loss=0.8775, metrics:accuracy:0.7140
INFO:root:22:26:18 [Epoch 4 Batch 12400/13130] loss=0.6721, lr=0.0000019, metrics:accuracy:0.7110
INFO:root:22:26:26 [Batch 1600/3750] loss=1.0587, metrics:accuracy:0.6488
INFO:root:22:26:29 [Batch 1600/3750] loss=1.0351, metrics:accuracy:0.6733
INFO:root:22:26:33 [Epoch 4 Batch 11600/13130] loss=0.7692, lr=0.0000010, metrics:accuracy:0.6534
INFO:root:22:26:47 [Batch 2000/3750] loss=1.0349, metrics:accuracy:0.6344
INFO:root:22:26:50 [Batch 2000/3750] loss=1.0445, metrics:accuracy:0.6453
INFO:root:22:26:53 [Epoch 4 Batch 12800/13130] loss=0.6883, lr=0.0000018, metrics:accuracy:0.7108
INFO:root:22:27:07 [Epoch 4 Batch 12000/13130] loss=0.7739, lr=0.0000010, metrics:accuracy:0.6535
INFO:root:22:27:07 [Batch 2400/3750] loss=1.0334, metrics:accuracy:0.6251
INFO:root:22:27:10 [Batch 2400/3750] loss=1.0601, metrics:accuracy:0.6270
INFO:root:22:27:23 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:27:29 [Batch 2800/3750] loss=1.1856, metrics:accuracy:0.6108
INFO:root:22:27:31 [Batch 2800/3750] loss=1.0769, metrics:accuracy:0.6150
INFO:root:22:27:41 [Epoch 4 Batch 12400/13130] loss=0.7791, lr=0.0000009, metrics:accuracy:0.6535
INFO:root:22:27:43 [Batch 400/3750] loss=0.8209, metrics:accuracy:0.7266
INFO:root:22:27:49 [Batch 3200/3750] loss=1.2299, metrics:accuracy:0.6015
INFO:root:22:27:52 [Batch 3200/3750] loss=1.0791, metrics:accuracy:0.6071
INFO:root:22:28:04 [Batch 800/3750] loss=0.8311, metrics:accuracy:0.7248
INFO:root:22:28:10 [Batch 3600/3750] loss=1.2210, metrics:accuracy:0.5928
INFO:root:22:28:12 [Batch 3600/3750] loss=1.0791, metrics:accuracy:0.6003
INFO:root:22:28:16 [Epoch 4 Batch 12800/13130] loss=0.7909, lr=0.0000009, metrics:accuracy:0.6531
INFO:root:22:28:17 validation metrics:accuracy:0.5906
INFO:root:22:28:17 Time cost=195.04s, throughput=153.81 samples/s
INFO:root:22:28:19 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:22:28:19 Time cost=1453.41s
INFO:root:22:28:20 validation metrics:accuracy:0.5984
INFO:root:22:28:20 Time cost=192.75s, throughput=155.65 samples/s
INFO:root:22:28:22 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:22:28:22 Time cost=1433.85s
INFO:root:22:28:23 [Batch 1200/3750] loss=0.8463, metrics:accuracy:0.7199
INFO:root:22:28:43 [Batch 1600/3750] loss=1.0297, metrics:accuracy:0.6731
INFO:root:22:28:47 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:28:54 [Epoch 5 Batch 400/13130] loss=0.4017, lr=0.0000043, metrics:accuracy:0.8497
INFO:root:22:28:59 [Epoch 5 Batch 400/13130] loss=0.5648, lr=0.0000022, metrics:accuracy:0.7669
INFO:root:22:29:04 [Batch 2000/3750] loss=1.0351, metrics:accuracy:0.6418
INFO:root:22:29:07 [Batch 400/3750] loss=0.7817, metrics:accuracy:0.7325
INFO:root:22:29:24 [Batch 2400/3750] loss=1.0595, metrics:accuracy:0.6198
INFO:root:22:29:27 [Batch 800/3750] loss=0.7878, metrics:accuracy:0.7303
INFO:root:22:29:28 [Epoch 5 Batch 800/13130] loss=0.3974, lr=0.0000042, metrics:accuracy:0.8500
INFO:root:22:29:33 [Epoch 5 Batch 800/13130] loss=0.5546, lr=0.0000021, metrics:accuracy:0.7714
INFO:root:22:29:45 [Batch 2800/3750] loss=1.0436, metrics:accuracy:0.6080
INFO:root:22:29:48 [Batch 1200/3750] loss=0.7913, metrics:accuracy:0.7285
INFO:root:22:30:04 [Epoch 5 Batch 1200/13130] loss=0.4164, lr=0.0000040, metrics:accuracy:0.8474
INFO:root:22:30:05 [Batch 3200/3750] loss=1.0286, metrics:accuracy:0.6005
INFO:root:22:30:07 [Epoch 5 Batch 1200/13130] loss=0.5791, lr=0.0000020, metrics:accuracy:0.7686
INFO:root:22:30:08 [Batch 1600/3750] loss=0.9200, metrics:accuracy:0.6827
INFO:root:22:30:25 [Batch 3600/3750] loss=1.0323, metrics:accuracy:0.5943
INFO:root:22:30:28 [Batch 2000/3750] loss=0.9264, metrics:accuracy:0.6524
INFO:root:22:30:32 validation metrics:accuracy:0.5933
INFO:root:22:30:32 Time cost=189.91s, throughput=157.97 samples/s
INFO:root:22:30:34 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:22:30:34 Time cost=1449.57s
INFO:root:22:30:38 [Epoch 5 Batch 1600/13130] loss=0.4135, lr=0.0000039, metrics:accuracy:0.8466
INFO:root:22:30:41 [Epoch 5 Batch 1600/13130] loss=0.5629, lr=0.0000019, metrics:accuracy:0.7694
INFO:root:22:30:48 [Batch 2400/3750] loss=0.9422, metrics:accuracy:0.6306
INFO:root:22:31:09 [Batch 2800/3750] loss=0.9855, metrics:accuracy:0.6156
INFO:root:22:31:15 [Epoch 5 Batch 400/13130] loss=0.6198, lr=0.0000017, metrics:accuracy:0.7447
INFO:root:22:31:17 [Epoch 5 Batch 2000/13130] loss=0.5493, lr=0.0000019, metrics:accuracy:0.7710
INFO:root:22:31:18 [Epoch 5 Batch 2000/13130] loss=0.4159, lr=0.0000038, metrics:accuracy:0.8447
INFO:root:22:31:30 [Batch 3200/3750] loss=0.9894, metrics:accuracy:0.6039
INFO:root:22:31:50 [Batch 3600/3750] loss=0.9877, metrics:accuracy:0.5947
INFO:root:22:31:52 [Epoch 5 Batch 2400/13130] loss=0.5647, lr=0.0000018, metrics:accuracy:0.7723
INFO:root:22:31:53 [Epoch 5 Batch 800/13130] loss=0.6160, lr=0.0000017, metrics:accuracy:0.7461
INFO:root:22:31:57 [Epoch 5 Batch 2400/13130] loss=0.4066, lr=0.0000036, metrics:accuracy:0.8450
INFO:root:22:31:58 validation metrics:accuracy:0.5927
INFO:root:22:31:58 Time cost=190.88s, throughput=157.17 samples/s
INFO:root:22:31:59 params saved in: ./output_dir/model_bert_weibo3_3.params
INFO:root:22:31:59 Time cost=1428.33s
INFO:root:22:32:28 [Epoch 5 Batch 2800/13130] loss=0.5593, lr=0.0000017, metrics:accuracy:0.7725
INFO:root:22:32:33 [Epoch 5 Batch 1200/13130] loss=0.6309, lr=0.0000016, metrics:accuracy:0.7447
INFO:root:22:32:36 [Epoch 5 Batch 2800/13130] loss=0.3990, lr=0.0000035, metrics:accuracy:0.8462
INFO:root:22:32:38 [Epoch 5 Batch 400/13130] loss=0.7438, lr=0.0000009, metrics:accuracy:0.6750
INFO:root:22:33:06 [Epoch 5 Batch 3200/13130] loss=0.5473, lr=0.0000017, metrics:accuracy:0.7725
INFO:root:22:33:11 [Epoch 5 Batch 1600/13130] loss=0.6235, lr=0.0000016, metrics:accuracy:0.7446
INFO:root:22:33:14 [Epoch 5 Batch 3200/13130] loss=0.3963, lr=0.0000034, metrics:accuracy:0.8456
INFO:root:22:33:16 [Epoch 5 Batch 800/13130] loss=0.7558, lr=0.0000008, metrics:accuracy:0.6706
INFO:root:22:33:45 [Epoch 5 Batch 3600/13130] loss=0.5498, lr=0.0000016, metrics:accuracy:0.7723
INFO:root:22:33:51 [Epoch 5 Batch 2000/13130] loss=0.6055, lr=0.0000015, metrics:accuracy:0.7459
INFO:root:22:33:54 [Epoch 5 Batch 3600/13130] loss=0.3972, lr=0.0000032, metrics:accuracy:0.8459
INFO:root:22:33:55 [Epoch 5 Batch 1200/13130] loss=0.7563, lr=0.0000008, metrics:accuracy:0.6711
INFO:root:22:34:23 [Epoch 5 Batch 4000/13130] loss=0.5355, lr=0.0000015, metrics:accuracy:0.7731
INFO:root:22:34:30 [Epoch 5 Batch 2400/13130] loss=0.6128, lr=0.0000015, metrics:accuracy:0.7472
INFO:root:22:34:33 [Epoch 5 Batch 1600/13130] loss=0.7610, lr=0.0000008, metrics:accuracy:0.6733
INFO:root:22:34:33 [Epoch 5 Batch 4000/13130] loss=0.3993, lr=0.0000031, metrics:accuracy:0.8460
INFO:root:22:35:02 [Epoch 5 Batch 4400/13130] loss=0.5527, lr=0.0000015, metrics:accuracy:0.7734
INFO:root:22:35:08 [Epoch 5 Batch 2800/13130] loss=0.6210, lr=0.0000014, metrics:accuracy:0.7456
INFO:root:22:35:11 [Epoch 5 Batch 4400/13130] loss=0.3866, lr=0.0000029, metrics:accuracy:0.8463
INFO:root:22:35:12 [Epoch 5 Batch 2000/13130] loss=0.7443, lr=0.0000008, metrics:accuracy:0.6729
INFO:root:22:35:41 [Epoch 5 Batch 4800/13130] loss=0.5477, lr=0.0000014, metrics:accuracy:0.7738
INFO:root:22:35:47 [Epoch 5 Batch 3200/13130] loss=0.5981, lr=0.0000013, metrics:accuracy:0.7465
INFO:root:22:35:50 [Epoch 5 Batch 4800/13130] loss=0.3737, lr=0.0000028, metrics:accuracy:0.8468
INFO:root:22:35:51 [Epoch 5 Batch 2400/13130] loss=0.7395, lr=0.0000007, metrics:accuracy:0.6739
INFO:root:22:36:20 [Epoch 5 Batch 5200/13130] loss=0.5573, lr=0.0000013, metrics:accuracy:0.7732
INFO:root:22:36:26 [Epoch 5 Batch 3600/13130] loss=0.6061, lr=0.0000013, metrics:accuracy:0.7462
INFO:root:22:36:29 [Epoch 5 Batch 2800/13130] loss=0.7569, lr=0.0000007, metrics:accuracy:0.6722
INFO:root:22:36:29 [Epoch 5 Batch 5200/13130] loss=0.4166, lr=0.0000027, metrics:accuracy:0.8466
INFO:root:22:36:58 [Epoch 5 Batch 5600/13130] loss=0.5439, lr=0.0000013, metrics:accuracy:0.7739
INFO:root:22:37:05 [Epoch 5 Batch 4000/13130] loss=0.5889, lr=0.0000012, metrics:accuracy:0.7470
INFO:root:22:37:08 [Epoch 5 Batch 3200/13130] loss=0.7263, lr=0.0000007, metrics:accuracy:0.6742
INFO:root:22:37:08 [Epoch 5 Batch 5600/13130] loss=0.4062, lr=0.0000025, metrics:accuracy:0.8470
INFO:root:22:37:36 [Epoch 5 Batch 6000/13130] loss=0.5724, lr=0.0000012, metrics:accuracy:0.7731
INFO:root:22:37:44 [Epoch 5 Batch 4400/13130] loss=0.6169, lr=0.0000012, metrics:accuracy:0.7463
INFO:root:22:37:46 [Epoch 5 Batch 3600/13130] loss=0.7421, lr=0.0000006, metrics:accuracy:0.6737
INFO:root:22:37:47 [Epoch 5 Batch 6000/13130] loss=0.4036, lr=0.0000024, metrics:accuracy:0.8469
INFO:root:22:38:15 [Epoch 5 Batch 6400/13130] loss=0.5443, lr=0.0000011, metrics:accuracy:0.7735
INFO:root:22:38:23 [Epoch 5 Batch 4800/13130] loss=0.6043, lr=0.0000011, metrics:accuracy:0.7463
INFO:root:22:38:25 [Epoch 5 Batch 4000/13130] loss=0.7251, lr=0.0000006, metrics:accuracy:0.6748
INFO:root:22:38:26 [Epoch 5 Batch 6400/13130] loss=0.3900, lr=0.0000023, metrics:accuracy:0.8475
INFO:root:22:38:52 [Epoch 5 Batch 6800/13130] loss=0.5821, lr=0.0000011, metrics:accuracy:0.7728
INFO:root:22:39:01 [Epoch 5 Batch 5200/13130] loss=0.6115, lr=0.0000011, metrics:accuracy:0.7464
INFO:root:22:39:02 [Epoch 5 Batch 4400/13130] loss=0.7517, lr=0.0000006, metrics:accuracy:0.6741
INFO:root:22:39:04 [Epoch 5 Batch 6800/13130] loss=0.4262, lr=0.0000021, metrics:accuracy:0.8465
INFO:root:22:39:31 [Epoch 5 Batch 7200/13130] loss=0.5450, lr=0.0000010, metrics:accuracy:0.7729
INFO:root:22:39:40 [Epoch 5 Batch 5600/13130] loss=0.5982, lr=0.0000010, metrics:accuracy:0.7468
INFO:root:22:39:41 [Epoch 5 Batch 4800/13130] loss=0.7335, lr=0.0000006, metrics:accuracy:0.6748
INFO:root:22:39:44 [Epoch 5 Batch 7200/13130] loss=0.4053, lr=0.0000020, metrics:accuracy:0.8464
INFO:root:22:40:09 [Epoch 5 Batch 7600/13130] loss=0.5674, lr=0.0000009, metrics:accuracy:0.7724
INFO:root:22:40:19 [Epoch 5 Batch 6000/13130] loss=0.6193, lr=0.0000010, metrics:accuracy:0.7463
INFO:root:22:40:19 [Epoch 5 Batch 5200/13130] loss=0.7400, lr=0.0000005, metrics:accuracy:0.6752
INFO:root:22:40:22 [Epoch 5 Batch 7600/13130] loss=0.3984, lr=0.0000019, metrics:accuracy:0.8466
INFO:root:22:40:48 [Epoch 5 Batch 8000/13130] loss=0.5205, lr=0.0000009, metrics:accuracy:0.7737
INFO:root:22:40:58 [Epoch 5 Batch 6400/13130] loss=0.6031, lr=0.0000009, metrics:accuracy:0.7466
INFO:root:22:40:58 [Epoch 5 Batch 5600/13130] loss=0.7305, lr=0.0000005, metrics:accuracy:0.6756
INFO:root:22:41:02 [Epoch 5 Batch 8000/13130] loss=0.3639, lr=0.0000017, metrics:accuracy:0.8474
INFO:root:22:41:27 [Epoch 5 Batch 8400/13130] loss=0.5510, lr=0.0000008, metrics:accuracy:0.7734
INFO:root:22:41:37 [Epoch 5 Batch 6800/13130] loss=0.6175, lr=0.0000009, metrics:accuracy:0.7463
INFO:root:22:41:37 [Epoch 5 Batch 6000/13130] loss=0.7516, lr=0.0000005, metrics:accuracy:0.6754
INFO:root:22:41:41 [Epoch 5 Batch 8400/13130] loss=0.4046, lr=0.0000016, metrics:accuracy:0.8471
INFO:root:22:42:05 [Epoch 5 Batch 8800/13130] loss=0.5601, lr=0.0000007, metrics:accuracy:0.7730
INFO:root:22:42:16 [Epoch 5 Batch 7200/13130] loss=0.6060, lr=0.0000008, metrics:accuracy:0.7463
INFO:root:22:42:16 [Epoch 5 Batch 6400/13130] loss=0.7404, lr=0.0000005, metrics:accuracy:0.6753
INFO:root:22:42:19 [Epoch 5 Batch 8800/13130] loss=0.3913, lr=0.0000015, metrics:accuracy:0.8472
INFO:root:22:42:44 [Epoch 5 Batch 9200/13130] loss=0.5688, lr=0.0000007, metrics:accuracy:0.7727
INFO:root:22:42:53 [Epoch 5 Batch 6800/13130] loss=0.7597, lr=0.0000004, metrics:accuracy:0.6742
INFO:root:22:42:54 [Epoch 5 Batch 7600/13130] loss=0.6095, lr=0.0000007, metrics:accuracy:0.7461
INFO:root:22:42:57 [Epoch 5 Batch 9200/13130] loss=0.4073, lr=0.0000013, metrics:accuracy:0.8471
INFO:root:22:43:22 [Epoch 5 Batch 9600/13130] loss=0.5647, lr=0.0000006, metrics:accuracy:0.7728
INFO:root:22:43:31 [Epoch 5 Batch 7200/13130] loss=0.7522, lr=0.0000004, metrics:accuracy:0.6739
INFO:root:22:43:33 [Epoch 5 Batch 8000/13130] loss=0.5770, lr=0.0000007, metrics:accuracy:0.7470
INFO:root:22:43:37 [Epoch 5 Batch 9600/13130] loss=0.4066, lr=0.0000012, metrics:accuracy:0.8470
INFO:root:22:43:59 [Epoch 5 Batch 10000/13130] loss=0.5362, lr=0.0000005, metrics:accuracy:0.7732
INFO:root:22:44:09 [Epoch 5 Batch 7600/13130] loss=0.7429, lr=0.0000004, metrics:accuracy:0.6742
INFO:root:22:44:12 [Epoch 5 Batch 8400/13130] loss=0.6110, lr=0.0000006, metrics:accuracy:0.7468
INFO:root:22:44:16 [Epoch 5 Batch 10000/13130] loss=0.3816, lr=0.0000011, metrics:accuracy:0.8470
INFO:root:22:44:39 [Epoch 5 Batch 10400/13130] loss=0.5538, lr=0.0000005, metrics:accuracy:0.7732
INFO:root:22:44:48 [Epoch 5 Batch 8000/13130] loss=0.7165, lr=0.0000003, metrics:accuracy:0.6749
INFO:root:22:44:51 [Epoch 5 Batch 8800/13130] loss=0.6068, lr=0.0000006, metrics:accuracy:0.7469
INFO:root:22:44:55 [Epoch 5 Batch 10400/13130] loss=0.3915, lr=0.0000009, metrics:accuracy:0.8470
INFO:root:22:45:17 [Epoch 5 Batch 10800/13130] loss=0.5648, lr=0.0000004, metrics:accuracy:0.7729
INFO:root:22:45:27 [Epoch 5 Batch 8400/13130] loss=0.7468, lr=0.0000003, metrics:accuracy:0.6749
INFO:root:22:45:30 [Epoch 5 Batch 9200/13130] loss=0.6185, lr=0.0000005, metrics:accuracy:0.7465
INFO:root:22:45:34 [Epoch 5 Batch 10800/13130] loss=0.3904, lr=0.0000008, metrics:accuracy:0.8469
INFO:root:22:45:56 [Epoch 5 Batch 11200/13130] loss=0.5589, lr=0.0000003, metrics:accuracy:0.7730
INFO:root:22:46:05 [Epoch 5 Batch 8800/13130] loss=0.7411, lr=0.0000003, metrics:accuracy:0.6749
INFO:root:22:46:09 [Epoch 5 Batch 9600/13130] loss=0.6303, lr=0.0000005, metrics:accuracy:0.7459
INFO:root:22:46:13 [Epoch 5 Batch 11200/13130] loss=0.4020, lr=0.0000006, metrics:accuracy:0.8468
INFO:root:22:46:34 [Epoch 5 Batch 11600/13130] loss=0.5501, lr=0.0000003, metrics:accuracy:0.7730
INFO:root:22:46:43 [Epoch 5 Batch 9200/13130] loss=0.7569, lr=0.0000003, metrics:accuracy:0.6747
INFO:root:22:46:48 [Epoch 5 Batch 10000/13130] loss=0.5989, lr=0.0000004, metrics:accuracy:0.7458
INFO:root:22:46:52 [Epoch 5 Batch 11600/13130] loss=0.3970, lr=0.0000005, metrics:accuracy:0.8472
INFO:root:22:47:12 [Epoch 5 Batch 12000/13130] loss=0.5514, lr=0.0000002, metrics:accuracy:0.7730
INFO:root:22:47:21 [Epoch 5 Batch 9600/13130] loss=0.7648, lr=0.0000002, metrics:accuracy:0.6740
INFO:root:22:47:27 [Epoch 5 Batch 10400/13130] loss=0.6140, lr=0.0000004, metrics:accuracy:0.7455
INFO:root:22:47:31 [Epoch 5 Batch 12000/13130] loss=0.4116, lr=0.0000004, metrics:accuracy:0.8469
INFO:root:22:47:51 [Epoch 5 Batch 12400/13130] loss=0.5776, lr=0.0000001, metrics:accuracy:0.7729
INFO:root:22:48:00 [Epoch 5 Batch 10000/13130] loss=0.7401, lr=0.0000002, metrics:accuracy:0.6739
INFO:root:22:48:06 [Epoch 5 Batch 10800/13130] loss=0.6280, lr=0.0000003, metrics:accuracy:0.7453
INFO:root:22:48:10 [Epoch 5 Batch 12400/13130] loss=0.4085, lr=0.0000002, metrics:accuracy:0.8469
INFO:root:22:48:29 [Epoch 5 Batch 12800/13130] loss=0.5567, lr=0.0000001, metrics:accuracy:0.7728
INFO:root:22:48:38 [Epoch 5 Batch 10400/13130] loss=0.7510, lr=0.0000002, metrics:accuracy:0.6736
INFO:root:22:48:44 [Epoch 5 Batch 11200/13130] loss=0.6121, lr=0.0000003, metrics:accuracy:0.7455
INFO:root:22:48:49 [Epoch 5 Batch 12800/13130] loss=0.3959, lr=0.0000001, metrics:accuracy:0.8469
INFO:root:22:48:58 Finish training step: 32812
INFO:root:22:48:58 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:49:14 [Epoch 5 Batch 10800/13130] loss=0.7561, lr=0.0000002, metrics:accuracy:0.6730
INFO:root:22:49:18 Finish training step: 32812
INFO:root:22:49:18 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:49:18 [Batch 400/3750] loss=1.0089, metrics:accuracy:0.6737
INFO:root:22:49:21 [Epoch 5 Batch 11600/13130] loss=0.6066, lr=0.0000002, metrics:accuracy:0.7451
INFO:root:22:49:38 [Batch 800/3750] loss=1.0285, metrics:accuracy:0.6714
INFO:root:22:49:39 [Batch 400/3750] loss=1.2493, metrics:accuracy:0.6575
INFO:root:22:49:48 [Epoch 5 Batch 11200/13130] loss=0.7608, lr=0.0000001, metrics:accuracy:0.6727
INFO:root:22:49:55 [Epoch 5 Batch 12000/13130] loss=0.6127, lr=0.0000001, metrics:accuracy:0.7450
INFO:root:22:49:59 [Batch 1200/3750] loss=1.0332, metrics:accuracy:0.6678
INFO:root:22:50:00 [Batch 800/3750] loss=1.2492, metrics:accuracy:0.6553
INFO:root:22:50:19 [Batch 1600/3750] loss=1.0856, metrics:accuracy:0.6373
INFO:root:22:50:21 [Batch 1200/3750] loss=1.3002, metrics:accuracy:0.6495
INFO:root:22:50:22 [Epoch 5 Batch 11600/13130] loss=0.7340, lr=0.0000001, metrics:accuracy:0.6727
INFO:root:22:50:30 [Epoch 5 Batch 12400/13130] loss=0.6325, lr=0.0000001, metrics:accuracy:0.7450
INFO:root:22:50:39 [Batch 2000/3750] loss=1.0795, metrics:accuracy:0.6174
INFO:root:22:50:42 [Batch 1600/3750] loss=1.3095, metrics:accuracy:0.6235
INFO:root:22:50:56 [Epoch 5 Batch 12000/13130] loss=0.7594, lr=0.0000001, metrics:accuracy:0.6720
INFO:root:22:50:59 [Batch 2400/3750] loss=1.0923, metrics:accuracy:0.6038
INFO:root:22:51:03 [Batch 2000/3750] loss=1.2797, metrics:accuracy:0.6072
INFO:root:22:51:04 [Epoch 5 Batch 12800/13130] loss=0.6043, lr=0.0000000, metrics:accuracy:0.7451
INFO:root:22:51:20 [Batch 2800/3750] loss=1.0683, metrics:accuracy:0.5987
INFO:root:22:51:23 [Batch 2400/3750] loss=1.2839, metrics:accuracy:0.5967
INFO:root:22:51:29 Finish training step: 32812
INFO:root:22:51:29 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:51:31 [Epoch 5 Batch 12400/13130] loss=0.7586, lr=0.0000000, metrics:accuracy:0.6719
INFO:root:22:51:41 [Batch 3200/3750] loss=1.0485, metrics:accuracy:0.5971
INFO:root:22:51:44 [Batch 2800/3750] loss=1.2705, metrics:accuracy:0.5893
INFO:root:22:51:50 [Batch 400/3750] loss=0.9528, metrics:accuracy:0.6778
INFO:root:22:52:01 [Batch 3600/3750] loss=1.0496, metrics:accuracy:0.5948
INFO:root:22:52:04 [Epoch 5 Batch 12800/13130] loss=0.7423, lr=0.0000000, metrics:accuracy:0.6718
INFO:root:22:52:05 [Batch 3200/3750] loss=1.2810, metrics:accuracy:0.5871
INFO:root:22:52:08 validation metrics:accuracy:0.5944
INFO:root:22:52:08 Time cost=190.26s, throughput=157.68 samples/s
INFO:root:22:52:10 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:22:52:10 Time cost=1428.19s
INFO:root:22:52:10 [Batch 800/3750] loss=0.9572, metrics:accuracy:0.6814
INFO:root:22:52:10 Best model at epoch 1. Validation metrics:accuracy:0.5993
INFO:root:22:52:10 Now we are doing testing on test with gpu(0).
INFO:root:22:52:25 [Batch 3600/3750] loss=1.2372, metrics:accuracy:0.5860
INFO:root:22:52:29 Finish training step: 32812
INFO:root:22:52:29 Now we are doing evaluation on dev with gpu(0).
INFO:root:22:52:30 [Batch 1200/3750] loss=0.9786, metrics:accuracy:0.6751
INFO:root:22:52:32 validation metrics:accuracy:0.5860
INFO:root:22:52:32 Time cost=194.57s, throughput=154.18 samples/s
INFO:root:22:52:34 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:22:52:34 Time cost=1454.88s
INFO:root:22:52:34 Best model at epoch 1. Validation metrics:accuracy:0.5976
INFO:root:22:52:34 Now we are doing testing on test with gpu(0).
INFO:root:22:52:48 [Batch 1600/3750] loss=1.0420, metrics:accuracy:0.6411
INFO:root:22:52:49 [Batch 400/3750] loss=0.8443, metrics:accuracy:0.7078
INFO:root:22:53:08 [Batch 2000/3750] loss=1.0376, metrics:accuracy:0.6172
INFO:root:22:53:09 [Batch 800/3750] loss=0.8455, metrics:accuracy:0.7091
INFO:root:22:53:29 [Batch 2400/3750] loss=1.0563, metrics:accuracy:0.6009
INFO:root:22:53:29 [Batch 1200/3750] loss=0.8500, metrics:accuracy:0.7069
INFO:root:22:53:41 Time cost=91.21s, throughput=164.45 samples/s
INFO:root:22:53:46 [Batch 1600/3750] loss=0.9577, metrics:accuracy:0.6609
INFO:root:22:53:49 [Batch 2800/3750] loss=1.0223, metrics:accuracy:0.5950
INFO:root:22:53:56 [Batch 2000/3750] loss=0.9572, metrics:accuracy:0.6314
INFO:root:22:54:07 [Batch 2400/3750] loss=0.9739, metrics:accuracy:0.6097
INFO:root:22:54:08 Time cost=93.38s, throughput=160.63 samples/s
INFO:root:22:54:09 [Batch 3200/3750] loss=0.9981, metrics:accuracy:0.5939
INFO:root:22:54:18 [Batch 2800/3750] loss=0.9429, metrics:accuracy:0.6013
INFO:root:22:54:20 [Batch 3600/3750] loss=0.9981, metrics:accuracy:0.5917
INFO:root:22:54:24 validation metrics:accuracy:0.5916
INFO:root:22:54:24 Time cost=174.65s, throughput=171.77 samples/s
INFO:root:22:54:25 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:22:54:25 Time cost=1430.96s
INFO:root:22:54:26 Best model at epoch 2. Validation metrics:accuracy:0.5962
INFO:root:22:54:26 Now we are doing testing on test with gpu(0).
INFO:root:22:54:29 [Batch 3200/3750] loss=0.9177, metrics:accuracy:0.5965
INFO:root:22:54:40 [Batch 3600/3750] loss=0.9179, metrics:accuracy:0.5932
INFO:root:22:54:44 validation metrics:accuracy:0.5931
INFO:root:22:54:44 Time cost=134.86s, throughput=222.46 samples/s
INFO:root:22:54:45 params saved in: ./output_dir/model_bert_weibo3_4.params
INFO:root:22:54:45 Time cost=1366.07s
INFO:root:22:54:46 Best model at epoch 4. Validation metrics:accuracy:0.5931
INFO:root:22:54:46 Now we are doing testing on test with gpu(0).
INFO:root:22:55:16 Time cost=50.60s, throughput=296.46 samples/s
INFO:root:22:55:36 Time cost=50.50s, throughput=297.04 samples/s
INFO:root:10:05:25 Namespace(accumulate=2, batch_size=8, bert_dataset='wiki_cn_cased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='weibo3', training_steps=None, warmup_ratio=0.1)
INFO:root:10:05:25 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
