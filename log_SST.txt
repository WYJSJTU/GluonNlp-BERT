INFO:root:14:38:20 Namespace(accumulate=None, batch_size=4, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:15:02:03 Namespace(accumulate=None, batch_size=4, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:15:02:16 processing dataset...
INFO:root:15:02:23 Now we are doing BERT classification training on gpu(0)!
INFO:root:15:02:23 training steps=84186
INFO:root:15:02:53 [Epoch 1 Batch 400/16842] loss=0.6841, lr=0.0000009, metrics:accuracy:0.5755
INFO:root:15:03:21 [Epoch 1 Batch 800/16842] loss=0.5622, lr=0.0000019, metrics:accuracy:0.6482
INFO:root:15:03:43 [Epoch 1 Batch 1200/16842] loss=0.3934, lr=0.0000028, metrics:accuracy:0.7126
INFO:root:15:04:04 [Epoch 1 Batch 1600/16842] loss=0.4645, lr=0.0000038, metrics:accuracy:0.7486
INFO:root:15:04:24 [Epoch 1 Batch 2000/16842] loss=0.4854, lr=0.0000047, metrics:accuracy:0.7704
INFO:root:15:04:44 [Epoch 1 Batch 2400/16842] loss=0.4224, lr=0.0000057, metrics:accuracy:0.7898
INFO:root:15:05:04 [Epoch 1 Batch 2800/16842] loss=0.4430, lr=0.0000067, metrics:accuracy:0.8038
INFO:root:15:05:22 [Epoch 1 Batch 3200/16842] loss=0.4287, lr=0.0000076, metrics:accuracy:0.8144
INFO:root:15:05:43 [Epoch 1 Batch 3600/16842] loss=0.4595, lr=0.0000086, metrics:accuracy:0.8216
INFO:root:15:06:00 [Epoch 1 Batch 4000/16842] loss=0.4143, lr=0.0000095, metrics:accuracy:0.8292
INFO:root:15:06:18 [Epoch 1 Batch 4400/16842] loss=0.3881, lr=0.0000105, metrics:accuracy:0.8356
INFO:root:15:06:35 [Epoch 1 Batch 4800/16842] loss=0.4419, lr=0.0000114, metrics:accuracy:0.8403
INFO:root:15:06:52 [Epoch 1 Batch 5200/16842] loss=0.3958, lr=0.0000124, metrics:accuracy:0.8454
INFO:root:15:07:08 [Epoch 1 Batch 5600/16842] loss=0.4093, lr=0.0000133, metrics:accuracy:0.8490
INFO:root:15:07:25 [Epoch 1 Batch 6000/16842] loss=0.4383, lr=0.0000143, metrics:accuracy:0.8518
INFO:root:15:07:42 [Epoch 1 Batch 6400/16842] loss=0.3724, lr=0.0000152, metrics:accuracy:0.8556
INFO:root:15:07:59 [Epoch 1 Batch 6800/16842] loss=0.3733, lr=0.0000162, metrics:accuracy:0.8588
INFO:root:15:08:17 [Epoch 1 Batch 7200/16842] loss=0.3562, lr=0.0000171, metrics:accuracy:0.8622
INFO:root:15:08:34 [Epoch 1 Batch 7600/16842] loss=0.4053, lr=0.0000181, metrics:accuracy:0.8645
INFO:root:15:08:51 [Epoch 1 Batch 8000/16842] loss=0.3959, lr=0.0000190, metrics:accuracy:0.8667
INFO:root:15:09:06 [Epoch 1 Batch 8400/16842] loss=0.3779, lr=0.0000200, metrics:accuracy:0.8690
INFO:root:15:09:21 [Epoch 1 Batch 8800/16842] loss=0.3945, lr=0.0000199, metrics:accuracy:0.8707
INFO:root:15:09:40 [Epoch 1 Batch 9200/16842] loss=0.3742, lr=0.0000198, metrics:accuracy:0.8724
INFO:root:15:09:55 [Epoch 1 Batch 9600/16842] loss=0.3385, lr=0.0000197, metrics:accuracy:0.8744
INFO:root:15:10:13 [Epoch 1 Batch 10000/16842] loss=0.3852, lr=0.0000196, metrics:accuracy:0.8759
INFO:root:15:10:30 [Epoch 1 Batch 10400/16842] loss=0.3299, lr=0.0000195, metrics:accuracy:0.8778
INFO:root:15:10:46 [Epoch 1 Batch 10800/16842] loss=0.3427, lr=0.0000194, metrics:accuracy:0.8794
INFO:root:15:11:06 [Epoch 1 Batch 11200/16842] loss=0.3343, lr=0.0000193, metrics:accuracy:0.8811
INFO:root:15:11:22 [Epoch 1 Batch 11600/16842] loss=0.3661, lr=0.0000192, metrics:accuracy:0.8823
INFO:root:15:11:38 [Epoch 1 Batch 12000/16842] loss=0.3343, lr=0.0000191, metrics:accuracy:0.8838
INFO:root:15:11:54 [Epoch 1 Batch 12400/16842] loss=0.3095, lr=0.0000189, metrics:accuracy:0.8854
INFO:root:15:12:10 [Epoch 1 Batch 12800/16842] loss=0.3409, lr=0.0000188, metrics:accuracy:0.8865
INFO:root:15:12:24 [Epoch 1 Batch 13200/16842] loss=0.3097, lr=0.0000187, metrics:accuracy:0.8878
INFO:root:15:12:41 [Epoch 1 Batch 13600/16842] loss=0.3054, lr=0.0000186, metrics:accuracy:0.8892
INFO:root:15:12:59 [Epoch 1 Batch 14000/16842] loss=0.3311, lr=0.0000185, metrics:accuracy:0.8903
INFO:root:15:13:14 [Epoch 1 Batch 14400/16842] loss=0.3198, lr=0.0000184, metrics:accuracy:0.8913
INFO:root:15:13:28 [Epoch 1 Batch 14800/16842] loss=0.3043, lr=0.0000183, metrics:accuracy:0.8924
INFO:root:15:13:45 [Epoch 1 Batch 15200/16842] loss=0.2814, lr=0.0000182, metrics:accuracy:0.8936
INFO:root:15:14:00 [Epoch 1 Batch 15600/16842] loss=0.2935, lr=0.0000181, metrics:accuracy:0.8947
INFO:root:15:14:15 [Epoch 1 Batch 16000/16842] loss=0.3184, lr=0.0000180, metrics:accuracy:0.8955
INFO:root:15:14:30 [Epoch 1 Batch 16400/16842] loss=0.2891, lr=0.0000179, metrics:accuracy:0.8965
INFO:root:15:14:46 [Epoch 1 Batch 16800/16842] loss=0.2799, lr=0.0000178, metrics:accuracy:0.8975
INFO:root:15:14:48 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:14:50 validation metrics:accuracy:0.9094
INFO:root:15:14:50 Time cost=1.89s, throughput=460.88 samples/s
INFO:root:15:14:51 params saved in: ./output_dir/model_bert_SST_0.params
INFO:root:15:14:51 Time cost=747.89s
INFO:root:15:15:16 [Epoch 2 Batch 400/16842] loss=0.2121, lr=0.0000177, metrics:accuracy:0.9525
INFO:root:15:15:33 [Epoch 2 Batch 800/16842] loss=0.2092, lr=0.0000176, metrics:accuracy:0.9559
INFO:root:15:15:55 [Epoch 2 Batch 1200/16842] loss=0.2437, lr=0.0000175, metrics:accuracy:0.9544
INFO:root:15:16:11 [Epoch 2 Batch 1600/16842] loss=0.2103, lr=0.0000174, metrics:accuracy:0.9555
INFO:root:15:16:26 [Epoch 2 Batch 2000/16842] loss=0.2388, lr=0.0000172, metrics:accuracy:0.9547
INFO:root:15:16:42 [Epoch 2 Batch 2400/16842] loss=0.2301, lr=0.0000171, metrics:accuracy:0.9546
INFO:root:15:16:58 [Epoch 2 Batch 2800/16842] loss=0.2415, lr=0.0000170, metrics:accuracy:0.9538
INFO:root:15:17:14 [Epoch 2 Batch 3200/16842] loss=0.2318, lr=0.0000169, metrics:accuracy:0.9536
INFO:root:15:17:29 [Epoch 2 Batch 3600/16842] loss=0.2190, lr=0.0000168, metrics:accuracy:0.9538
INFO:root:15:17:44 [Epoch 2 Batch 4000/16842] loss=0.2065, lr=0.0000167, metrics:accuracy:0.9542
INFO:root:15:18:05 [Epoch 2 Batch 4400/16842] loss=0.2195, lr=0.0000166, metrics:accuracy:0.9542
INFO:root:15:18:21 [Epoch 2 Batch 4800/16842] loss=0.2215, lr=0.0000165, metrics:accuracy:0.9539
INFO:root:15:18:37 [Epoch 2 Batch 5200/16842] loss=0.2519, lr=0.0000164, metrics:accuracy:0.9530
INFO:root:15:18:52 [Epoch 2 Batch 5600/16842] loss=0.2201, lr=0.0000163, metrics:accuracy:0.9532
INFO:root:15:19:08 [Epoch 2 Batch 6000/16842] loss=0.2028, lr=0.0000162, metrics:accuracy:0.9534
INFO:root:15:19:26 [Epoch 2 Batch 6400/16842] loss=0.2186, lr=0.0000161, metrics:accuracy:0.9535
INFO:root:15:19:41 [Epoch 2 Batch 6800/16842] loss=0.1887, lr=0.0000160, metrics:accuracy:0.9543
INFO:root:15:19:59 [Epoch 2 Batch 7200/16842] loss=0.2119, lr=0.0000159, metrics:accuracy:0.9545
INFO:root:15:20:14 [Epoch 2 Batch 7600/16842] loss=0.2274, lr=0.0000158, metrics:accuracy:0.9545
INFO:root:15:20:29 [Epoch 2 Batch 8000/16842] loss=0.2185, lr=0.0000157, metrics:accuracy:0.9546
INFO:root:15:20:44 [Epoch 2 Batch 8400/16842] loss=0.2113, lr=0.0000156, metrics:accuracy:0.9548
INFO:root:15:21:00 [Epoch 2 Batch 8800/16842] loss=0.2092, lr=0.0000155, metrics:accuracy:0.9550
INFO:root:15:21:19 [Epoch 2 Batch 9200/16842] loss=0.2369, lr=0.0000153, metrics:accuracy:0.9548
INFO:root:15:21:35 [Epoch 2 Batch 9600/16842] loss=0.2308, lr=0.0000152, metrics:accuracy:0.9546
INFO:root:15:21:50 [Epoch 2 Batch 10000/16842] loss=0.1986, lr=0.0000151, metrics:accuracy:0.9548
INFO:root:15:22:05 [Epoch 2 Batch 10400/16842] loss=0.2073, lr=0.0000150, metrics:accuracy:0.9548
INFO:root:15:22:21 [Epoch 2 Batch 10800/16842] loss=0.2019, lr=0.0000149, metrics:accuracy:0.9549
INFO:root:15:22:35 [Epoch 2 Batch 11200/16842] loss=0.1960, lr=0.0000148, metrics:accuracy:0.9552
INFO:root:15:22:50 [Epoch 2 Batch 11600/16842] loss=0.1666, lr=0.0000147, metrics:accuracy:0.9556
INFO:root:15:23:05 [Epoch 2 Batch 12000/16842] loss=0.2015, lr=0.0000146, metrics:accuracy:0.9557
INFO:root:15:23:21 [Epoch 2 Batch 12400/16842] loss=0.1874, lr=0.0000145, metrics:accuracy:0.9560
INFO:root:15:23:40 [Epoch 2 Batch 12800/16842] loss=0.2493, lr=0.0000144, metrics:accuracy:0.9558
INFO:root:15:23:56 [Epoch 2 Batch 13200/16842] loss=0.1828, lr=0.0000143, metrics:accuracy:0.9560
INFO:root:15:24:13 [Epoch 2 Batch 13600/16842] loss=0.1745, lr=0.0000142, metrics:accuracy:0.9563
INFO:root:15:24:28 [Epoch 2 Batch 14000/16842] loss=0.1932, lr=0.0000141, metrics:accuracy:0.9564
INFO:root:15:24:44 [Epoch 2 Batch 14400/16842] loss=0.2338, lr=0.0000140, metrics:accuracy:0.9563
INFO:root:15:25:00 [Epoch 2 Batch 14800/16842] loss=0.2411, lr=0.0000139, metrics:accuracy:0.9561
INFO:root:15:25:16 [Epoch 2 Batch 15200/16842] loss=0.2290, lr=0.0000138, metrics:accuracy:0.9560
INFO:root:15:25:32 [Epoch 2 Batch 15600/16842] loss=0.2175, lr=0.0000137, metrics:accuracy:0.9559
INFO:root:15:25:49 [Epoch 2 Batch 16000/16842] loss=0.2451, lr=0.0000136, metrics:accuracy:0.9557
INFO:root:15:26:05 [Epoch 2 Batch 16400/16842] loss=0.2231, lr=0.0000134, metrics:accuracy:0.9556
INFO:root:15:26:20 [Epoch 2 Batch 16800/16842] loss=0.1965, lr=0.0000133, metrics:accuracy:0.9557
INFO:root:15:26:21 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:26:23 validation metrics:accuracy:0.8991
INFO:root:15:26:23 Time cost=1.82s, throughput=480.02 samples/s
INFO:root:15:26:24 params saved in: ./output_dir/model_bert_SST_1.params
INFO:root:15:26:24 Time cost=693.10s
INFO:root:15:26:42 [Epoch 3 Batch 400/16842] loss=0.1493, lr=0.0000132, metrics:accuracy:0.9694
INFO:root:15:27:00 [Epoch 3 Batch 800/16842] loss=0.1042, lr=0.0000131, metrics:accuracy:0.9744
INFO:root:15:27:18 [Epoch 3 Batch 1200/16842] loss=0.1073, lr=0.0000130, metrics:accuracy:0.9750
INFO:root:15:27:34 [Epoch 3 Batch 1600/16842] loss=0.1384, lr=0.0000129, metrics:accuracy:0.9748
INFO:root:15:27:50 [Epoch 3 Batch 2000/16842] loss=0.1191, lr=0.0000128, metrics:accuracy:0.9750
INFO:root:15:28:04 [Epoch 3 Batch 2400/16842] loss=0.1154, lr=0.0000127, metrics:accuracy:0.9755
INFO:root:15:28:22 [Epoch 3 Batch 2800/16842] loss=0.1375, lr=0.0000126, metrics:accuracy:0.9753
INFO:root:15:28:38 [Epoch 3 Batch 3200/16842] loss=0.1292, lr=0.0000125, metrics:accuracy:0.9751
INFO:root:15:28:52 [Epoch 3 Batch 3600/16842] loss=0.1300, lr=0.0000124, metrics:accuracy:0.9749
INFO:root:15:29:07 [Epoch 3 Batch 4000/16842] loss=0.1146, lr=0.0000123, metrics:accuracy:0.9751
INFO:root:15:29:25 [Epoch 3 Batch 4400/16842] loss=0.1426, lr=0.0000122, metrics:accuracy:0.9750
INFO:root:15:29:40 [Epoch 3 Batch 4800/16842] loss=0.1177, lr=0.0000121, metrics:accuracy:0.9752
INFO:root:15:29:56 [Epoch 3 Batch 5200/16842] loss=0.1544, lr=0.0000120, metrics:accuracy:0.9748
INFO:root:15:30:12 [Epoch 3 Batch 5600/16842] loss=0.1378, lr=0.0000119, metrics:accuracy:0.9745
INFO:root:15:30:28 [Epoch 3 Batch 6000/16842] loss=0.1334, lr=0.0000117, metrics:accuracy:0.9744
INFO:root:15:30:43 [Epoch 3 Batch 6400/16842] loss=0.1175, lr=0.0000116, metrics:accuracy:0.9747
INFO:root:15:30:58 [Epoch 3 Batch 6800/16842] loss=0.1204, lr=0.0000115, metrics:accuracy:0.9748
INFO:root:15:31:13 [Epoch 3 Batch 7200/16842] loss=0.1315, lr=0.0000114, metrics:accuracy:0.9747
INFO:root:15:31:30 [Epoch 3 Batch 7600/16842] loss=0.0998, lr=0.0000113, metrics:accuracy:0.9749
INFO:root:15:31:46 [Epoch 3 Batch 8000/16842] loss=0.1359, lr=0.0000112, metrics:accuracy:0.9749
INFO:root:15:32:01 [Epoch 3 Batch 8400/16842] loss=0.1345, lr=0.0000111, metrics:accuracy:0.9748
INFO:root:15:32:16 [Epoch 3 Batch 8800/16842] loss=0.1639, lr=0.0000110, metrics:accuracy:0.9745
INFO:root:15:32:31 [Epoch 3 Batch 9200/16842] loss=0.1362, lr=0.0000109, metrics:accuracy:0.9744
INFO:root:15:32:45 [Epoch 3 Batch 9600/16842] loss=0.1210, lr=0.0000108, metrics:accuracy:0.9746
INFO:root:15:33:00 [Epoch 3 Batch 10000/16842] loss=0.1497, lr=0.0000107, metrics:accuracy:0.9744
INFO:root:15:33:16 [Epoch 3 Batch 10400/16842] loss=0.1309, lr=0.0000106, metrics:accuracy:0.9745
INFO:root:15:33:32 [Epoch 3 Batch 10800/16842] loss=0.1210, lr=0.0000105, metrics:accuracy:0.9746
INFO:root:15:33:52 [Epoch 3 Batch 11200/16842] loss=0.1713, lr=0.0000104, metrics:accuracy:0.9741
INFO:root:15:34:08 [Epoch 3 Batch 11600/16842] loss=0.1234, lr=0.0000103, metrics:accuracy:0.9742
INFO:root:15:34:22 [Epoch 3 Batch 12000/16842] loss=0.1319, lr=0.0000102, metrics:accuracy:0.9741
INFO:root:15:34:37 [Epoch 3 Batch 12400/16842] loss=0.1168, lr=0.0000101, metrics:accuracy:0.9741
INFO:root:15:34:53 [Epoch 3 Batch 12800/16842] loss=0.1080, lr=0.0000100, metrics:accuracy:0.9743
INFO:root:15:35:09 [Epoch 3 Batch 13200/16842] loss=0.1374, lr=0.0000098, metrics:accuracy:0.9742
INFO:root:15:35:25 [Epoch 3 Batch 13600/16842] loss=0.1357, lr=0.0000097, metrics:accuracy:0.9742
INFO:root:15:35:41 [Epoch 3 Batch 14000/16842] loss=0.1354, lr=0.0000096, metrics:accuracy:0.9742
INFO:root:15:35:57 [Epoch 3 Batch 14400/16842] loss=0.1323, lr=0.0000095, metrics:accuracy:0.9742
INFO:root:15:36:13 [Epoch 3 Batch 14800/16842] loss=0.1404, lr=0.0000094, metrics:accuracy:0.9741
INFO:root:15:36:28 [Epoch 3 Batch 15200/16842] loss=0.1534, lr=0.0000093, metrics:accuracy:0.9739
INFO:root:15:36:44 [Epoch 3 Batch 15600/16842] loss=0.1641, lr=0.0000092, metrics:accuracy:0.9737
INFO:root:15:37:00 [Epoch 3 Batch 16000/16842] loss=0.1449, lr=0.0000091, metrics:accuracy:0.9736
INFO:root:15:37:15 [Epoch 3 Batch 16400/16842] loss=0.1751, lr=0.0000090, metrics:accuracy:0.9734
INFO:root:15:37:31 [Epoch 3 Batch 16800/16842] loss=0.1062, lr=0.0000089, metrics:accuracy:0.9736
INFO:root:15:37:32 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:37:34 validation metrics:accuracy:0.9048
INFO:root:15:37:34 Time cost=1.86s, throughput=468.17 samples/s
INFO:root:15:37:35 params saved in: ./output_dir/model_bert_SST_2.params
INFO:root:15:37:35 Time cost=671.29s
INFO:root:15:37:52 [Epoch 4 Batch 400/16842] loss=0.0666, lr=0.0000088, metrics:accuracy:0.9869
INFO:root:15:38:13 [Epoch 4 Batch 800/16842] loss=0.0787, lr=0.0000087, metrics:accuracy:0.9862
INFO:root:15:38:29 [Epoch 4 Batch 1200/16842] loss=0.0578, lr=0.0000086, metrics:accuracy:0.9875
INFO:root:15:38:44 [Epoch 4 Batch 1600/16842] loss=0.0723, lr=0.0000085, metrics:accuracy:0.9870
INFO:root:15:38:59 [Epoch 4 Batch 2000/16842] loss=0.0849, lr=0.0000084, metrics:accuracy:0.9864
INFO:root:15:39:14 [Epoch 4 Batch 2400/16842] loss=0.0541, lr=0.0000083, metrics:accuracy:0.9871
INFO:root:15:39:29 [Epoch 4 Batch 2800/16842] loss=0.0992, lr=0.0000081, metrics:accuracy:0.9862
INFO:root:15:39:43 [Epoch 4 Batch 3200/16842] loss=0.0673, lr=0.0000080, metrics:accuracy:0.9866
INFO:root:15:39:58 [Epoch 4 Batch 3600/16842] loss=0.0712, lr=0.0000079, metrics:accuracy:0.9867
INFO:root:15:40:15 [Epoch 4 Batch 4000/16842] loss=0.0829, lr=0.0000078, metrics:accuracy:0.9866
INFO:root:15:40:31 [Epoch 4 Batch 4400/16842] loss=0.0788, lr=0.0000077, metrics:accuracy:0.9864
INFO:root:15:40:46 [Epoch 4 Batch 4800/16842] loss=0.0685, lr=0.0000076, metrics:accuracy:0.9864
INFO:root:15:41:02 [Epoch 4 Batch 5200/16842] loss=0.0883, lr=0.0000075, metrics:accuracy:0.9862
INFO:root:15:41:18 [Epoch 4 Batch 5600/16842] loss=0.0985, lr=0.0000074, metrics:accuracy:0.9858
INFO:root:15:41:33 [Epoch 4 Batch 6000/16842] loss=0.0690, lr=0.0000073, metrics:accuracy:0.9858
INFO:root:15:41:49 [Epoch 4 Batch 6400/16842] loss=0.0601, lr=0.0000072, metrics:accuracy:0.9860
INFO:root:15:42:04 [Epoch 4 Batch 6800/16842] loss=0.0791, lr=0.0000071, metrics:accuracy:0.9858
INFO:root:15:42:20 [Epoch 4 Batch 7200/16842] loss=0.0792, lr=0.0000070, metrics:accuracy:0.9857
INFO:root:15:42:35 [Epoch 4 Batch 7600/16842] loss=0.0702, lr=0.0000069, metrics:accuracy:0.9857
INFO:root:15:42:50 [Epoch 4 Batch 8000/16842] loss=0.0925, lr=0.0000068, metrics:accuracy:0.9855
INFO:root:15:43:06 [Epoch 4 Batch 8400/16842] loss=0.0953, lr=0.0000067, metrics:accuracy:0.9852
INFO:root:15:43:22 [Epoch 4 Batch 8800/16842] loss=0.0640, lr=0.0000066, metrics:accuracy:0.9854
INFO:root:15:43:38 [Epoch 4 Batch 9200/16842] loss=0.1055, lr=0.0000065, metrics:accuracy:0.9852
INFO:root:15:43:54 [Epoch 4 Batch 9600/16842] loss=0.0898, lr=0.0000064, metrics:accuracy:0.9851
INFO:root:15:44:10 [Epoch 4 Batch 10000/16842] loss=0.0802, lr=0.0000062, metrics:accuracy:0.9851
INFO:root:15:44:26 [Epoch 4 Batch 10400/16842] loss=0.0654, lr=0.0000061, metrics:accuracy:0.9852
INFO:root:15:44:41 [Epoch 4 Batch 10800/16842] loss=0.0736, lr=0.0000060, metrics:accuracy:0.9852
INFO:root:15:44:57 [Epoch 4 Batch 11200/16842] loss=0.0796, lr=0.0000059, metrics:accuracy:0.9852
INFO:root:15:45:11 [Epoch 4 Batch 11600/16842] loss=0.0572, lr=0.0000058, metrics:accuracy:0.9853
INFO:root:15:45:26 [Epoch 4 Batch 12000/16842] loss=0.1068, lr=0.0000057, metrics:accuracy:0.9850
INFO:root:15:45:41 [Epoch 4 Batch 12400/16842] loss=0.0732, lr=0.0000056, metrics:accuracy:0.9850
INFO:root:15:45:56 [Epoch 4 Batch 12800/16842] loss=0.0831, lr=0.0000055, metrics:accuracy:0.9850
INFO:root:15:46:12 [Epoch 4 Batch 13200/16842] loss=0.0978, lr=0.0000054, metrics:accuracy:0.9848
INFO:root:15:46:28 [Epoch 4 Batch 13600/16842] loss=0.0711, lr=0.0000053, metrics:accuracy:0.9848
INFO:root:15:46:43 [Epoch 4 Batch 14000/16842] loss=0.0652, lr=0.0000052, metrics:accuracy:0.9849
INFO:root:15:46:57 [Epoch 4 Batch 14400/16842] loss=0.0806, lr=0.0000051, metrics:accuracy:0.9849
INFO:root:15:47:12 [Epoch 4 Batch 14800/16842] loss=0.0669, lr=0.0000050, metrics:accuracy:0.9850
INFO:root:15:47:27 [Epoch 4 Batch 15200/16842] loss=0.1141, lr=0.0000049, metrics:accuracy:0.9848
INFO:root:15:47:42 [Epoch 4 Batch 15600/16842] loss=0.0729, lr=0.0000048, metrics:accuracy:0.9849
INFO:root:15:47:57 [Epoch 4 Batch 16000/16842] loss=0.1000, lr=0.0000047, metrics:accuracy:0.9848
INFO:root:15:48:13 [Epoch 4 Batch 16400/16842] loss=0.0902, lr=0.0000046, metrics:accuracy:0.9848
INFO:root:15:48:29 [Epoch 4 Batch 16800/16842] loss=0.0788, lr=0.0000045, metrics:accuracy:0.9848
INFO:root:15:48:30 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:48:32 validation metrics:accuracy:0.9048
INFO:root:15:48:32 Time cost=1.86s, throughput=469.21 samples/s
INFO:root:15:48:33 params saved in: ./output_dir/model_bert_SST_3.params
INFO:root:15:48:33 Time cost=657.83s
INFO:root:15:48:52 [Epoch 5 Batch 400/16842] loss=0.0380, lr=0.0000043, metrics:accuracy:0.9925
INFO:root:15:49:07 [Epoch 5 Batch 800/16842] loss=0.0591, lr=0.0000042, metrics:accuracy:0.9900
INFO:root:15:49:23 [Epoch 5 Batch 1200/16842] loss=0.0371, lr=0.0000041, metrics:accuracy:0.9912
INFO:root:15:49:39 [Epoch 5 Batch 1600/16842] loss=0.0495, lr=0.0000040, metrics:accuracy:0.9914
INFO:root:15:49:55 [Epoch 5 Batch 2000/16842] loss=0.0494, lr=0.0000039, metrics:accuracy:0.9914
INFO:root:15:50:10 [Epoch 5 Batch 2400/16842] loss=0.0655, lr=0.0000038, metrics:accuracy:0.9906
INFO:root:15:50:25 [Epoch 5 Batch 2800/16842] loss=0.0373, lr=0.0000037, metrics:accuracy:0.9908
INFO:root:15:50:41 [Epoch 5 Batch 3200/16842] loss=0.0298, lr=0.0000036, metrics:accuracy:0.9913
INFO:root:15:50:56 [Epoch 5 Batch 3600/16842] loss=0.0549, lr=0.0000035, metrics:accuracy:0.9912
INFO:root:15:51:12 [Epoch 5 Batch 4000/16842] loss=0.0357, lr=0.0000034, metrics:accuracy:0.9915
INFO:root:15:51:28 [Epoch 5 Batch 4400/16842] loss=0.0533, lr=0.0000033, metrics:accuracy:0.9914
INFO:root:15:51:44 [Epoch 5 Batch 4800/16842] loss=0.0276, lr=0.0000032, metrics:accuracy:0.9915
INFO:root:15:52:00 [Epoch 5 Batch 5200/16842] loss=0.0216, lr=0.0000031, metrics:accuracy:0.9918
INFO:root:15:52:15 [Epoch 5 Batch 5600/16842] loss=0.0545, lr=0.0000030, metrics:accuracy:0.9917
INFO:root:15:52:30 [Epoch 5 Batch 6000/16842] loss=0.0442, lr=0.0000029, metrics:accuracy:0.9917
INFO:root:15:52:45 [Epoch 5 Batch 6400/16842] loss=0.0249, lr=0.0000028, metrics:accuracy:0.9920
INFO:root:15:53:01 [Epoch 5 Batch 6800/16842] loss=0.0622, lr=0.0000026, metrics:accuracy:0.9918
INFO:root:15:53:17 [Epoch 5 Batch 7200/16842] loss=0.0622, lr=0.0000025, metrics:accuracy:0.9916
INFO:root:15:53:32 [Epoch 5 Batch 7600/16842] loss=0.0616, lr=0.0000024, metrics:accuracy:0.9914
INFO:root:15:53:48 [Epoch 5 Batch 8000/16842] loss=0.0416, lr=0.0000023, metrics:accuracy:0.9913
INFO:root:15:54:04 [Epoch 5 Batch 8400/16842] loss=0.0592, lr=0.0000022, metrics:accuracy:0.9912
INFO:root:15:54:18 [Epoch 5 Batch 8800/16842] loss=0.0809, lr=0.0000021, metrics:accuracy:0.9909
INFO:root:15:54:33 [Epoch 5 Batch 9200/16842] loss=0.0664, lr=0.0000020, metrics:accuracy:0.9908
INFO:root:15:54:48 [Epoch 5 Batch 9600/16842] loss=0.0355, lr=0.0000019, metrics:accuracy:0.9908
INFO:root:15:55:04 [Epoch 5 Batch 10000/16842] loss=0.0422, lr=0.0000018, metrics:accuracy:0.9909
INFO:root:15:55:18 [Epoch 5 Batch 10400/16842] loss=0.0504, lr=0.0000017, metrics:accuracy:0.9908
INFO:root:15:55:33 [Epoch 5 Batch 10800/16842] loss=0.0397, lr=0.0000016, metrics:accuracy:0.9908
INFO:root:15:55:50 [Epoch 5 Batch 11200/16842] loss=0.0429, lr=0.0000015, metrics:accuracy:0.9908
INFO:root:15:56:09 [Epoch 5 Batch 11600/16842] loss=0.0510, lr=0.0000014, metrics:accuracy:0.9908
INFO:root:15:56:24 [Epoch 5 Batch 12000/16842] loss=0.0320, lr=0.0000013, metrics:accuracy:0.9909
INFO:root:15:56:41 [Epoch 5 Batch 12400/16842] loss=0.0498, lr=0.0000012, metrics:accuracy:0.9909
INFO:root:15:56:57 [Epoch 5 Batch 12800/16842] loss=0.0709, lr=0.0000011, metrics:accuracy:0.9908
INFO:root:15:57:13 [Epoch 5 Batch 13200/16842] loss=0.0457, lr=0.0000010, metrics:accuracy:0.9908
INFO:root:15:57:29 [Epoch 5 Batch 13600/16842] loss=0.0276, lr=0.0000008, metrics:accuracy:0.9909
INFO:root:15:57:44 [Epoch 5 Batch 14000/16842] loss=0.0389, lr=0.0000007, metrics:accuracy:0.9909
INFO:root:15:57:59 [Epoch 5 Batch 14400/16842] loss=0.0323, lr=0.0000006, metrics:accuracy:0.9910
INFO:root:15:58:14 [Epoch 5 Batch 14800/16842] loss=0.0573, lr=0.0000005, metrics:accuracy:0.9909
INFO:root:15:58:28 [Epoch 5 Batch 15200/16842] loss=0.0584, lr=0.0000004, metrics:accuracy:0.9908
INFO:root:15:58:44 [Epoch 5 Batch 15600/16842] loss=0.0465, lr=0.0000003, metrics:accuracy:0.9908
INFO:root:15:58:59 [Epoch 5 Batch 16000/16842] loss=0.0481, lr=0.0000002, metrics:accuracy:0.9908
INFO:root:15:59:14 [Epoch 5 Batch 16400/16842] loss=0.0368, lr=0.0000001, metrics:accuracy:0.9908
INFO:root:15:59:29 [Epoch 5 Batch 16800/16842] loss=0.0321, lr=0.0000000, metrics:accuracy:0.9909
INFO:root:15:59:29 Finish training step: 84186
INFO:root:15:59:29 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:59:31 validation metrics:accuracy:0.9106
INFO:root:15:59:31 Time cost=1.88s, throughput=464.91 samples/s
INFO:root:15:59:32 params saved in: ./output_dir/model_bert_SST_4.params
INFO:root:15:59:32 Time cost=659.09s
INFO:root:15:59:32 Best model at epoch 4. Validation metrics:accuracy:0.9106
INFO:root:15:59:32 Now we are doing testing on test with gpu(0).
INFO:root:15:59:37 Time cost=4.44s, throughput=410.52 samples/s
INFO:root:11:39:06 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:11:39:13 processing dataset...
INFO:root:16:29:30 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:16:29:30 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:16:29:35 processing dataset...
INFO:root:16:29:40 Now we are doing BERT classification training on gpu(0)!
INFO:root:16:29:40 training steps=21046
INFO:root:16:29:55 [Epoch 1 Batch 400/8424] loss=0.6446, lr=0.0000019, metrics:accuracy:0.6291
INFO:root:16:30:08 [Epoch 1 Batch 800/8424] loss=0.3781, lr=0.0000038, metrics:accuracy:0.7378
INFO:root:16:30:23 [Epoch 1 Batch 1200/8424] loss=0.3326, lr=0.0000057, metrics:accuracy:0.7815
INFO:root:16:30:37 [Epoch 1 Batch 1600/8424] loss=0.2864, lr=0.0000076, metrics:accuracy:0.8097
INFO:root:16:30:52 [Epoch 1 Batch 2000/8424] loss=0.2928, lr=0.0000095, metrics:accuracy:0.8260
INFO:root:16:31:07 [Epoch 1 Batch 2400/8424] loss=0.2444, lr=0.0000114, metrics:accuracy:0.8390
INFO:root:16:31:23 [Epoch 1 Batch 2800/8424] loss=0.3040, lr=0.0000133, metrics:accuracy:0.8472
INFO:root:16:31:37 [Epoch 1 Batch 3200/8424] loss=0.2553, lr=0.0000152, metrics:accuracy:0.8540
INFO:root:16:31:52 [Epoch 1 Batch 3600/8424] loss=0.2547, lr=0.0000171, metrics:accuracy:0.8604
INFO:root:16:32:06 [Epoch 1 Batch 4000/8424] loss=0.2713, lr=0.0000190, metrics:accuracy:0.8653
INFO:root:16:32:21 [Epoch 1 Batch 4400/8424] loss=0.2666, lr=0.0000199, metrics:accuracy:0.8690
INFO:root:16:32:36 [Epoch 1 Batch 4800/8424] loss=0.2226, lr=0.0000197, metrics:accuracy:0.8732
INFO:root:16:32:50 [Epoch 1 Batch 5200/8424] loss=0.2413, lr=0.0000195, metrics:accuracy:0.8772
INFO:root:16:33:06 [Epoch 1 Batch 5600/8424] loss=0.2209, lr=0.0000193, metrics:accuracy:0.8808
INFO:root:16:33:26 [Epoch 1 Batch 6000/8424] loss=0.2294, lr=0.0000191, metrics:accuracy:0.8840
INFO:root:16:33:47 [Epoch 1 Batch 6400/8424] loss=0.2242, lr=0.0000188, metrics:accuracy:0.8868
INFO:root:16:34:08 [Epoch 1 Batch 6800/8424] loss=0.2286, lr=0.0000186, metrics:accuracy:0.8890
INFO:root:16:34:28 [Epoch 1 Batch 7200/8424] loss=0.2172, lr=0.0000184, metrics:accuracy:0.8917
INFO:root:16:34:49 [Epoch 1 Batch 7600/8424] loss=0.2042, lr=0.0000182, metrics:accuracy:0.8938
INFO:root:16:35:09 [Epoch 1 Batch 8000/8424] loss=0.2091, lr=0.0000180, metrics:accuracy:0.8959
INFO:root:16:35:29 [Epoch 1 Batch 8400/8424] loss=0.2135, lr=0.0000178, metrics:accuracy:0.8978
INFO:root:16:35:30 Now we are doing evaluation on dev with gpu(0).
INFO:root:16:35:32 validation metrics:accuracy:0.9209
INFO:root:16:35:32 Time cost=2.63s, throughput=332.10 samples/s
INFO:root:16:35:34 params saved in: ./output_dir/model_bert_SST_0.params
INFO:root:16:35:34 Time cost=354.83s
INFO:root:16:35:53 [Epoch 2 Batch 400/8424] loss=0.1732, lr=0.0000176, metrics:accuracy:0.9525
INFO:root:16:36:11 [Epoch 2 Batch 800/8424] loss=0.1741, lr=0.0000174, metrics:accuracy:0.9513
INFO:root:16:36:32 [Epoch 2 Batch 1200/8424] loss=0.1525, lr=0.0000171, metrics:accuracy:0.9547
INFO:root:16:36:48 [Epoch 2 Batch 1600/8424] loss=0.1542, lr=0.0000169, metrics:accuracy:0.9560
INFO:root:16:37:03 [Epoch 2 Batch 2000/8424] loss=0.1759, lr=0.0000167, metrics:accuracy:0.9563
INFO:root:16:37:18 [Epoch 2 Batch 2400/8424] loss=0.1591, lr=0.0000165, metrics:accuracy:0.9567
INFO:root:16:37:32 [Epoch 2 Batch 2800/8424] loss=0.1518, lr=0.0000163, metrics:accuracy:0.9572
INFO:root:16:37:46 [Epoch 2 Batch 3200/8424] loss=0.1971, lr=0.0000161, metrics:accuracy:0.9563
INFO:root:16:38:00 [Epoch 2 Batch 3600/8424] loss=0.1675, lr=0.0000159, metrics:accuracy:0.9564
INFO:root:16:38:15 [Epoch 2 Batch 4000/8424] loss=0.1580, lr=0.0000157, metrics:accuracy:0.9568
INFO:root:16:38:29 [Epoch 2 Batch 4400/8424] loss=0.1717, lr=0.0000155, metrics:accuracy:0.9569
INFO:root:16:38:43 [Epoch 2 Batch 4800/8424] loss=0.1789, lr=0.0000152, metrics:accuracy:0.9566
INFO:root:16:38:57 [Epoch 2 Batch 5200/8424] loss=0.1900, lr=0.0000150, metrics:accuracy:0.9561
INFO:root:16:39:12 [Epoch 2 Batch 5600/8424] loss=0.1551, lr=0.0000148, metrics:accuracy:0.9562
INFO:root:16:39:26 [Epoch 2 Batch 6000/8424] loss=0.1734, lr=0.0000146, metrics:accuracy:0.9564
INFO:root:16:39:39 [Epoch 2 Batch 6400/8424] loss=0.1522, lr=0.0000144, metrics:accuracy:0.9565
INFO:root:16:39:52 [Epoch 2 Batch 6800/8424] loss=0.1674, lr=0.0000142, metrics:accuracy:0.9565
INFO:root:16:40:05 [Epoch 2 Batch 7200/8424] loss=0.1641, lr=0.0000140, metrics:accuracy:0.9565
INFO:root:16:40:18 [Epoch 2 Batch 7600/8424] loss=0.1622, lr=0.0000138, metrics:accuracy:0.9564
INFO:root:16:40:30 [Epoch 2 Batch 8000/8424] loss=0.1693, lr=0.0000136, metrics:accuracy:0.9565
INFO:root:16:40:43 [Epoch 2 Batch 8400/8424] loss=0.1672, lr=0.0000133, metrics:accuracy:0.9564
INFO:root:16:40:44 Now we are doing evaluation on dev with gpu(0).
INFO:root:16:40:46 validation metrics:accuracy:0.9278
INFO:root:16:40:46 Time cost=1.86s, throughput=469.16 samples/s
INFO:root:16:40:47 params saved in: ./output_dir/model_bert_SST_1.params
INFO:root:16:40:47 Time cost=312.70s
INFO:root:16:41:00 [Epoch 3 Batch 400/8424] loss=0.1022, lr=0.0000131, metrics:accuracy:0.9756
INFO:root:16:41:13 [Epoch 3 Batch 800/8424] loss=0.1112, lr=0.0000129, metrics:accuracy:0.9758
INFO:root:16:41:26 [Epoch 3 Batch 1200/8424] loss=0.1077, lr=0.0000127, metrics:accuracy:0.9755
INFO:root:16:41:38 [Epoch 3 Batch 1600/8424] loss=0.0917, lr=0.0000125, metrics:accuracy:0.9765
INFO:root:16:41:51 [Epoch 3 Batch 2000/8424] loss=0.1120, lr=0.0000123, metrics:accuracy:0.9756
INFO:root:16:42:04 [Epoch 3 Batch 2400/8424] loss=0.1260, lr=0.0000121, metrics:accuracy:0.9743
INFO:root:16:42:17 [Epoch 3 Batch 2800/8424] loss=0.1238, lr=0.0000118, metrics:accuracy:0.9737
INFO:root:16:42:29 [Epoch 3 Batch 3200/8424] loss=0.1128, lr=0.0000116, metrics:accuracy:0.9736
INFO:root:16:42:42 [Epoch 3 Batch 3600/8424] loss=0.1245, lr=0.0000114, metrics:accuracy:0.9735
INFO:root:16:42:54 [Epoch 3 Batch 4000/8424] loss=0.0975, lr=0.0000112, metrics:accuracy:0.9737
INFO:root:16:43:07 [Epoch 3 Batch 4400/8424] loss=0.1052, lr=0.0000110, metrics:accuracy:0.9740
INFO:root:16:43:20 [Epoch 3 Batch 4800/8424] loss=0.1137, lr=0.0000108, metrics:accuracy:0.9739
INFO:root:16:43:32 [Epoch 3 Batch 5200/8424] loss=0.1379, lr=0.0000106, metrics:accuracy:0.9736
INFO:root:16:43:45 [Epoch 3 Batch 5600/8424] loss=0.1415, lr=0.0000104, metrics:accuracy:0.9731
INFO:root:16:43:58 [Epoch 3 Batch 6000/8424] loss=0.1044, lr=0.0000102, metrics:accuracy:0.9732
INFO:root:16:44:11 [Epoch 3 Batch 6400/8424] loss=0.1162, lr=0.0000099, metrics:accuracy:0.9732
INFO:root:16:44:25 [Epoch 3 Batch 6800/8424] loss=0.1303, lr=0.0000097, metrics:accuracy:0.9729
INFO:root:16:44:39 [Epoch 3 Batch 7200/8424] loss=0.1036, lr=0.0000095, metrics:accuracy:0.9732
INFO:root:16:44:53 [Epoch 3 Batch 7600/8424] loss=0.1164, lr=0.0000093, metrics:accuracy:0.9730
INFO:root:16:45:07 [Epoch 3 Batch 8000/8424] loss=0.1305, lr=0.0000091, metrics:accuracy:0.9729
INFO:root:16:45:21 [Epoch 3 Batch 8400/8424] loss=0.1166, lr=0.0000089, metrics:accuracy:0.9729
INFO:root:16:45:22 Now we are doing evaluation on dev with gpu(0).
INFO:root:16:45:24 validation metrics:accuracy:0.9369
INFO:root:16:45:24 Time cost=1.97s, throughput=441.53 samples/s
INFO:root:16:45:27 params saved in: ./output_dir/model_bert_SST_2.params
INFO:root:16:45:27 Time cost=279.91s
INFO:root:16:45:41 [Epoch 4 Batch 400/8424] loss=0.0517, lr=0.0000087, metrics:accuracy:0.9875
INFO:root:16:45:55 [Epoch 4 Batch 800/8424] loss=0.0606, lr=0.0000085, metrics:accuracy:0.9875
INFO:root:16:46:09 [Epoch 4 Batch 1200/8424] loss=0.0656, lr=0.0000082, metrics:accuracy:0.9866
INFO:root:16:46:22 [Epoch 4 Batch 1600/8424] loss=0.0888, lr=0.0000080, metrics:accuracy:0.9855
INFO:root:16:46:36 [Epoch 4 Batch 2000/8424] loss=0.0672, lr=0.0000078, metrics:accuracy:0.9854
INFO:root:16:46:49 [Epoch 4 Batch 2400/8424] loss=0.0800, lr=0.0000076, metrics:accuracy:0.9848
INFO:root:16:47:02 [Epoch 4 Batch 2800/8424] loss=0.0783, lr=0.0000074, metrics:accuracy:0.9845
INFO:root:16:47:16 [Epoch 4 Batch 3200/8424] loss=0.0900, lr=0.0000072, metrics:accuracy:0.9838
INFO:root:16:47:29 [Epoch 4 Batch 3600/8424] loss=0.0868, lr=0.0000070, metrics:accuracy:0.9836
INFO:root:16:47:43 [Epoch 4 Batch 4000/8424] loss=0.0834, lr=0.0000068, metrics:accuracy:0.9833
INFO:root:16:47:59 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:16:47:59 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:16:48:04 processing dataset...
INFO:root:16:48:08 Now we are doing BERT classification training on gpu(0)!
INFO:root:16:48:08 training steps=21046
INFO:root:16:48:23 [Epoch 1 Batch 400/8424] loss=0.6446, lr=0.0000019, metrics:accuracy:0.6291
INFO:root:16:48:37 [Epoch 1 Batch 800/8424] loss=0.3781, lr=0.0000038, metrics:accuracy:0.7378
INFO:root:16:56:15 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:16:56:15 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:16:56:20 processing dataset...
INFO:root:17:01:01 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:01:01 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:01:06 processing dataset...
INFO:root:17:01:10 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:01:10 training steps=21046
INFO:root:17:01:34 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:01:34 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:01:38 processing dataset...
INFO:root:17:01:42 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:01:42 training steps=21046
INFO:root:17:01:55 [Epoch 1 Batch 400/8424] loss=0.6446, lr=0.0000019, metrics:accuracy:0.6291
INFO:root:17:02:09 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:02:09 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:02:14 processing dataset...
INFO:root:17:02:18 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:02:18 training steps=21046
INFO:root:17:02:31 [Epoch 1 Batch 400/8424] loss=0.6446, lr=0.0000019, metrics:accuracy:0.6291
INFO:root:17:02:49 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:02:49 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:02:54 processing dataset...
INFO:root:17:02:58 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:02:58 training steps=21046
INFO:root:17:04:24 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:04:24 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:04:28 processing dataset...
INFO:root:17:04:32 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:04:32 training steps=21046
INFO:root:17:04:45 [Epoch 1 Batch 400/8424] loss=0.6446, lr=0.0000019, metrics:accuracy:0.6291
INFO:root:17:04:54 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:04:54 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:04:59 processing dataset...
INFO:root:17:05:03 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:05:03 training steps=21046
INFO:root:17:05:24 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:05:24 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:05:29 processing dataset...
INFO:root:17:05:34 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:05:34 training steps=21046
INFO:root:17:06:01 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:06:01 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:06:06 processing dataset...
INFO:root:17:06:10 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:06:10 training steps=21046
INFO:root:17:47:49 Namespace(accumulate=2, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:17:47:49 Using gradient accumulation. Effective batch size = batch_size * accumulate = 16
INFO:root:17:47:54 processing dataset...
INFO:root:17:47:58 Now we are doing BERT classification training on gpu(0)!
INFO:root:17:47:58 training steps=21046
INFO:root:17:48:14 [Epoch 1 Batch 400/8424] loss=0.6446, lr=0.0000019, metrics:accuracy:0.6291
INFO:root:18:41:37 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:18:41:42 processing dataset...
INFO:root:18:41:46 Now we are doing BERT classification training on gpu(0)!
INFO:root:18:41:46 training steps=10523
INFO:root:18:42:13 [Epoch 1 Batch 400/2108] loss=0.4250, lr=0.0000076, metrics:accuracy:0.7931
INFO:root:13:03:49 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:13:04:08 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:13:04:18 processing dataset...
INFO:root:13:04:23 Now we are doing BERT classification training on gpu(0)!
INFO:root:13:04:23 training steps=10523
INFO:root:13:04:52 [Epoch 1 Batch 400/2108] loss=0.4250, lr=0.0000076, metrics:accuracy:0.7931
INFO:root:13:05:20 [Epoch 1 Batch 800/2108] loss=0.2497, lr=0.0000152, metrics:accuracy:0.8478
INFO:root:13:05:48 [Epoch 1 Batch 1200/2108] loss=0.2322, lr=0.0000197, metrics:accuracy:0.8689
INFO:root:13:06:16 [Epoch 1 Batch 1600/2108] loss=0.2000, lr=0.0000188, metrics:accuracy:0.8832
INFO:root:13:06:44 [Epoch 1 Batch 2000/2108] loss=0.1835, lr=0.0000180, metrics:accuracy:0.8932
INFO:root:13:06:52 Now we are doing evaluation on dev with gpu(0).
INFO:root:13:06:54 validation metrics:accuracy:0.9151
INFO:root:13:06:54 Time cost=1.95s, throughput=448.14 samples/s
INFO:root:13:06:55 params saved in: ./output_dir/model_bert_SST_0.params
INFO:root:13:06:55 Time cost=151.96s
INFO:root:13:07:23 [Epoch 2 Batch 400/2108] loss=0.1198, lr=0.0000169, metrics:accuracy:0.9624
INFO:root:13:07:51 [Epoch 2 Batch 800/2108] loss=0.1274, lr=0.0000161, metrics:accuracy:0.9602
INFO:root:13:08:19 [Epoch 2 Batch 1200/2108] loss=0.1211, lr=0.0000152, metrics:accuracy:0.9605
INFO:root:13:08:47 [Epoch 2 Batch 1600/2108] loss=0.1278, lr=0.0000144, metrics:accuracy:0.9600
INFO:root:13:09:16 [Epoch 2 Batch 2000/2108] loss=0.1220, lr=0.0000135, metrics:accuracy:0.9598
INFO:root:13:09:24 Now we are doing evaluation on dev with gpu(0).
INFO:root:13:09:26 validation metrics:accuracy:0.9358
INFO:root:13:09:26 Time cost=1.95s, throughput=446.37 samples/s
INFO:root:13:09:29 params saved in: ./output_dir/model_bert_SST_1.params
INFO:root:13:09:29 Time cost=153.97s
INFO:root:13:09:58 [Epoch 3 Batch 400/2108] loss=0.0746, lr=0.0000125, metrics:accuracy:0.9780
INFO:root:13:10:28 [Epoch 3 Batch 800/2108] loss=0.0867, lr=0.0000116, metrics:accuracy:0.9759
INFO:root:13:10:58 [Epoch 3 Batch 1200/2108] loss=0.0825, lr=0.0000108, metrics:accuracy:0.9757
INFO:root:13:11:27 [Epoch 3 Batch 1600/2108] loss=0.0890, lr=0.0000099, metrics:accuracy:0.9751
INFO:root:13:11:56 [Epoch 3 Batch 2000/2108] loss=0.0791, lr=0.0000091, metrics:accuracy:0.9752
INFO:root:13:12:04 Now we are doing evaluation on dev with gpu(0).
INFO:root:13:12:06 validation metrics:accuracy:0.9300
INFO:root:13:12:06 Time cost=2.04s, throughput=427.75 samples/s
INFO:root:13:12:07 params saved in: ./output_dir/model_bert_SST_2.params
INFO:root:13:12:07 Time cost=157.88s
INFO:root:13:12:34 [Epoch 4 Batch 400/2108] loss=0.0555, lr=0.0000080, metrics:accuracy:0.9842
INFO:root:13:13:03 [Epoch 4 Batch 800/2108] loss=0.0588, lr=0.0000072, metrics:accuracy:0.9834
INFO:root:13:13:31 [Epoch 4 Batch 1200/2108] loss=0.0559, lr=0.0000063, metrics:accuracy:0.9836
INFO:root:13:13:59 [Epoch 4 Batch 1600/2108] loss=0.0529, lr=0.0000055, metrics:accuracy:0.9838
INFO:root:13:14:28 [Epoch 4 Batch 2000/2108] loss=0.0623, lr=0.0000046, metrics:accuracy:0.9835
INFO:root:13:14:36 Now we are doing evaluation on dev with gpu(0).
INFO:root:13:14:38 validation metrics:accuracy:0.9312
INFO:root:13:14:38 Time cost=1.97s, throughput=443.08 samples/s
INFO:root:13:14:39 params saved in: ./output_dir/model_bert_SST_3.params
INFO:root:13:14:39 Time cost=152.60s
INFO:root:13:15:09 [Epoch 5 Batch 400/2108] loss=0.0400, lr=0.0000036, metrics:accuracy:0.9889
INFO:root:13:15:38 [Epoch 5 Batch 800/2108] loss=0.0342, lr=0.0000027, metrics:accuracy:0.9892
INFO:root:13:16:07 [Epoch 5 Batch 1200/2108] loss=0.0446, lr=0.0000019, metrics:accuracy:0.9885
INFO:root:13:16:35 [Epoch 5 Batch 1600/2108] loss=0.0348, lr=0.0000010, metrics:accuracy:0.9889
INFO:root:13:17:03 [Epoch 5 Batch 2000/2108] loss=0.0402, lr=0.0000002, metrics:accuracy:0.9889
INFO:root:13:17:09 Finish training step: 10523
INFO:root:13:17:09 Now we are doing evaluation on dev with gpu(0).
INFO:root:13:17:11 validation metrics:accuracy:0.9335
INFO:root:13:17:11 Time cost=2.01s, throughput=433.97 samples/s
INFO:root:13:17:12 params saved in: ./output_dir/model_bert_SST_4.params
INFO:root:13:17:12 Time cost=152.52s
INFO:root:13:17:12 Best model at epoch 1. Validation metrics:accuracy:0.9358
INFO:root:13:17:12 Now we are doing testing on test with gpu(0).
INFO:root:13:17:16 Time cost=3.86s, throughput=472.60 samples/s
INFO:root:14:27:24 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:14:27:35 processing dataset...
INFO:root:14:27:41 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:27:42 training steps=10523
INFO:root:14:28:16 Namespace(accumulate=2, batch_size=16, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:14:28:16 Using gradient accumulation. Effective batch size = batch_size * accumulate = 32
INFO:root:14:28:25 processing dataset...
INFO:root:14:28:29 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:28:29 training steps=10523
INFO:root:14:29:16 [Epoch 1 Batch 400/4214] loss=0.5116, lr=0.0000038, metrics:accuracy:0.7307
INFO:root:14:30:02 [Epoch 1 Batch 800/4214] loss=0.2746, lr=0.0000076, metrics:accuracy:0.8113
INFO:root:14:30:48 [Epoch 1 Batch 1200/4214] loss=0.2370, lr=0.0000114, metrics:accuracy:0.8453
INFO:root:14:31:31 [Epoch 1 Batch 1600/4214] loss=0.2389, lr=0.0000152, metrics:accuracy:0.8614
INFO:root:14:32:16 [Epoch 1 Batch 2000/4214] loss=0.2137, lr=0.0000190, metrics:accuracy:0.8741
INFO:root:14:32:59 [Epoch 1 Batch 2400/4214] loss=0.2012, lr=0.0000197, metrics:accuracy:0.8827
INFO:root:14:33:44 [Epoch 1 Batch 2800/4214] loss=0.1886, lr=0.0000193, metrics:accuracy:0.8899
INFO:root:14:42:16 Namespace(accumulate=4, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:14:42:16 Using gradient accumulation. Effective batch size = batch_size * accumulate = 32
INFO:root:14:42:24 processing dataset...
INFO:root:14:42:31 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:42:31 training steps=10523
INFO:root:14:43:01 [Epoch 1 Batch 400/8424] loss=0.6792, lr=0.0000019, metrics:accuracy:0.5816
INFO:root:14:43:29 [Epoch 1 Batch 800/8424] loss=0.4099, lr=0.0000038, metrics:accuracy:0.7087
INFO:root:14:43:56 [Epoch 1 Batch 1200/8424] loss=0.2836, lr=0.0000057, metrics:accuracy:0.7693
INFO:root:14:44:23 [Epoch 1 Batch 1600/8424] loss=0.2458, lr=0.0000076, metrics:accuracy:0.8032
INFO:root:14:44:49 [Epoch 1 Batch 2000/8424] loss=0.2461, lr=0.0000095, metrics:accuracy:0.8240
INFO:root:14:45:15 [Epoch 1 Batch 2400/8424] loss=0.2163, lr=0.0000114, metrics:accuracy:0.8396
INFO:root:14:45:43 [Epoch 1 Batch 2800/8424] loss=0.2395, lr=0.0000133, metrics:accuracy:0.8493
INFO:root:14:46:12 [Epoch 1 Batch 3200/8424] loss=0.2038, lr=0.0000152, metrics:accuracy:0.8583
INFO:root:14:46:41 [Epoch 1 Batch 3600/8424] loss=0.2174, lr=0.0000171, metrics:accuracy:0.8653
INFO:root:14:47:12 [Epoch 1 Batch 4000/8424] loss=0.2164, lr=0.0000190, metrics:accuracy:0.8707
INFO:root:14:47:42 [Epoch 1 Batch 4400/8424] loss=0.2205, lr=0.0000199, metrics:accuracy:0.8753
INFO:root:14:48:09 [Epoch 1 Batch 4800/8424] loss=0.1977, lr=0.0000197, metrics:accuracy:0.8801
INFO:root:14:48:38 [Epoch 1 Batch 5200/8424] loss=0.1985, lr=0.0000195, metrics:accuracy:0.8842
INFO:root:14:49:05 [Epoch 1 Batch 5600/8424] loss=0.1851, lr=0.0000193, metrics:accuracy:0.8876
INFO:root:14:49:35 [Epoch 1 Batch 6000/8424] loss=0.1856, lr=0.0000191, metrics:accuracy:0.8910
INFO:root:14:50:02 [Epoch 1 Batch 6400/8424] loss=0.1952, lr=0.0000188, metrics:accuracy:0.8937
INFO:root:14:50:32 [Epoch 1 Batch 6800/8424] loss=0.1895, lr=0.0000186, metrics:accuracy:0.8959
INFO:root:14:50:59 [Epoch 1 Batch 7200/8424] loss=0.1687, lr=0.0000184, metrics:accuracy:0.8987
INFO:root:14:51:29 [Epoch 1 Batch 7600/8424] loss=0.1849, lr=0.0000182, metrics:accuracy:0.9004
INFO:root:14:51:57 [Epoch 1 Batch 8000/8424] loss=0.1714, lr=0.0000180, metrics:accuracy:0.9024
INFO:root:14:52:25 [Epoch 1 Batch 8400/8424] loss=0.1656, lr=0.0000178, metrics:accuracy:0.9046
INFO:root:14:52:26 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:52:31 validation metrics:accuracy:0.9243
INFO:root:14:52:31 Time cost=4.56s, throughput=191.34 samples/s
INFO:root:14:52:39 params saved in: ./output_dir/model_bert_SST_0.params
INFO:root:14:52:39 Time cost=608.17s
INFO:root:14:53:08 [Epoch 2 Batch 400/8424] loss=0.1375, lr=0.0000176, metrics:accuracy:0.9622
INFO:root:14:53:36 [Epoch 2 Batch 800/8424] loss=0.1268, lr=0.0000174, metrics:accuracy:0.9598
INFO:root:14:54:05 [Epoch 2 Batch 1200/8424] loss=0.1248, lr=0.0000171, metrics:accuracy:0.9612
INFO:root:14:54:33 [Epoch 2 Batch 1600/8424] loss=0.1223, lr=0.0000169, metrics:accuracy:0.9627
INFO:root:14:55:01 [Epoch 2 Batch 2000/8424] loss=0.1304, lr=0.0000167, metrics:accuracy:0.9625
INFO:root:14:55:29 [Epoch 2 Batch 2400/8424] loss=0.1273, lr=0.0000165, metrics:accuracy:0.9623
INFO:root:14:55:55 [Epoch 2 Batch 2800/8424] loss=0.1138, lr=0.0000163, metrics:accuracy:0.9622
INFO:root:14:56:23 [Epoch 2 Batch 3200/8424] loss=0.1320, lr=0.0000161, metrics:accuracy:0.9618
INFO:root:14:56:51 [Epoch 2 Batch 3600/8424] loss=0.1367, lr=0.0000159, metrics:accuracy:0.9620
INFO:root:14:57:21 [Epoch 2 Batch 4000/8424] loss=0.1256, lr=0.0000157, metrics:accuracy:0.9622
INFO:root:14:57:50 [Epoch 2 Batch 4400/8424] loss=0.1365, lr=0.0000155, metrics:accuracy:0.9623
INFO:root:14:58:17 [Epoch 2 Batch 4800/8424] loss=0.1379, lr=0.0000152, metrics:accuracy:0.9623
INFO:root:14:58:44 [Epoch 2 Batch 5200/8424] loss=0.1548, lr=0.0000150, metrics:accuracy:0.9621
INFO:root:14:59:12 [Epoch 2 Batch 5600/8424] loss=0.1210, lr=0.0000148, metrics:accuracy:0.9623
INFO:root:14:59:39 [Epoch 2 Batch 6000/8424] loss=0.1311, lr=0.0000146, metrics:accuracy:0.9622
INFO:root:15:00:07 [Epoch 2 Batch 6400/8424] loss=0.1369, lr=0.0000144, metrics:accuracy:0.9618
INFO:root:15:00:35 [Epoch 2 Batch 6800/8424] loss=0.1231, lr=0.0000142, metrics:accuracy:0.9620
INFO:root:15:01:02 [Epoch 2 Batch 7200/8424] loss=0.1253, lr=0.0000140, metrics:accuracy:0.9621
INFO:root:15:01:30 [Epoch 2 Batch 7600/8424] loss=0.1439, lr=0.0000138, metrics:accuracy:0.9620
INFO:root:15:01:58 [Epoch 2 Batch 8000/8424] loss=0.1325, lr=0.0000136, metrics:accuracy:0.9620
INFO:root:15:02:26 [Epoch 2 Batch 8400/8424] loss=0.1291, lr=0.0000133, metrics:accuracy:0.9621
INFO:root:15:02:28 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:02:32 validation metrics:accuracy:0.9266
INFO:root:15:02:32 Time cost=4.42s, throughput=197.36 samples/s
INFO:root:15:02:37 params saved in: ./output_dir/model_bert_SST_1.params
INFO:root:15:02:37 Time cost=597.96s
INFO:root:15:03:09 [Epoch 3 Batch 400/8424] loss=0.0783, lr=0.0000131, metrics:accuracy:0.9800
INFO:root:15:03:36 [Epoch 3 Batch 800/8424] loss=0.0773, lr=0.0000129, metrics:accuracy:0.9809
INFO:root:15:04:04 [Epoch 3 Batch 1200/8424] loss=0.0965, lr=0.0000127, metrics:accuracy:0.9800
INFO:root:15:04:33 [Epoch 3 Batch 1600/8424] loss=0.0857, lr=0.0000125, metrics:accuracy:0.9795
INFO:root:15:05:04 [Epoch 3 Batch 2000/8424] loss=0.1004, lr=0.0000123, metrics:accuracy:0.9785
INFO:root:15:05:34 [Epoch 3 Batch 2400/8424] loss=0.0916, lr=0.0000121, metrics:accuracy:0.9785
INFO:root:15:06:00 [Epoch 3 Batch 2800/8424] loss=0.0964, lr=0.0000119, metrics:accuracy:0.9784
INFO:root:15:06:29 [Epoch 3 Batch 3200/8424] loss=0.1062, lr=0.0000116, metrics:accuracy:0.9780
INFO:root:15:06:56 [Epoch 3 Batch 3600/8424] loss=0.1042, lr=0.0000114, metrics:accuracy:0.9775
INFO:root:15:07:23 [Epoch 3 Batch 4000/8424] loss=0.0805, lr=0.0000112, metrics:accuracy:0.9776
INFO:root:15:07:51 [Epoch 3 Batch 4400/8424] loss=0.0884, lr=0.0000110, metrics:accuracy:0.9776
INFO:root:15:08:17 [Epoch 3 Batch 4800/8424] loss=0.0904, lr=0.0000108, metrics:accuracy:0.9778
INFO:root:15:08:44 [Epoch 3 Batch 5200/8424] loss=0.0972, lr=0.0000106, metrics:accuracy:0.9777
INFO:root:15:09:11 [Epoch 3 Batch 5600/8424] loss=0.1093, lr=0.0000104, metrics:accuracy:0.9773
INFO:root:15:09:36 [Epoch 3 Batch 6000/8424] loss=0.0872, lr=0.0000102, metrics:accuracy:0.9775
INFO:root:15:10:03 [Epoch 3 Batch 6400/8424] loss=0.0970, lr=0.0000100, metrics:accuracy:0.9774
INFO:root:15:10:32 [Epoch 3 Batch 6800/8424] loss=0.1036, lr=0.0000097, metrics:accuracy:0.9771
INFO:root:15:11:01 [Epoch 3 Batch 7200/8424] loss=0.0832, lr=0.0000095, metrics:accuracy:0.9772
INFO:root:15:11:28 [Epoch 3 Batch 7600/8424] loss=0.1056, lr=0.0000093, metrics:accuracy:0.9771
INFO:root:15:11:53 [Epoch 3 Batch 8000/8424] loss=0.1034, lr=0.0000091, metrics:accuracy:0.9770
INFO:root:15:12:21 [Epoch 3 Batch 8400/8424] loss=0.0888, lr=0.0000089, metrics:accuracy:0.9771
INFO:root:15:12:23 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:12:27 validation metrics:accuracy:0.9335
INFO:root:15:12:27 Time cost=4.21s, throughput=207.35 samples/s
INFO:root:15:12:40 params saved in: ./output_dir/model_bert_SST_2.params
INFO:root:15:12:40 Time cost=602.17s
INFO:root:15:13:07 [Epoch 4 Batch 400/8424] loss=0.0493, lr=0.0000087, metrics:accuracy:0.9869
INFO:root:15:13:33 [Epoch 4 Batch 800/8424] loss=0.0514, lr=0.0000085, metrics:accuracy:0.9875
INFO:root:15:13:59 [Epoch 4 Batch 1200/8424] loss=0.0509, lr=0.0000082, metrics:accuracy:0.9879
INFO:root:15:14:27 [Epoch 4 Batch 1600/8424] loss=0.0634, lr=0.0000080, metrics:accuracy:0.9879
INFO:root:15:14:52 [Epoch 4 Batch 2000/8424] loss=0.0597, lr=0.0000078, metrics:accuracy:0.9875
INFO:root:15:15:19 [Epoch 4 Batch 2400/8424] loss=0.0503, lr=0.0000076, metrics:accuracy:0.9876
INFO:root:15:15:46 [Epoch 4 Batch 2800/8424] loss=0.0724, lr=0.0000074, metrics:accuracy:0.9870
INFO:root:15:16:13 [Epoch 4 Batch 3200/8424] loss=0.0785, lr=0.0000072, metrics:accuracy:0.9860
INFO:root:15:16:40 [Epoch 4 Batch 3600/8424] loss=0.0746, lr=0.0000070, metrics:accuracy:0.9856
INFO:root:15:17:07 [Epoch 4 Batch 4000/8424] loss=0.0757, lr=0.0000068, metrics:accuracy:0.9853
INFO:root:15:17:34 [Epoch 4 Batch 4400/8424] loss=0.0736, lr=0.0000066, metrics:accuracy:0.9851
INFO:root:15:18:01 [Epoch 4 Batch 4800/8424] loss=0.0665, lr=0.0000063, metrics:accuracy:0.9849
INFO:root:15:18:27 [Epoch 4 Batch 5200/8424] loss=0.0664, lr=0.0000061, metrics:accuracy:0.9849
INFO:root:15:18:53 [Epoch 4 Batch 5600/8424] loss=0.0851, lr=0.0000059, metrics:accuracy:0.9845
INFO:root:15:19:19 [Epoch 4 Batch 6000/8424] loss=0.0552, lr=0.0000057, metrics:accuracy:0.9845
INFO:root:15:19:45 [Epoch 4 Batch 6400/8424] loss=0.0776, lr=0.0000055, metrics:accuracy:0.9843
INFO:root:15:20:13 [Epoch 4 Batch 6800/8424] loss=0.0663, lr=0.0000053, metrics:accuracy:0.9843
INFO:root:15:20:40 [Epoch 4 Batch 7200/8424] loss=0.0730, lr=0.0000051, metrics:accuracy:0.9842
INFO:root:15:21:06 [Epoch 4 Batch 7600/8424] loss=0.0617, lr=0.0000049, metrics:accuracy:0.9843
INFO:root:15:21:32 [Epoch 4 Batch 8000/8424] loss=0.0516, lr=0.0000047, metrics:accuracy:0.9844
INFO:root:15:21:58 [Epoch 4 Batch 8400/8424] loss=0.0718, lr=0.0000044, metrics:accuracy:0.9845
INFO:root:15:21:59 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:22:04 validation metrics:accuracy:0.9381
INFO:root:15:22:04 Time cost=4.45s, throughput=196.03 samples/s
INFO:root:15:22:09 params saved in: ./output_dir/model_bert_SST_3.params
INFO:root:15:22:09 Time cost=569.61s
INFO:root:15:22:35 [Epoch 5 Batch 400/8424] loss=0.0254, lr=0.0000042, metrics:accuracy:0.9934
INFO:root:15:23:01 [Epoch 5 Batch 800/8424] loss=0.0296, lr=0.0000040, metrics:accuracy:0.9933
INFO:root:15:23:27 [Epoch 5 Batch 1200/8424] loss=0.0555, lr=0.0000038, metrics:accuracy:0.9918
INFO:root:15:23:53 [Epoch 5 Batch 1600/8424] loss=0.0508, lr=0.0000036, metrics:accuracy:0.9906
INFO:root:15:24:19 [Epoch 5 Batch 2000/8424] loss=0.0454, lr=0.0000034, metrics:accuracy:0.9904
INFO:root:15:24:45 [Epoch 5 Batch 2400/8424] loss=0.0384, lr=0.0000032, metrics:accuracy:0.9905
INFO:root:15:25:11 [Epoch 5 Batch 2800/8424] loss=0.0434, lr=0.0000030, metrics:accuracy:0.9903
INFO:root:15:25:37 [Epoch 5 Batch 3200/8424] loss=0.0299, lr=0.0000027, metrics:accuracy:0.9906
INFO:root:15:26:02 [Epoch 5 Batch 3600/8424] loss=0.0426, lr=0.0000025, metrics:accuracy:0.9906
INFO:root:15:26:28 [Epoch 5 Batch 4000/8424] loss=0.0442, lr=0.0000023, metrics:accuracy:0.9906
INFO:root:15:26:55 [Epoch 5 Batch 4400/8424] loss=0.0381, lr=0.0000021, metrics:accuracy:0.9906
INFO:root:15:27:24 [Epoch 5 Batch 4800/8424] loss=0.0462, lr=0.0000019, metrics:accuracy:0.9904
INFO:root:15:27:50 [Epoch 5 Batch 5200/8424] loss=0.0374, lr=0.0000017, metrics:accuracy:0.9905
INFO:root:15:28:15 [Epoch 5 Batch 5600/8424] loss=0.0444, lr=0.0000015, metrics:accuracy:0.9906
INFO:root:15:28:41 [Epoch 5 Batch 6000/8424] loss=0.0323, lr=0.0000013, metrics:accuracy:0.9907
INFO:root:15:29:08 [Epoch 5 Batch 6400/8424] loss=0.0459, lr=0.0000011, metrics:accuracy:0.9906
INFO:root:15:29:35 [Epoch 5 Batch 6800/8424] loss=0.0360, lr=0.0000008, metrics:accuracy:0.9907
INFO:root:15:30:01 [Epoch 5 Batch 7200/8424] loss=0.0384, lr=0.0000006, metrics:accuracy:0.9907
INFO:root:15:30:29 [Epoch 5 Batch 7600/8424] loss=0.0530, lr=0.0000004, metrics:accuracy:0.9906
INFO:root:15:30:57 [Epoch 5 Batch 8000/8424] loss=0.0424, lr=0.0000002, metrics:accuracy:0.9905
INFO:root:15:31:22 Finish training step: 10523
INFO:root:15:31:22 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:31:26 validation metrics:accuracy:0.9335
INFO:root:15:31:26 Time cost=4.19s, throughput=208.11 samples/s
INFO:root:15:31:33 params saved in: ./output_dir/model_bert_SST_4.params
INFO:root:15:31:33 Time cost=564.10s
INFO:root:15:31:34 Best model at epoch 3. Validation metrics:accuracy:0.9381
INFO:root:15:31:34 Now we are doing testing on test with gpu(0).
INFO:root:15:31:44 Time cost=9.65s, throughput=188.96 samples/s
INFO:root:14:30:40 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_12_768_12', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float16', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:14:30:40 Using AMP
INFO:root:14:31:00 processing dataset...
INFO:root:14:31:05 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:31:05 training steps=10523
WARNING:py.warnings:14:31:23 finetune_classifier.py:643: UserWarning: nan or inf is detected. Clipping results will be undefined.
  nlp.utils.clip_grad_global_norm(params, 1)

INFO:root:14:31:23 AMP: decreasing loss scale to 32768.000000
INFO:root:14:31:34 [Epoch 1 Batch 400/2108] loss=0.7097, lr=0.0000076, metrics:accuracy:0.5605
INFO:root:14:31:57 [Epoch 1 Batch 800/2108] loss=0.6863, lr=0.0000152, metrics:accuracy:0.5577
INFO:root:14:32:22 [Epoch 1 Batch 1200/2108] loss=0.6745, lr=0.0000197, metrics:accuracy:0.5649
INFO:root:14:32:22 AMP: decreasing loss scale to 16384.000000
INFO:root:14:32:46 [Epoch 1 Batch 1600/2108] loss=0.6438, lr=0.0000188, metrics:accuracy:0.5827
INFO:root:14:33:10 [Epoch 1 Batch 2000/2108] loss=0.5431, lr=0.0000180, metrics:accuracy:0.6151
INFO:root:14:33:16 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:33:18 validation metrics:accuracy:0.7729
INFO:root:14:33:18 Time cost=2.17s, throughput=402.70 samples/s
INFO:root:14:33:23 params saved in: ./output_dir/model_bert_SST_0.params
INFO:root:14:33:23 Time cost=137.73s
INFO:root:14:33:46 [Epoch 2 Batch 400/2108] loss=0.4225, lr=0.0000169, metrics:accuracy:0.8168
INFO:root:14:34:11 [Epoch 2 Batch 800/2108] loss=0.3816, lr=0.0000161, metrics:accuracy:0.8278
INFO:root:14:34:11 AMP: decreasing loss scale to 8192.000000
INFO:root:14:34:35 [Epoch 2 Batch 1200/2108] loss=0.3601, lr=0.0000152, metrics:accuracy:0.8349
INFO:root:14:34:59 [Epoch 2 Batch 1600/2108] loss=0.3523, lr=0.0000144, metrics:accuracy:0.8392
INFO:root:14:35:23 [Epoch 2 Batch 2000/2108] loss=0.3375, lr=0.0000135, metrics:accuracy:0.8433
INFO:root:14:35:30 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:35:32 validation metrics:accuracy:0.8567
INFO:root:14:35:32 Time cost=1.85s, throughput=472.13 samples/s
INFO:root:14:35:37 params saved in: ./output_dir/model_bert_SST_1.params
INFO:root:14:35:37 Time cost=133.90s
INFO:root:14:36:01 [Epoch 3 Batch 400/2108] loss=0.3276, lr=0.0000125, metrics:accuracy:0.8656
INFO:root:14:36:20 AMP: increasing loss scale to 16384.000000
INFO:root:14:36:27 [Epoch 3 Batch 800/2108] loss=0.3161, lr=0.0000116, metrics:accuracy:0.8686
INFO:root:14:36:51 [Epoch 3 Batch 1200/2108] loss=0.3125, lr=0.0000108, metrics:accuracy:0.8699
INFO:root:14:37:16 [Epoch 3 Batch 1600/2108] loss=0.3158, lr=0.0000099, metrics:accuracy:0.8706
INFO:root:14:37:33 AMP: decreasing loss scale to 8192.000000
INFO:root:14:37:39 [Epoch 3 Batch 2000/2108] loss=0.2979, lr=0.0000091, metrics:accuracy:0.8721
INFO:root:14:37:46 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:37:48 validation metrics:accuracy:0.8658
INFO:root:14:37:48 Time cost=1.81s, throughput=481.76 samples/s
INFO:root:14:37:54 params saved in: ./output_dir/model_bert_SST_2.params
INFO:root:14:37:54 Time cost=137.03s
INFO:root:14:38:17 [Epoch 4 Batch 400/2108] loss=0.3004, lr=0.0000080, metrics:accuracy:0.8793
INFO:root:14:38:42 [Epoch 4 Batch 800/2108] loss=0.3112, lr=0.0000072, metrics:accuracy:0.8753
INFO:root:14:39:07 [Epoch 4 Batch 1200/2108] loss=0.2942, lr=0.0000063, metrics:accuracy:0.8775
INFO:root:14:39:31 [Epoch 4 Batch 1600/2108] loss=0.2906, lr=0.0000055, metrics:accuracy:0.8785
INFO:root:14:39:42 AMP: increasing loss scale to 16384.000000
INFO:root:14:39:55 [Epoch 4 Batch 2000/2108] loss=0.2954, lr=0.0000046, metrics:accuracy:0.8787
INFO:root:14:40:02 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:40:04 validation metrics:accuracy:0.8716
INFO:root:14:40:04 Time cost=1.75s, throughput=499.08 samples/s
INFO:root:14:40:06 params saved in: ./output_dir/model_bert_SST_3.params
INFO:root:14:40:06 Time cost=132.05s
INFO:root:14:40:30 [Epoch 5 Batch 400/2108] loss=0.2931, lr=0.0000036, metrics:accuracy:0.8795
INFO:root:14:40:54 [Epoch 5 Batch 800/2108] loss=0.2871, lr=0.0000027, metrics:accuracy:0.8814
INFO:root:14:41:19 [Epoch 5 Batch 1200/2108] loss=0.2988, lr=0.0000019, metrics:accuracy:0.8805
INFO:root:14:41:43 [Epoch 5 Batch 1600/2108] loss=0.2868, lr=0.0000010, metrics:accuracy:0.8810
INFO:root:14:41:48 AMP: increasing loss scale to 32768.000000
INFO:root:14:41:48 AMP: decreasing loss scale to 16384.000000
INFO:root:14:42:07 [Epoch 5 Batch 2000/2108] loss=0.2851, lr=0.0000002, metrics:accuracy:0.8814
INFO:root:14:42:12 Finish training step: 10523
INFO:root:14:42:12 Now we are doing evaluation on dev with gpu(0).
INFO:root:14:42:14 validation metrics:accuracy:0.8716
INFO:root:14:42:14 Time cost=1.84s, throughput=472.64 samples/s
INFO:root:14:42:17 params saved in: ./output_dir/model_bert_SST_4.params
INFO:root:14:42:17 Time cost=131.09s
INFO:root:14:42:17 Best model at epoch 3. Validation metrics:accuracy:0.8716
INFO:root:14:42:17 Now we are doing testing on test with gpu(0).
INFO:root:14:42:21 Time cost=3.63s, throughput=502.04 samples/s
INFO:root:14:47:04 Namespace(accumulate=None, batch_size=32, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:14:47:17 processing dataset...
INFO:root:14:47:21 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:47:21 training steps=10523
INFO:root:14:48:07 Namespace(accumulate=2, batch_size=16, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:14:48:07 Using gradient accumulation. Effective batch size = batch_size * accumulate = 32
INFO:root:14:48:19 processing dataset...
INFO:root:14:48:23 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:48:23 training steps=10523
INFO:root:14:49:03 [Epoch 1 Batch 400/4214] loss=0.5116, lr=0.0000038, metrics:accuracy:0.7307
INFO:root:14:49:42 [Epoch 1 Batch 800/4214] loss=0.2746, lr=0.0000076, metrics:accuracy:0.8113
INFO:root:14:50:20 [Epoch 1 Batch 1200/4214] loss=0.2370, lr=0.0000114, metrics:accuracy:0.8453
INFO:root:14:50:58 [Epoch 1 Batch 1600/4214] loss=0.2389, lr=0.0000152, metrics:accuracy:0.8614
INFO:root:14:51:35 [Epoch 1 Batch 2000/4214] loss=0.2137, lr=0.0000190, metrics:accuracy:0.8741
INFO:root:14:52:14 [Epoch 1 Batch 2400/4214] loss=0.2012, lr=0.0000197, metrics:accuracy:0.8827
INFO:root:14:52:54 [Epoch 1 Batch 2800/4214] loss=0.1886, lr=0.0000193, metrics:accuracy:0.8899
INFO:root:14:54:00 Namespace(accumulate=4, batch_size=8, bert_dataset='book_corpus_wiki_en_uncased', bert_model='bert_24_1024_16', calib_mode='customize', deploy=False, dev_batch_size=8, dtype='float32', early_stop=None, epochs=5, epsilon=1e-06, gpu=0, log_interval=400, lr=2e-05, max_len=128, model_parameters=None, model_prefix=None, num_calib_batches=5, only_calibration=False, only_inference=False, optimizer='bertadam', output_dir='./output_dir', pretrained_bert_parameters=None, quantized_dtype='auto', round_to=None, seed=2, task_name='SST', training_steps=None, warmup_ratio=0.1)
INFO:root:14:54:00 Using gradient accumulation. Effective batch size = batch_size * accumulate = 32
INFO:root:14:54:13 processing dataset...
INFO:root:14:54:16 Now we are doing BERT classification training on gpu(0)!
INFO:root:14:54:16 training steps=10523
INFO:root:14:54:40 [Epoch 1 Batch 400/8424] loss=0.6792, lr=0.0000019, metrics:accuracy:0.5816
INFO:root:14:55:04 [Epoch 1 Batch 800/8424] loss=0.4099, lr=0.0000038, metrics:accuracy:0.7087
INFO:root:14:55:28 [Epoch 1 Batch 1200/8424] loss=0.2836, lr=0.0000057, metrics:accuracy:0.7693
INFO:root:14:55:51 [Epoch 1 Batch 1600/8424] loss=0.2458, lr=0.0000076, metrics:accuracy:0.8032
INFO:root:14:56:14 [Epoch 1 Batch 2000/8424] loss=0.2461, lr=0.0000095, metrics:accuracy:0.8240
INFO:root:14:56:38 [Epoch 1 Batch 2400/8424] loss=0.2163, lr=0.0000114, metrics:accuracy:0.8396
INFO:root:14:57:02 [Epoch 1 Batch 2800/8424] loss=0.2395, lr=0.0000133, metrics:accuracy:0.8493
INFO:root:14:57:26 [Epoch 1 Batch 3200/8424] loss=0.2038, lr=0.0000152, metrics:accuracy:0.8583
INFO:root:14:57:49 [Epoch 1 Batch 3600/8424] loss=0.2174, lr=0.0000171, metrics:accuracy:0.8653
INFO:root:14:58:13 [Epoch 1 Batch 4000/8424] loss=0.2164, lr=0.0000190, metrics:accuracy:0.8707
INFO:root:14:58:36 [Epoch 1 Batch 4400/8424] loss=0.2205, lr=0.0000199, metrics:accuracy:0.8753
INFO:root:14:59:00 [Epoch 1 Batch 4800/8424] loss=0.1977, lr=0.0000197, metrics:accuracy:0.8801
INFO:root:14:59:23 [Epoch 1 Batch 5200/8424] loss=0.1985, lr=0.0000195, metrics:accuracy:0.8842
INFO:root:14:59:47 [Epoch 1 Batch 5600/8424] loss=0.1851, lr=0.0000193, metrics:accuracy:0.8876
INFO:root:15:00:10 [Epoch 1 Batch 6000/8424] loss=0.1856, lr=0.0000191, metrics:accuracy:0.8910
INFO:root:15:00:35 [Epoch 1 Batch 6400/8424] loss=0.1952, lr=0.0000188, metrics:accuracy:0.8937
INFO:root:15:00:58 [Epoch 1 Batch 6800/8424] loss=0.1895, lr=0.0000186, metrics:accuracy:0.8959
INFO:root:15:01:21 [Epoch 1 Batch 7200/8424] loss=0.1687, lr=0.0000184, metrics:accuracy:0.8987
INFO:root:15:01:45 [Epoch 1 Batch 7600/8424] loss=0.1849, lr=0.0000182, metrics:accuracy:0.9004
INFO:root:15:02:08 [Epoch 1 Batch 8000/8424] loss=0.1714, lr=0.0000180, metrics:accuracy:0.9024
INFO:root:15:02:31 [Epoch 1 Batch 8400/8424] loss=0.1656, lr=0.0000178, metrics:accuracy:0.9046
INFO:root:15:02:33 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:02:37 validation metrics:accuracy:0.9243
INFO:root:15:02:37 Time cost=4.19s, throughput=208.20 samples/s
INFO:root:15:02:41 params saved in: ./output_dir/model_bert_SST_0.params
INFO:root:15:02:41 Time cost=504.26s
INFO:root:15:03:04 [Epoch 2 Batch 400/8424] loss=0.1375, lr=0.0000176, metrics:accuracy:0.9622
INFO:root:15:03:27 [Epoch 2 Batch 800/8424] loss=0.1268, lr=0.0000174, metrics:accuracy:0.9598
INFO:root:15:03:50 [Epoch 2 Batch 1200/8424] loss=0.1248, lr=0.0000171, metrics:accuracy:0.9612
INFO:root:15:04:14 [Epoch 2 Batch 1600/8424] loss=0.1223, lr=0.0000169, metrics:accuracy:0.9627
INFO:root:15:04:37 [Epoch 2 Batch 2000/8424] loss=0.1304, lr=0.0000167, metrics:accuracy:0.9625
INFO:root:15:05:00 [Epoch 2 Batch 2400/8424] loss=0.1273, lr=0.0000165, metrics:accuracy:0.9623
INFO:root:15:05:24 [Epoch 2 Batch 2800/8424] loss=0.1138, lr=0.0000163, metrics:accuracy:0.9622
INFO:root:15:05:47 [Epoch 2 Batch 3200/8424] loss=0.1320, lr=0.0000161, metrics:accuracy:0.9618
INFO:root:15:06:10 [Epoch 2 Batch 3600/8424] loss=0.1367, lr=0.0000159, metrics:accuracy:0.9620
INFO:root:15:06:34 [Epoch 2 Batch 4000/8424] loss=0.1256, lr=0.0000157, metrics:accuracy:0.9622
INFO:root:15:06:57 [Epoch 2 Batch 4400/8424] loss=0.1365, lr=0.0000155, metrics:accuracy:0.9623
INFO:root:15:07:20 [Epoch 2 Batch 4800/8424] loss=0.1379, lr=0.0000152, metrics:accuracy:0.9623
INFO:root:15:07:44 [Epoch 2 Batch 5200/8424] loss=0.1548, lr=0.0000150, metrics:accuracy:0.9621
INFO:root:15:08:07 [Epoch 2 Batch 5600/8424] loss=0.1210, lr=0.0000148, metrics:accuracy:0.9623
INFO:root:15:08:30 [Epoch 2 Batch 6000/8424] loss=0.1311, lr=0.0000146, metrics:accuracy:0.9622
INFO:root:15:08:54 [Epoch 2 Batch 6400/8424] loss=0.1369, lr=0.0000144, metrics:accuracy:0.9618
INFO:root:15:09:17 [Epoch 2 Batch 6800/8424] loss=0.1231, lr=0.0000142, metrics:accuracy:0.9620
INFO:root:15:09:41 [Epoch 2 Batch 7200/8424] loss=0.1253, lr=0.0000140, metrics:accuracy:0.9621
INFO:root:15:10:05 [Epoch 2 Batch 7600/8424] loss=0.1439, lr=0.0000138, metrics:accuracy:0.9620
INFO:root:15:10:28 [Epoch 2 Batch 8000/8424] loss=0.1325, lr=0.0000136, metrics:accuracy:0.9620
INFO:root:15:10:52 [Epoch 2 Batch 8400/8424] loss=0.1291, lr=0.0000133, metrics:accuracy:0.9621
INFO:root:15:10:54 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:10:58 validation metrics:accuracy:0.9266
INFO:root:15:10:58 Time cost=4.35s, throughput=200.65 samples/s
INFO:root:15:11:02 params saved in: ./output_dir/model_bert_SST_1.params
INFO:root:15:11:02 Time cost=501.12s
INFO:root:15:11:26 [Epoch 3 Batch 400/8424] loss=0.0783, lr=0.0000131, metrics:accuracy:0.9800
INFO:root:15:11:49 [Epoch 3 Batch 800/8424] loss=0.0773, lr=0.0000129, metrics:accuracy:0.9809
INFO:root:15:12:13 [Epoch 3 Batch 1200/8424] loss=0.0965, lr=0.0000127, metrics:accuracy:0.9800
INFO:root:15:12:38 [Epoch 3 Batch 1600/8424] loss=0.0857, lr=0.0000125, metrics:accuracy:0.9795
INFO:root:15:13:02 [Epoch 3 Batch 2000/8424] loss=0.1004, lr=0.0000123, metrics:accuracy:0.9785
INFO:root:15:13:26 [Epoch 3 Batch 2400/8424] loss=0.0916, lr=0.0000121, metrics:accuracy:0.9785
INFO:root:15:13:49 [Epoch 3 Batch 2800/8424] loss=0.0964, lr=0.0000119, metrics:accuracy:0.9784
INFO:root:15:14:13 [Epoch 3 Batch 3200/8424] loss=0.1062, lr=0.0000116, metrics:accuracy:0.9780
INFO:root:15:14:37 [Epoch 3 Batch 3600/8424] loss=0.1042, lr=0.0000114, metrics:accuracy:0.9775
INFO:root:15:15:01 [Epoch 3 Batch 4000/8424] loss=0.0805, lr=0.0000112, metrics:accuracy:0.9776
INFO:root:15:15:25 [Epoch 3 Batch 4400/8424] loss=0.0884, lr=0.0000110, metrics:accuracy:0.9776
INFO:root:15:15:49 [Epoch 3 Batch 4800/8424] loss=0.0904, lr=0.0000108, metrics:accuracy:0.9778
INFO:root:15:16:14 [Epoch 3 Batch 5200/8424] loss=0.0972, lr=0.0000106, metrics:accuracy:0.9777
INFO:root:15:16:38 [Epoch 3 Batch 5600/8424] loss=0.1093, lr=0.0000104, metrics:accuracy:0.9773
INFO:root:15:17:02 [Epoch 3 Batch 6000/8424] loss=0.0872, lr=0.0000102, metrics:accuracy:0.9775
INFO:root:15:17:26 [Epoch 3 Batch 6400/8424] loss=0.0970, lr=0.0000100, metrics:accuracy:0.9774
INFO:root:15:17:50 [Epoch 3 Batch 6800/8424] loss=0.1036, lr=0.0000097, metrics:accuracy:0.9771
INFO:root:15:18:14 [Epoch 3 Batch 7200/8424] loss=0.0832, lr=0.0000095, metrics:accuracy:0.9772
INFO:root:15:18:38 [Epoch 3 Batch 7600/8424] loss=0.1056, lr=0.0000093, metrics:accuracy:0.9771
INFO:root:15:19:02 [Epoch 3 Batch 8000/8424] loss=0.1034, lr=0.0000091, metrics:accuracy:0.9770
INFO:root:15:19:26 [Epoch 3 Batch 8400/8424] loss=0.0888, lr=0.0000089, metrics:accuracy:0.9771
INFO:root:15:19:27 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:19:31 validation metrics:accuracy:0.9335
INFO:root:15:19:31 Time cost=4.17s, throughput=209.26 samples/s
INFO:root:15:19:36 params saved in: ./output_dir/model_bert_SST_2.params
INFO:root:15:19:36 Time cost=513.72s
INFO:root:15:19:59 [Epoch 4 Batch 400/8424] loss=0.0493, lr=0.0000087, metrics:accuracy:0.9869
INFO:root:15:20:23 [Epoch 4 Batch 800/8424] loss=0.0514, lr=0.0000085, metrics:accuracy:0.9875
INFO:root:15:20:47 [Epoch 4 Batch 1200/8424] loss=0.0509, lr=0.0000082, metrics:accuracy:0.9879
INFO:root:15:21:11 [Epoch 4 Batch 1600/8424] loss=0.0634, lr=0.0000080, metrics:accuracy:0.9879
INFO:root:15:21:34 [Epoch 4 Batch 2000/8424] loss=0.0597, lr=0.0000078, metrics:accuracy:0.9875
INFO:root:15:21:58 [Epoch 4 Batch 2400/8424] loss=0.0503, lr=0.0000076, metrics:accuracy:0.9876
INFO:root:15:22:22 [Epoch 4 Batch 2800/8424] loss=0.0724, lr=0.0000074, metrics:accuracy:0.9870
INFO:root:15:22:45 [Epoch 4 Batch 3200/8424] loss=0.0785, lr=0.0000072, metrics:accuracy:0.9860
INFO:root:15:23:09 [Epoch 4 Batch 3600/8424] loss=0.0746, lr=0.0000070, metrics:accuracy:0.9856
INFO:root:15:23:32 [Epoch 4 Batch 4000/8424] loss=0.0757, lr=0.0000068, metrics:accuracy:0.9853
INFO:root:15:23:55 [Epoch 4 Batch 4400/8424] loss=0.0736, lr=0.0000066, metrics:accuracy:0.9851
INFO:root:15:24:19 [Epoch 4 Batch 4800/8424] loss=0.0665, lr=0.0000063, metrics:accuracy:0.9849
INFO:root:15:24:43 [Epoch 4 Batch 5200/8424] loss=0.0664, lr=0.0000061, metrics:accuracy:0.9849
INFO:root:15:25:06 [Epoch 4 Batch 5600/8424] loss=0.0851, lr=0.0000059, metrics:accuracy:0.9845
INFO:root:15:25:30 [Epoch 4 Batch 6000/8424] loss=0.0552, lr=0.0000057, metrics:accuracy:0.9845
INFO:root:15:25:53 [Epoch 4 Batch 6400/8424] loss=0.0776, lr=0.0000055, metrics:accuracy:0.9843
INFO:root:15:26:17 [Epoch 4 Batch 6800/8424] loss=0.0663, lr=0.0000053, metrics:accuracy:0.9843
INFO:root:15:26:41 [Epoch 4 Batch 7200/8424] loss=0.0730, lr=0.0000051, metrics:accuracy:0.9842
INFO:root:15:27:05 [Epoch 4 Batch 7600/8424] loss=0.0617, lr=0.0000049, metrics:accuracy:0.9843
INFO:root:15:27:28 [Epoch 4 Batch 8000/8424] loss=0.0516, lr=0.0000047, metrics:accuracy:0.9844
INFO:root:15:27:51 [Epoch 4 Batch 8400/8424] loss=0.0718, lr=0.0000044, metrics:accuracy:0.9845
INFO:root:15:27:53 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:27:57 validation metrics:accuracy:0.9381
INFO:root:15:27:57 Time cost=4.28s, throughput=203.88 samples/s
INFO:root:15:28:01 params saved in: ./output_dir/model_bert_SST_3.params
INFO:root:15:28:01 Time cost=505.41s
INFO:root:15:28:24 [Epoch 5 Batch 400/8424] loss=0.0254, lr=0.0000042, metrics:accuracy:0.9934
INFO:root:15:28:48 [Epoch 5 Batch 800/8424] loss=0.0296, lr=0.0000040, metrics:accuracy:0.9933
INFO:root:15:29:11 [Epoch 5 Batch 1200/8424] loss=0.0555, lr=0.0000038, metrics:accuracy:0.9918
INFO:root:15:29:35 [Epoch 5 Batch 1600/8424] loss=0.0508, lr=0.0000036, metrics:accuracy:0.9906
INFO:root:15:29:58 [Epoch 5 Batch 2000/8424] loss=0.0454, lr=0.0000034, metrics:accuracy:0.9904
INFO:root:15:30:21 [Epoch 5 Batch 2400/8424] loss=0.0384, lr=0.0000032, metrics:accuracy:0.9905
INFO:root:15:30:45 [Epoch 5 Batch 2800/8424] loss=0.0434, lr=0.0000030, metrics:accuracy:0.9903
INFO:root:15:31:09 [Epoch 5 Batch 3200/8424] loss=0.0299, lr=0.0000027, metrics:accuracy:0.9906
INFO:root:15:31:32 [Epoch 5 Batch 3600/8424] loss=0.0426, lr=0.0000025, metrics:accuracy:0.9906
INFO:root:15:31:56 [Epoch 5 Batch 4000/8424] loss=0.0442, lr=0.0000023, metrics:accuracy:0.9906
INFO:root:15:32:19 [Epoch 5 Batch 4400/8424] loss=0.0381, lr=0.0000021, metrics:accuracy:0.9906
INFO:root:15:32:43 [Epoch 5 Batch 4800/8424] loss=0.0462, lr=0.0000019, metrics:accuracy:0.9904
INFO:root:15:33:06 [Epoch 5 Batch 5200/8424] loss=0.0374, lr=0.0000017, metrics:accuracy:0.9905
INFO:root:15:33:30 [Epoch 5 Batch 5600/8424] loss=0.0444, lr=0.0000015, metrics:accuracy:0.9906
INFO:root:15:33:53 [Epoch 5 Batch 6000/8424] loss=0.0323, lr=0.0000013, metrics:accuracy:0.9907
INFO:root:15:34:17 [Epoch 5 Batch 6400/8424] loss=0.0459, lr=0.0000011, metrics:accuracy:0.9906
INFO:root:15:34:40 [Epoch 5 Batch 6800/8424] loss=0.0360, lr=0.0000008, metrics:accuracy:0.9907
INFO:root:15:35:03 [Epoch 5 Batch 7200/8424] loss=0.0384, lr=0.0000006, metrics:accuracy:0.9907
INFO:root:15:35:26 [Epoch 5 Batch 7600/8424] loss=0.0530, lr=0.0000004, metrics:accuracy:0.9906
INFO:root:15:35:50 [Epoch 5 Batch 8000/8424] loss=0.0424, lr=0.0000002, metrics:accuracy:0.9905
INFO:root:15:36:13 Finish training step: 10523
INFO:root:15:36:13 Now we are doing evaluation on dev with gpu(0).
INFO:root:15:36:18 validation metrics:accuracy:0.9335
INFO:root:15:36:18 Time cost=4.19s, throughput=207.92 samples/s
INFO:root:15:36:22 params saved in: ./output_dir/model_bert_SST_4.params
INFO:root:15:36:22 Time cost=501.03s
INFO:root:15:36:23 Best model at epoch 3. Validation metrics:accuracy:0.9381
INFO:root:15:36:23 Now we are doing testing on test with gpu(0).
INFO:root:15:36:32 Time cost=9.56s, throughput=190.82 samples/s
